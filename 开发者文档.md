本文档记录 MedOverview 的 RAG（Retrieval-Augmented Generation）实现方案与落地步骤。

你当前站点部署在 GitHub Pages（纯静态）：https://kkzyu.github.io/MedOverview 。
因此“上线可用”的 RAG 需要默认走 **前端 BYOK**（用户在浏览器输入自己的 DeepSeek Key），在浏览器侧完成检索并直接调用 DeepSeek 生成。

同时保留一条“本地/自建服务”路线：FastAPI + LangChain + FAISS（方案A），用于你将来迁移到可运行后端的平台（云服务器/Render/Fly.io/自建）。

下面把任务拆成小步，每步完成后都要：`git add` → `git commit -m "..."`。

---

## 0. 约束与目标

### 0.1 部署约束（GitHub Pages）
- 只能托管静态文件，**不能运行 FastAPI**。
- 不能把任何 API Key 写进仓库/页面。
- BYOK key 只能存在浏览器端（建议 localStorage，可一键清除）。

### 0.2 RAG 交互目标
- 页面新增“问答”面板：输入问题、选择回答风格、点击预设问题。
- 默认只在“当前筛选论文集合”中检索（与右侧列表联动）。
- 输出支持引用：返回 paper_id 列表，且可在 UI 里点击跳转到论文链接（或高亮右侧列表）。

---

## 1) 前端（GitHub Pages 可用）RAG：检索（本地）+ 生成（DeepSeek）

### 1.1 第一步：加 UI（不改检索/生成逻辑）
在 `web/index.html` 增加问答区域（不影响现有 Sankey 与右侧列表）：
- API Key 输入框（password）+ “记住/清除”按钮
- 问题输入框 + Ask 按钮
- 风格切换：`overview`（综述型）/ `cite`（精确引用型）
- 预设问题按钮组
- Answer 区域 + 引用列表（可点击）

完成后 commit：`feat(web): add QA panel UI`

### 1.2 第二步：实现检索（纯前端，默认 BM25/词匹配）
原因：浏览器侧跑 sentence-transformers 不现实；而 DeepSeek 是否提供可 CORS 的 embeddings 端点不确定。
因此默认实现一个“足够好用”的本地检索：
- 文档文本：title + keywords + triple + abstract
- 基于分词/词频的 BM25 或简化 TF-IDF（规模 200~300 篇，足够快）
- 支持 `paper_ids` 限定：只在当前筛选集合内计算分数

完成后 commit：`feat(rag): add client-side retrieval`

### 1.3 第三步：实现生成（DeepSeek Chat Completions，BYOK）
浏览器直接调用：`POST https://api.deepseek.com/v1/chat/completions`
- Header：`Authorization: Bearer <api_key>`
- 强制 JSON 输出：`response_format: {"type":"json_object"}`
- 两种风格分别用不同 prompt：
  - `overview`：主题归纳 + 方法/结果对比 + 代表论文
  - `cite`：每条结论都要带 citations（paper_id 列表）

完成后 commit：`feat(rag): add deepseek generation (BYOK)`

已落地代码位置：
- UI：web/index.html
- 逻辑：web/app.js（BM25 检索 + DeepSeek 生成）
- 预设问题：data/presets.json

**注意：CORS 风险**
- 部分浏览器/网络环境可能阻止从 GitHub Pages 直连第三方 API（表现为请求失败/TypeError）。
- 若遇到这种情况：
  1) 优先用本地运行模式验证（见“本地验证”）；或
  2) 部署一个你可控的后端（方案A），由后端转发调用 DeepSeek。

### 1.4 第四步：安全与体验增强
- 不在 console 打印 api_key
- 请求失败提示（余额不足/限流/CORS）
- 限制 question 长度、top_k 范围
- citations 校验：只允许 data/papers.json 里存在的 id

完成后 commit：`chore(rag): harden client-side rag`

---

## 2) 后端（可选，方案A：FastAPI + LangChain + FAISS）

这条路线只在你有可运行后端的部署平台时启用（GitHub Pages 不适用）。

### 2.1 API 设计（与前端兼容）
保留 `/api/ask`：
- Request：`question, style, top_k, paper_ids, api_key?, model?`
- Response：`answer_cn, key_points, citations[], used_style`

### 2.2 索引与检索
- 文档：按 abstract 段落切 chunk（提高命中精度）
- embeddings：sentence-transformers（本地）
- 向量库：FAISS（可持久化）
- retriever：MMR

完成后 commit：`feat(api): langchain rag ask endpoint`

---

## 3) 本地验证

### 3.1 仅静态页面（推荐）
运行 `serve_web.ps1`，打开 http://localhost:8000/web/ ，测试：
- UI 是否正常渲染
- 在右侧筛选到某个主题后再 Ask，检索是否只覆盖当前 paper_ids

GitHub Pages 上使用：
- 打开 https://kkzyu.github.io/MedOverview
- 在“问答（RAG）”里粘贴自己的 DeepSeek API Key
- 选风格（综述型/精确引用型）→ Ask

### 3.2 含后端（可选）
使用 `code/build/api_server.py` 运行 FastAPI（需要依赖）。


**1) 后端（FastAPI）RAG 架构（LangChain 方案A）**
- **数据源**：继续用 papers.json（以及 `labels.json` 的 stage 等 metadata）
- **索引构建**（启动时或离线构建）
  - 把每篇论文转成 LangChain `Document`
    - `page_content`：title + abstract + method/result/contribution + keywords（你现在 build_doc_text 那套就很好）
    - `metadata`：`paper_id, url, venue, year, stage_l1/l2 ...`
  - **文本切分**：建议按段落/长度切 chunk（abstract 可能较长），这样检索命中更准、引用也更细
  - **向量化**：继续用你现在的本地 `sentence-transformers` embedding（成本低、稳定）
  - **VectorStore**：优先 FAISS（可持久化），如果 Windows 依赖麻烦就先用内存向量检索（规模 247 很小）
- **检索器（Retriever）**
  - 默认用 MMR（减少重复）
  - 支持 `paper_ids` 限定（前端把“当前视图过滤出来的论文集合”传上来）
  - 支持 metadata filter（比如只看某 stage）
- **生成器（LLM）**
  - DeepSeek OpenAI-compatible：LangChain 用 `langchain-openai` 的 `ChatOpenAI(base_url=..., api_key=...)`
  - **重点**：`api_key` 优先来自请求（用户输入），服务器端环境变量只作为“站点自带 key”的可选模式
- **两种回答风格（同一个检索，两个 prompt/chain）**
  - `overview`（综述型）：输出“主题归纳 + 方法流派/结果类型对比 + 代表论文列表”
  - `cite`（精确引用型）：每条关键结论后面强制带 citation（paper_id），并返回 citations 数组
  - LangChain 实现上用 `RunnableBranch` 或简单 if 分支，根据 `style` 走不同 prompt/输出 schema

**2) API 设计（支持前端切换风格 + BYOK）**
建议保留一个主接口 `/api/ask`：
- Request（示例字段）
  - `question`: string（限制长度，比如 2k）
  - `style`: `"overview" | "cite"`
  - `top_k`: int（3-12）
  - `paper_ids`: string[]（可选，来自前端当前筛选）
  - `api_key`: string（可选；BYOK 模式下必填；仅用于本次请求，不落盘）
  - `model`: string（可选，默认 deepseek-chat；给高级用户切模型）
- Response
  - `answer_cn`: string
  - `key_points`: string[]（可选）
  - `citations`: `{paper_id,title,url,score}`[]（至少包含 paper_id，便于前端跳转）
  - `used_style`: `"overview" | "cite"`

可选再加一个 `/api/presets`：
- 返回预设问题列表（方便前端初始化，不必写死在 JS）

**3) 前端交互（按钮切换风格 + 预设问题）**
在页面加一个 “问答” 小面板（不影响你现有 Sankey/详情）：
- **API Key 输入框**（password）
  - 默认不填也能用：如果站点部署者在服务端配置了 key
  - 若服务端未配置 key，则提示用户输入自己的 key（BYOK）
  - 存储建议：只存浏览器 `localStorage`（可一键清除），不上传到任何日志
- **回答风格切换**
  - 两个 radio/button：`综述型` / `精确引用型`
- **预设问题**
  - 下拉或按钮列表：点击自动填入问题并触发 ask
  - 预设示例（你这类数据很适合）：
    - “2024-2025 医疗 AI 里，最热门的研究方向有哪些？各举 3 篇代表论文”
    - “在【诊断/影像分割/临床推理】方向，主流方法路线是什么？优缺点对比”
    - “当前数据中，多模态医疗大模型主要解决什么问题？常见评测方式是什么？”
    - “有哪些工作关注可靠性/校准/不确定性？结论是什么？”
    - “哪些工作更贴近临床工作流/部署？有什么真实世界评估证据？”
- **与当前视图联动**
  - 默认问答只在“当前筛选论文集合”里检索（把当前 `paper_ids` 传给后端），用户会明显感觉“对当前节点/阶段提问”

**4) 上线安全性（你这个场景必须考虑的点）**
- **BYOK 默认策略**：不在服务端保存用户 key；请求里带 key 只用于当次调用
- **日志与错误**：后端严格避免把 `api_key` 打进日志/异常（包括请求体日志、traceback）
- **CORS**：本地可放开；上线时改成 allowlist（只允许你的域名）
- **限流/防滥用**（上线很关键）
  - `/api/ask` 做简单 rate limit（IP/分钟），并限制 `top_k`、question 长度、响应 tokens
- **提示词注入防护**
  - system prompt 强制“只基于 contexts”，并要求输出 JSON；不要把用户问题直接当指令执行
- **HTTPS**：上线必须 HTTPS，否则 BYOK key 明文传输风险很大
- **内容安全**：返回中不泄露任何 server 环境变量信息；错误提示只给“缺少 key/额度不足/限流”等

如果你确认这个设计，我下一步可以按这个方案把后端 `/api/ask` 升级成 LangChain 版（DeepSeek OpenAI 兼容），并把前端加上“风格切换 + 预设问题 + BYOK 输入框”，同时把“当前筛选 paper_ids”联动进去。