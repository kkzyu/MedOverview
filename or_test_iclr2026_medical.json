[
  {
    "venue": "ICLR",
    "search_title": "Cancer-Myth: Evaluating Large Language Models on Patient Questions with False Presuppositions",
    "full_title": "Cancer-Myth: Evaluating Large Language Models on Patient Questions with False Presuppositions",
    "url": "https://openreview.net/forum?id=fOXLhZIaUj",
    "year": 2026,
    "is_main_conference": true,
    "abstract_snippet": "Cancer patients are increasingly turning to large language models (LLMs) for medical information, making it critical to assess how well these models handle complex, personalized questions. \nHowever, current medical benchmarks focus on medical exams or consumer-searched questions and do not evaluate LLMs on real patient questions with patient details. \nIn this paper, we first have three hematology-oncology physicians evaluate cancer-related questions drawn from real patients. \nWhile LLM responses are generally accurate, the models frequently fail to recognize or address false presuppositions} in the questions, posing risks to safe medical decision-making.\nTo study this limitation systematically, we introduce Cancer-Myth, an expert-verified adversarial dataset of 585 cancer-related questions with false presuppositions.\nOn this benchmark, no frontier LLM---including GPT-5, Gemini-2.5-Pro, and Claude-4-Sonnet---corrects these false presuppositions more than $43\\%$ of the time.\nTo study mitigation strategies, we further construct a 150-question Cancer-Myth-NFP set, in which physicians confirm the absence of false presuppositions.\nWe find typical mitigation strategies, such as adding precautionary prompts with GEPA optimization, can raise accuracy on Cancer-Myth to $80\\%$, but at the cost of misidentifying presuppositions in $41\\%$ of Cancer-Myth-NFP questions and causing a $10\\%$ relative performance drop on other medical benchmarks.\nThese findings highlight a critical gap in the reliability of LLMs, show that prompting alone is not a reliable remedy for false presuppositions, and underscore the need for more robust safeguards in medical AI systems.",
    "abstract": "Cancer patients are increasingly turning to large language models (LLMs) for medical information, making it critical to assess how well these models handle complex, personalized questions. \nHowever, current medical benchmarks focus on medical exams or consumer-searched questions and do not evaluate LLMs on real patient questions with patient details. \nIn this paper, we first have three hematology-oncology physicians evaluate cancer-related questions drawn from real patients. \nWhile LLM responses are generally accurate, the models frequently fail to recognize or address false presuppositions} in the questions, posing risks to safe medical decision-making.\nTo study this limitation systematically, we introduce Cancer-Myth, an expert-verified adversarial dataset of 585 cancer-related questions with false presuppositions.\nOn this benchmark, no frontier LLM---including GPT-5, Gemini-2.5-Pro, and Claude-4-Sonnet---corrects these false presuppositions more than $43\\%$ of the time.\nTo study mitigation strategies, we further construct a 150-question Cancer-Myth-NFP set, in which physicians confirm the absence of false presuppositions.\nWe find typical mitigation strategies, such as adding precautionary prompts with GEPA optimization, can raise accuracy on Cancer-Myth to $80\\%$, but at the cost of misidentifying presuppositions in $41\\%$ of Cancer-Myth-NFP questions and causing a $10\\%$ relative performance drop on other medical benchmarks.\nThese findings highlight a critical gap in the reliability of LLMs, show that prompting alone is not a reliable remedy for false presuppositions, and underscore the need for more robust safeguards in medical AI systems.",
    "abstract_source_venue": "OpenReview",
    "abstract_source_url": "https://openreview.net/forum?id=fOXLhZIaUj",
    "openreview_id": "fOXLhZIaUj",
    "openreview_forum_id": "fOXLhZIaUj",
    "authors": [],
    "pdf_url": "https://openreview.net/pdf/a21f83e80e84a8f0b6d4546a959c75408be46cd2.pdf"
  },
  {
    "venue": "ICLR",
    "search_title": "MedAgent-Pro: Towards Evidence-based Multi-modal Medical Diagnosis via Reasoning Agentic Workflow",
    "full_title": "MedAgent-Pro: Towards Evidence-based Multi-modal Medical Diagnosis via Reasoning Agentic Workflow",
    "url": "https://openreview.net/forum?id=ZOuU0udyA4",
    "year": 2026,
    "is_main_conference": true,
    "abstract_snippet": "Modern clinical diagnosis relies on the comprehensive analysis of multi-modal patient data, drawing on medical expertise to ensure systematic and rigorous reasoning. Recent advances in Vision–Language Models (VLMs) and agent-based methods are reshaping medical diagnosis by effectively integrating multi-modal information. However, they often output direct answers and empirical-driven conclusions without clinical evidence supported by quantitative analysis, which compromises their reliability and hinders clinical usability. \nHere we propose MedAgent-Pro, an agentic reasoning paradigm that mirrors modern diagnosis principles via a hierarchical diagnostic workflow, consisting of disease-level standardized plan generation and patient-level personalized step-by-step reasoning. To support disease-level planning, a retrieval-augmented generation agent is designed to access medical guidelines for alignment with clinical standards.  For patient-level reasoning, MedAgent-Pro leverages professional tools such as visual models to take various actions to analyze multi-modal input, and performs evidence-based reflection to iteratively adjust memory, enforcing rigorous reasoning throughout the process. Extensive experiments across a wide range of anatomical regions, imaging modalities, and diseases demonstrate the superiority of MedAgent-Pro over mainstream VLMs, agentic systems and leading expert models. Ablation studies and expert evaluation further confirm its robustness and clinical relevance. Anonymized code link is available in the reproducibility statement.",
    "abstract": "Modern clinical diagnosis relies on the comprehensive analysis of multi-modal patient data, drawing on medical expertise to ensure systematic and rigorous reasoning. Recent advances in Vision–Language Models (VLMs) and agent-based methods are reshaping medical diagnosis by effectively integrating multi-modal information. However, they often output direct answers and empirical-driven conclusions without clinical evidence supported by quantitative analysis, which compromises their reliability and hinders clinical usability. \nHere we propose MedAgent-Pro, an agentic reasoning paradigm that mirrors modern diagnosis principles via a hierarchical diagnostic workflow, consisting of disease-level standardized plan generation and patient-level personalized step-by-step reasoning. To support disease-level planning, a retrieval-augmented generation agent is designed to access medical guidelines for alignment with clinical standards.  For patient-level reasoning, MedAgent-Pro leverages professional tools such as visual models to take various actions to analyze multi-modal input, and performs evidence-based reflection to iteratively adjust memory, enforcing rigorous reasoning throughout the process. Extensive experiments across a wide range of anatomical regions, imaging modalities, and diseases demonstrate the superiority of MedAgent-Pro over mainstream VLMs, agentic systems and leading expert models. Ablation studies and expert evaluation further confirm its robustness and clinical relevance. Anonymized code link is available in the reproducibility statement.",
    "abstract_source_venue": "OpenReview",
    "abstract_source_url": "https://openreview.net/forum?id=ZOuU0udyA4",
    "openreview_id": "ZOuU0udyA4",
    "openreview_forum_id": "ZOuU0udyA4",
    "authors": [],
    "pdf_url": "https://openreview.net/pdf/e7110936d04fac257223b380ae256b851b59bfa3.pdf"
  }
]