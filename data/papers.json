[
  {
    "id": "af8c55fc2338",
    "venue": "ICLR",
    "year": 2024,
    "title": "MOTOR: A Time-to-Event Foundation Model For Structured Medical Records",
    "url": "https://iclr.cc/virtual/2024/poster/18777",
    "abstract": "We present a self-supervised, time-to-event (TTE) foundation model called MOTOR (Many Outcome Time Oriented Representations) which is pretrained on timestamped sequences of events in electronic health records (EHR) and health insurance claims. TTE models are used for estimating the probability distribution of the time until a specific event occurs, which is an important task in medical settings. TTE models provide many advantages over classification using fixed time horizons, including naturally handling censored observations, but are challenging to train with limited labeled data. MOTOR addresses this challenge by pretraining on up to 55M patient records (9B clinical events). We evaluate MOTOR's transfer learning performance on 19 tasks, across 3 patient databases (a private EHR system, MIMIC-IV, and Merative claims data). Task-specific models adapted from MOTOR improve time-dependent C statistics by 4.6\\% over state-of-the-art, improve label efficiency by up to 95\\%, and are more robust to temporal distributional shifts. We further evaluate cross-site portability by adapting our MOTOR foundation model for six prediction tasks on the MIMIC-IV dataset, where it outperforms all baselines. MOTOR is the first foundation model for medical TTE predictions and we release a 143M parameter pretrained model for research use at https://huggingface.co/StanfordShahLab/motor-t-base.",
    "summary_cn": "MOTOR是首个用于医疗时间到事件预测的基础模型，通过自监督预训练处理电子健康记录，在19个任务中提升预测性能、标签效率和鲁棒性。",
    "keywords": [
      "时间到事件模型",
      "电子健康记录",
      "自监督学习",
      "基础模型",
      "转移学习",
      "预测性能"
    ],
    "triple": {
      "method": "自监督预训练于55M患者记录",
      "result": "在19个任务中C统计量提升4.6%，标签效率提高95%",
      "contribution": "首个医疗TTE基础模型，提升预测鲁棒性和跨站点可移植性"
    }
  },
  {
    "id": "56d626d45cda",
    "venue": "ICLR",
    "year": 2025,
    "title": "Reliable and Diverse Evaluation of LLM Medical Knowledge Mastery",
    "url": "https://iclr.cc/virtual/2025/poster/29535",
    "abstract": "Mastering medical knowledge is crucial for medical-specific LLMs. However, despite the existence of medical benchmarks like MedQA, a unified framework that fully leverages existing knowledge bases to evaluate LLMs' mastery of medical knowledge is still lacking. We propose PretexEval, a novel framework that dynamically generates reliable and diverse test samples to evaluate LLMs for any given medical knowledge base. We notice that test samples produced directly from knowledge bases by templates or LLMs may introduce factual errors and also lack diversity. To address these issues, our framework employs predicate equivalence transformations to produce a series of variants for any given medical knowledge point. Finally, these produced predicate variants are converted into textual language, resulting in a series of reliable and diverse test samples. Here, we use our proposed framework to systematically investigate the mastery of medical factual knowledge of 12 well-known LLMs, based on two knowledge bases that are crucial for clinical diagnosis and treatment. The evaluation results illustrate that current LLMs still exhibit significant deficiencies in fully mastering medical knowledge, despite achieving considerable success on some famous public benchmarks. These new findings provide valuable insights for developing medical-specific LLMs, highlighting that current LLMs urgently need to strengthen their comprehensive and in-depth mastery of medical knowledge before being applied to real-world medical scenarios.",
    "summary_cn": "提出PretexEval框架，通过谓词等价变换动态生成可靠多样的测试样本，评估12个知名LLM对医学知识的掌握。结果显示，尽管在公共基准上表现良好，LLM在全面掌握医学知识方面仍有显著不足。",
    "keywords": [
      "医学知识评估",
      "LLM评估框架",
      "谓词等价变换",
      "测试样本生成",
      "临床知识库",
      "多样性评估"
    ],
    "triple": {
      "method": "谓词等价变换生成测试样本",
      "result": "LLM在医学知识掌握上存在显著不足",
      "contribution": "提供可靠多样的评估框架与洞察"
    }
  },
  {
    "id": "de94191095a6",
    "venue": "ICLR",
    "year": 2024,
    "title": "Medical Event Data Standard (MEDS): Facilitating Machine Learning for Health",
    "url": "https://iclr.cc/virtual/2024/23574",
    "abstract": "We introduce the Medical Event Data Standard (MEDS), a lightweight schema for enabling machine learning over electronic health record (EHR) data. Unlike common data models and data interoperability formats, MEDS is a minimal standard designed for maximum interoperability across datasets, existing tools, and model architectures. By providing a simple standardization layer between datasets and model-specific code, MEDS will enable more reproducible, robust, computationally performant, and collaborative machine learning research using EHR data. We highlight several existing MEDS integrations with models, datasets, and tools, and invite the community for further development and adoption.",
    "summary_cn": "提出医学事件数据标准（MEDS），作为轻量级模式，旨在提升电子健康记录数据的机器学习互操作性与可复现性。",
    "keywords": [
      "医学事件数据标准",
      "电子健康记录",
      "机器学习",
      "互操作性",
      "数据标准化",
      "可复现研究"
    ],
    "triple": {
      "method": "设计轻量级数据模式",
      "result": "提升数据集、工具与模型的互操作性",
      "contribution": "促进基于EHR的机器学习研究可复现与协作"
    }
  },
  {
    "id": "fc1d508739de",
    "venue": "ICLR",
    "year": 2025,
    "title": "MMed-RAG: Versatile Multimodal RAG System for Medical Vision Language Models",
    "url": "https://proceedings.iclr.cc/paper_files/paper/2025/hash/a559a5a8aa5ae6682ced009ad97cdb16-Abstract-Conference.html",
    "abstract": "Artificial Intelligence (AI) has demonstrated significant potential in healthcare, particularly in disease diagnosis and treatment planning. Recent progress in Medical Large Vision-Language Models (Med-LVLMs) has opened up new possibilities for interactive diagnostic tools. However, these models often suffer from factual hallucination, which can lead to incorrect diagnoses. Fine-tuning and retrieval-augmented generation (RAG) have emerged as methods to address these issues. However, the amount of high-quality data and distribution shifts between training data and deployment data limit the application of fine-tuning methods. Although RAG is lightweight and effective, existing RAG-based approaches are not sufficiently general to different medical domains and can potentially cause misalignment issues, both between modalities and between the model and the ground truth. In this paper, we propose a versatile multimodal RAG system, MMed-RAG, designed to enhance the factuality of Med-LVLMs. Our approach introduces a domain-aware retrieval mechanism, an adaptive retrieved contexts selection, and a provable RAG-based preference fine-tuning strategy. These innovations make the RAG process sufficiently general and reliable, significantly improving alignment when introducing retrieved contexts. Experimental results across five medical datasets (involving radiology, ophthalmology, pathology) on medical VQA and report generation demonstrate that MMed-RAG can achieve an average improvement of 43.8% in factual accuracy in the factual accuracy of Med-LVLMs.",
    "summary_cn": "提出MMed-RAG多模态检索增强生成系统，通过领域感知检索和自适应选择，提升医疗视觉语言模型的事实准确性，在多个医学数据集上平均提升43.8%。",
    "keywords": [
      "检索增强生成",
      "医疗视觉语言模型",
      "多模态",
      "事实准确性",
      "领域感知检索",
      "自适应选择"
    ],
    "triple": {
      "method": "领域感知检索、自适应选择、基于RAG的偏好微调",
      "result": "在五个医学数据集上平均提升事实准确性43.8%",
      "contribution": "提出通用可靠的MMed-RAG系统，增强医疗模型事实性"
    }
  },
  {
    "id": "a51c369d9384",
    "venue": "ICLR",
    "year": 2024,
    "title": "Enhancing Small Medical Learners with Privacy-preserving Contextual Prompting",
    "url": "https://iclr.cc/virtual/2024/poster/17369",
    "abstract": "The ICLR Logo above may be used on presentations. Right-click and choose\n                            download. It is a vector graphic and may be used at any scale.",
    "summary_cn": "该研究提出一种隐私保护的情境提示方法，用于增强小型医学模型性能，通过上下文学习提升准确性同时保护数据隐私。",
    "keywords": [
      "隐私保护",
      "情境提示",
      "小型医学模型",
      "上下文学习",
      "数据隐私",
      "模型增强"
    ],
    "triple": {
      "method": "隐私保护的情境提示",
      "result": "提升小型医学模型性能",
      "contribution": "增强准确性并保护数据隐私"
    }
  },
  {
    "id": "1da25b777b31",
    "venue": "ICLR",
    "year": 2025,
    "title": "[Tiny] Synthetic-based retrieval of patient medical data",
    "url": "https://iclr.cc/virtual/2025/32180",
    "abstract": "Medical retrieval systems play a crucial role in facilitating an accurate and efficient diagnosis by allowing physicians to access relevant radiological reports and patient descriptions. However, the development of such systems is often hindered by the limited availability of high-quality labeled data due to privacy concerns and data scarcity. In this work, we propose an approach to address this challenge by using synthetic data generation using Large Language Models (LLMs). Our experiments show that synthetic data is useful for improving retrieval performance in various tasks, both in training modes entirely on synthetic data and in a mixed-with-real-data mode.",
    "summary_cn": "本研究利用大语言模型生成合成数据，以解决医疗检索系统因隐私和数据稀缺导致的高质量标注数据不足问题，实验表明合成数据能有效提升检索性能。",
    "keywords": [
      "合成数据",
      "医疗检索",
      "大语言模型",
      "数据稀缺",
      "隐私保护",
      "检索性能"
    ],
    "triple": {
      "method": "使用大语言模型生成合成数据",
      "result": "合成数据提升检索性能",
      "contribution": "解决医疗数据稀缺问题"
    }
  },
  {
    "id": "3ddd8b7d04ec",
    "venue": "ICLR",
    "year": 2025,
    "title": "Time-to-Event Pretraining for 3D Medical Imaging",
    "url": "https://iclr.cc/virtual/2025/poster/27661",
    "abstract": "With the rise of medical foundation models and the growing availability of imaging data, scalable pretraining techniques offer a promising way to identify imaging biomarkers predictive of future disease risk. While current self-supervised methods for 3D medical imaging models capture local structural features like organ morphology, they fail to link pixel biomarkers with long-term health outcomes due to a missing context problem. Current approaches lack the temporal context necessary to identify biomarkers correlated with disease progression, as they rely on supervision derived only from images and concurrent text descriptions. To address this, we introduce time-to-event pretraining, a pretraining framework for 3D medical imaging models that leverages large-scale temporal supervision from paired, longitudinal electronic health records (EHRs). Using a dataset of 18,945 CT scans (4.2 million 2D images) and time-to-event distributions across thousands of EHR-derived tasks, our method improves outcome prediction, achieving an average AUROC increase of 23.7% and a 29.4% gain in Harrell’s C-index across 8 benchmark tasks. Importantly, these gains are achieved without sacrificing diagnostic classification performance. This study lays the foundation for integrating longitudinal EHR and 3D imaging data to advance clinical risk prediction.",
    "summary_cn": "提出时间-事件预训练框架，利用纵向电子健康记录为3D医学影像模型提供时间监督，显著提升疾病风险预测性能，不损害诊断分类。",
    "keywords": [
      "时间-事件预训练",
      "3D医学影像",
      "电子健康记录",
      "疾病风险预测",
      "纵向数据",
      "成像生物标志物"
    ],
    "triple": {
      "method": "利用纵向EHR数据的时间监督进行预训练",
      "result": "AUROC平均提升23.7%，C-index提升29.4%",
      "contribution": "整合纵向EHR与3D影像，提升临床风险预测"
    }
  },
  {
    "id": "6e5bb5535c7d",
    "venue": "ICLR",
    "year": 2024,
    "title": "FairTune: Optimizing Parameter Efficient Fine Tuning for Fairness in Medical Image Analysis",
    "url": "https://iclr.cc/virtual/2024/poster/19237",
    "abstract": "Training models with robust group fairness properties is crucial in ethically sensitive application areas such as medical diagnosis. Despite the growing body of work aiming to minimise demographic bias in AI, this problem remains challenging. A key reason for this challenge is the fairness generalisation gap: High-capacity deep learning models can fit all training data nearly perfectly, and thus also exhibit perfect fairness during training. In this case, bias emerges only during testing when generalisation performance differs across sub-groups. This motivates us to take a bi-level optimisation perspective on fair learning: Optimising the learning strategy based on validation fairness. Specifically, we consider the highly effective workflow of adapting pre-trained models to downstream medical imaging tasks using parameter-efficient fine-tuning (PEFT) techniques. There is a trade-off between updating more parameters, enabling a better fit to the task of interest vs. fewer parameters, potentially reducing the generalisation gap. To manage this tradeoff, we propose FairTune, a framework to optimise the choice of PEFT parameters with respect to fairness. We demonstrate empirically that FairTune leads to improved fairness on a range of medical imaging datasets. The code is available at https://github.com/Raman1121/FairTune.",
    "summary_cn": "FairTune框架通过双层优化调整参数高效微调，以减少医学影像分析中的群体公平性泛化差距，提升模型公平性。",
    "keywords": [
      "公平性",
      "医学影像分析",
      "参数高效微调",
      "双层优化",
      "泛化差距",
      "群体偏见"
    ],
    "triple": {
      "method": "双层优化调整PEFT参数",
      "result": "提升多数据集公平性",
      "contribution": "提出FairTune框架"
    }
  },
  {
    "id": "92a29cbb9f1d",
    "venue": "ICLR",
    "year": 2024,
    "title": "FairSeg: A Large-Scale Medical Image Segmentation Dataset for Fairness Learning Using Segment Anything Model with Fair Error-Bound Scaling",
    "url": "https://proceedings.iclr.cc/paper_files/paper/2024/hash/bfdd520867b56af18dc9e80a8235b887-Abstract-Conference.html",
    "abstract": "Fairness in artificial intelligence models has gained significantly more attention in recent years, especially in the area of medicine, as fairness in medical models is critical to people's well-being and lives. High-quality medical fairness datasets are needed to promote fairness learning research. Existing medical fairness datasets are all for classification tasks, and no fairness datasets are available for medical segmentation, while medical segmentation is an equally important clinical task as classifications, which can provide detailed spatial information on organ abnormalities ready to be assessed by clinicians. In this paper, we propose the first fairness dataset for medical segmentation named Harvard-FairSeg with 10,000 subject samples. In addition, we propose a fair error-bound scaling approach to reweight the loss function with the upper error-bound in each identity group, using the segment anything model (SAM). We anticipate that the segmentation performance equity can be improved by explicitly tackling the hard cases with high training errors in each identity group. To facilitate fair comparisons, we utilize a novel equity-scaled segmentation performance metric to compare segmentation metrics in the context of fairness, such as the equity-scaled Dice coefficient. Through comprehensive experiments, we demonstrate that our fair error-bound scaling approach either has superior or comparable fairness performance to the state-of-the-art fairness learning models. The dataset and code are publicly accessible via https://ophai.hms.harvard.edu/datasets/harvard-fairseg10k.",
    "summary_cn": "提出首个医学分割公平性数据集Harvard-FairSeg及公平误差边界缩放方法，利用SAM模型提升分割性能公平性，优于现有方法。",
    "keywords": [
      "医学图像分割",
      "公平性学习",
      "Segment Anything Model",
      "误差边界缩放",
      "数据集",
      "性能公平性"
    ],
    "triple": {
      "method": "公平误差边界缩放与SAM模型",
      "result": "分割公平性优于或媲美现有方法",
      "contribution": "首个医学分割公平数据集与公平性提升框架"
    }
  },
  {
    "id": "20e5bf9a43e4",
    "venue": "ICLR",
    "year": 2025,
    "title": "Small Models are LLM Knowledge Triggers for Medical Tabular Prediction",
    "url": "https://iclr.cc/virtual/2025/poster/29343",
    "abstract": "Recent development in large language models (LLMs) has demonstrated impressive domain proficiency on unstructured textual or multi-modal tasks. However, despite with intrinsic world knowledge, their application on structured tabular data prediction still lags behind, primarily due to the numerical insensitivity and modality discrepancy that brings a gap between LLM reasoning and statistical tabular learning. Unlike textual or vision data (e.g., electronic clinical notes or medical imaging data), tabular data is often presented in heterogeneous numerical values (e.g., CBC reports). This ubiquitous data format requires intensive expert annotation, and its numerical nature limits LLMs' capability to effectively transfer untapped domain expertise. In this paper, we propose SERSAL, a general self-prompting method by synergy learning with small models to enhance LLM tabular prediction in an unsupervised manner. Specifically, SERSAL utilizes the LLM's prior outcomes as original soft noisy annotations, which are dynamically leveraged to teach a better small student model. Reversely, the outcomes from the trained small model are used to teach the LLM to further refine its real capability. This process can be repeatedly applied to gradually distill refined knowledge for continuous progress. Comprehensive experiments on widely used medical domain tabular datasets show that, without access to gold labels, applying SERSAL to OpenAI GPT reasoning process attains substantial improvement compared to linguistic prompting methods, which serves as an orthogonal direction for tabular LLM, and increasing prompting bonus is observed as more powerful LLMs appear. Codes are available at https://github.com/jyansir/sersal.",
    "summary_cn": "提出SERSAL方法，通过小模型与LLM协同学习，无监督提升LLM在医疗表格数据预测中的性能，实验显示显著优于语言提示方法。",
    "keywords": [
      "SERSAL",
      "LLM",
      "表格数据预测",
      "无监督学习",
      "协同学习",
      "医疗数据"
    ],
    "triple": {
      "method": "小模型与LLM协同自提示学习",
      "result": "显著提升LLM表格预测性能",
      "contribution": "提供无监督表格LLM新方向"
    }
  },
  {
    "id": "807037447531",
    "venue": "ICLR",
    "year": 2025,
    "title": "MediConfusion: Can you trust your AI radiologist? Probing the reliability of multimodal medical foundation models",
    "url": "https://iclr.cc/virtual/2025/poster/30242",
    "abstract": "Multimodal Large Language Models (MLLMs) have tremendous potential to improve the accuracy, availability, and cost-effectiveness of healthcare by providing automated solutions or serving as aids to medical professionals. Despite promising first steps in developing medical MLLMs in the past few years, their capabilities and limitations are not well understood. Recently, many benchmark datasets have been proposed that test the general medical knowledge of such models across a variety of medical areas. However, the systematic failure modes and vulnerabilities of such models are severely underexplored with most medical benchmarks failing to expose the shortcomings of existing models in this safety-critical domain. In this paper, we introduce MediConfusion, a challenging medical Visual Question Answering (VQA) benchmark dataset, that probes the failure modes of medical MLLMs from a vision perspective. We reveal that state-of-the-art models are easily confused by image pairs that are otherwise visually dissimilar and clearly distinct for medical experts. Strikingly, all available models (open-source or proprietary) achieve performance below random guessing on MediConfusion, raising serious concerns about the reliability of existing medical MLLMs for healthcare deployment. We also extract common patterns of model failure that may help the design of a new generation of more trustworthy and reliable MLLMs in healthcare.",
    "summary_cn": "提出MediConfusion医学VQA基准，揭示多模态大模型在视觉相似但医学不同的图像对上表现差，性能低于随机猜测，暴露其可靠性问题。",
    "keywords": [
      "多模态大语言模型",
      "医学视觉问答",
      "基准数据集",
      "模型可靠性",
      "失败模式",
      "医疗AI"
    ],
    "triple": {
      "method": "构建MediConfusion医学VQA基准数据集",
      "result": "所有模型性能低于随机猜测，易被视觉相似医学不同的图像混淆",
      "contribution": "揭示医疗MLLMs可靠性缺陷，为设计更可信模型提供见解"
    }
  },
  {
    "id": "d86b1e6c17fb",
    "venue": "ICLR",
    "year": 2025,
    "title": "Interpretable Bilingual Multimodal Large Language Model for Diverse Biomedical Tasks",
    "url": "https://proceedings.iclr.cc/paper_files/paper/2025/hash/414fd191b3246a19a55741b938380136-Abstract-Conference.html",
    "abstract": "Several medical Multimodal Large Languange Models (MLLMs) have been developed to address tasks involving visual images with textual instructions across various medical modalities, achieving impressive results. Most current medical generalist models are region-agnostic, treating the entire image as a holistic representation. However, they struggle to identify which specific regions they are focusing on when generating a sentence.To mimic the behavior of doctors, who typically begin by reviewing the entire image before concentrating on specific regions for a thorough evaluation, we aim to enhance the capability of medical MLLMs in understanding anatomical regions within entire medical scans.To achieve it, we first formulate \\textbf{Region-Centric tasks} and construct a \\textbf{large-scale dataset, MedRegInstruct,} to incorporate regional information into training. Combining our collected dataset with other medical multimodal corpora for training, we propose a \\textbf{Region-Aware medical MLLM, MedRegA}, which is the first bilingual generalist medical AI system to simultaneously handle image-level and region-level medical vision-language tasks across a broad range of modalities. Our MedRegA not only enables three region-centric tasks, but also achieves the best performance for visual question answering, report generation and medical image classification over 8 modalities, showcasing significant versatility. Experiments demonstrate that our model can not only accomplish powerful performance across various medical vision-language tasks in bilingual settings, but also recognize and detect structures in multimodal medical scans, boosting the interpretability and user interactivity of medical MLLMs. The codes and model will be made publicly available.",
    "summary_cn": "提出首个双语区域感知医学多模态大模型MedRegA，通过MedRegInstruct数据集增强区域理解，在多种医学视觉语言任务中表现优异，提升模型可解释性与交互性。",
    "keywords": [
      "医学多模态大模型",
      "区域感知",
      "双语处理",
      "可解释性",
      "MedRegInstruct数据集",
      "视觉语言任务"
    ],
    "triple": {
      "method": "构建MedRegInstruct数据集并训练区域感知模型",
      "result": "在8种模态的视觉问答、报告生成和分类任务中取得最佳性能",
      "contribution": "提升医学MLLMs的区域理解能力与可解释性"
    }
  },
  {
    "id": "327d91ccf4bf",
    "venue": "ICLR",
    "year": 2025,
    "title": "KGARevion: An AI Agent for Knowledge-Intensive Biomedical QA",
    "url": "https://iclr.cc/virtual/2025/poster/32054",
    "abstract": "Biomedical reasoning integrates structured, codified knowledge with tacit, experience-driven insights. Depending on the context, quantity, and nature of available evidence, researchers and clinicians use diverse strategies, including rule-based, prototype-based, and case-based reasoning. Effective medical AI models must handle this complexity while ensuring reliability and adaptability. We introduce KGARevion, a knowledge graph-based agent that answers knowledge-intensive questions. Upon receiving a query, KGARevion generates relevant triplets by leveraging the latent knowledge embedded in a large language model. It then verifies these triplets against a grounded knowledge graph, filtering out errors and retaining only accurate, contextually relevant information for the final answer. This multi-step process strengthens reasoning, adapts to different models of medical inference, and outperforms retrieval-augmented generation-based approaches that lack effective verification mechanisms. Evaluations on medical QA benchmarks show that KGARevion improves accuracy by over 5.2% over 15 models in handling complex medical queries. To further assess its effectiveness, we curated three new medical QA datasets with varying levels of semantic complexity, where KGARevion improved accuracy by 10.4%. The agent integrates with different LLMs and biomedical knowledge graphs for broad applicability across knowledge-intensive tasks. We evaluated KGARevion on AfriMed-QA, a newly introduced dataset focused on African healthcare, demonstrating its strong zero-shot generalization to underrepresented medical contexts.",
    "summary_cn": "KGARevion是基于知识图谱的AI代理，通过生成并验证三元组来回答生物医学问题，提升复杂查询的准确性和推理能力。",
    "keywords": [
      "知识图谱",
      "生物医学问答",
      "AI代理",
      "三元组验证",
      "推理增强",
      "零样本泛化"
    ],
    "triple": {
      "method": "生成并验证三元组",
      "result": "准确率提升5.2%-10.4%",
      "contribution": "增强推理与泛化能力"
    }
  },
  {
    "id": "31317d0c8eb9",
    "venue": "ICLR",
    "year": 2024,
    "title": "Hallucination Benchmark in Medical Visual Question Answering",
    "url": "https://iclr.cc/virtual/2024/20859",
    "abstract": "The recent success of large language and vision models on vision question answering (VQA), particularly their applications in medicine (Med-VQA), has shown a great potential of realizing effective visual assistants for healthcare. However, these models are not extensively tested on the hallucination phenomenon in clinical settings. Here, we created a hallucination benchmark of medical images paired with question-answer sets and conducted a comprehensive evaluation of the state-of-the-art models. The study provides an in-depth analysis of current models' limitations and reveals the effectiveness of various prompting strategies.",
    "summary_cn": "研究构建了医学视觉问答的幻觉基准，评估了前沿模型在临床环境中的局限性，并分析了不同提示策略的效果。",
    "keywords": [
      "医学视觉问答",
      "幻觉基准",
      "模型评估",
      "提示策略",
      "临床应用",
      "大型模型"
    ],
    "triple": {
      "method": "构建幻觉基准并评估模型",
      "result": "揭示模型局限性及提示策略效果",
      "contribution": "提供医学VQA幻觉分析基准"
    }
  },
  {
    "id": "58b6c125547b",
    "venue": "ICLR",
    "year": 2025,
    "title": "Efficiently Democratizing Medical LLMs for 50 Languages via a Mixture of Language Family Experts",
    "url": "https://iclr.cc/virtual/2025/poster/30111",
    "abstract": "Adapting medical Large  Language Models to local languages can reduce barriers to accessing healthcare services, but data scarcity remains a significant challenge, particularly for low-resource languages. To address this, we first construct a high-quality medical dataset and conduct analysis to ensure its quality. In order to leverage the generalization capability of multilingual LLMs to efficiently scale to more resource-constrained languages, we explore the internal information flow of LLMs from a multilingual perspective using Mixture of Experts (MoE) modularity. Technically, we propose a novel MoE routing method that employs language-specific experts and cross-lingual routing. Inspired by circuit theory, our routing analysis revealed a \\textit{ Spread Out in the End } information flow mechanism: while earlier layers concentrate cross-lingual information flow, the later layers exhibit language-specific divergence. This insight directly led to the development of the Post-MoE architecture, which applies sparse routing only in the later layers while maintaining dense others. Experimental results demonstrate that this approach enhances the generalization of multilingual models to other languages while preserving interpretability. Finally, to efficiently scale the model to 50 languages, we introduce the concept of \\textit{language family} experts, drawing on linguistic priors, which enables scaling the number of languages without adding additional parameters.",
    "summary_cn": "提出基于语言家族专家的MoE路由方法，构建高质量医疗数据集，实现50种语言的高效多语言医疗LLM，提升泛化能力并保持可解释性。",
    "keywords": [
      "多语言医疗LLM",
      "混合专家",
      "语言家族专家",
      "路由分析",
      "泛化能力",
      "可解释性"
    ],
    "triple": {
      "method": "基于语言家族专家的MoE路由与Post-MoE架构",
      "result": "增强多语言模型泛化至50种语言，保持可解释性",
      "contribution": "高效民主化医疗LLM，解决低资源语言数据稀缺问题"
    }
  },
  {
    "id": "e558cce9cb84",
    "venue": "ICLR",
    "year": 2025,
    "title": "Scale-Aware Contrastive Reverse Distillation for Unsupervised Medical Anomaly Detection",
    "url": "https://iclr.cc/virtual/2025/poster/30228",
    "abstract": "Unsupervised anomaly detection using deep learning has garnered significant research attention due to its broad applicability, particularly in medical imaging where labeled anomalous data are scarce. While earlier approaches leverage generative models like autoencoders and generative adversarial networks (GANs), they often fall short due to overgeneralization. Recent methods explore various strategies, including memory banks, normalizing flows, self-supervised learning, and knowledge distillation, to enhance discrimination. Among these, knowledge distillation, particularly reverse distillation, has shown promise. Following this paradigm, we propose a novel scale-aware contrastive reverse distillation model that addresses two key limitations of existing reverse distillation methods: insufficient feature discriminability and inability to handle anomaly scale variations. Specifically, we introduce a contrastive student-teacher learning approach to derive more discriminative representations by generating and exploring out-of-normal distributions. Further, we design a scale adaptation mechanism to softly weight contrastive distillation losses at different scales to account for the scale variation issue. Extensive experiments on benchmark datasets demonstrate state-of-the-art performance, validating the efficacy of the proposed method. The code will be made publicly available.",
    "summary_cn": "提出一种尺度感知对比反向蒸馏模型，通过对比师生学习和尺度适应机制，提升无监督医学异常检测性能，在基准数据集上达到最优效果。",
    "keywords": [
      "无监督异常检测",
      "反向蒸馏",
      "对比学习",
      "尺度适应",
      "医学影像",
      "知识蒸馏"
    ],
    "triple": {
      "method": "尺度感知对比反向蒸馏",
      "result": "在基准数据集上实现最优性能",
      "contribution": "提升特征判别力并处理异常尺度变化"
    }
  },
  {
    "id": "7ffecb2f6f94",
    "venue": "ICLR",
    "year": 2025,
    "title": "Uncertainty of Vision Medical Foundation Models",
    "url": "https://iclr.cc/virtual/2025/32883",
    "abstract": "Accurate uncertainty estimation is essential for machine learning systems deployed in high-stakes domains such as medicine. Traditional approaches primarily rely on probability outputs from trained models (point predictions), which provide no formal guarantees on prediction coverage and often require additional calibration techniques to improve reliability. In contrast, conformal prediction (region prediction) offers a principled alternative by generating prediction sets with finite-sample validity guarantees, ensuring that the ground truth is contained within the set at a specified confidence level.In this study, we explore the impact of pre-training approach, dataset scale and domain on both point and region-level uncertainty quantification, by studying domain-specific vision medical foundation models vs. general domain vision foundation models. We conduct a comprehensive evaluation across foundation models trained on retinal, histopathological, and magnetic resonance imaging (MRI) data, applying various calibration techniques. Our results demonstrate that (1) pre-training on larger, higher-quality datasets along with self-supervised learning leads to better-calibrated point predictions, irrespective of whether the data source is domain-specific, (2) better point prediction calibration does not directly translate to improved region prediction performance, (3) standard re-calibration methods alone cannot fully mitigate uncertainty discrepancies across models trained on different data sources.These findings highlight the importance of careful model selection and the integration of both point and region prediction strategies to enhance the reliability and trustworthiness of medical AI systems. Our work underscores the need for a holistic approach to uncertainty quantification in recent development of medical vision foundation model, ensuring robust and interpretable AI-driven decision-making.",
    "summary_cn": "研究探索预训练方法、数据规模和领域对医学视觉基础模型不确定性估计的影响，发现更大高质量数据集和自监督学习能改善点预测校准，但需结合点与区域预测策略提升AI可靠性。",
    "keywords": [
      "不确定性估计",
      "医学视觉基础模型",
      "保形预测",
      "校准技术",
      "自监督学习",
      "区域预测"
    ],
    "triple": {
      "method": "评估不同预训练模型与校准技术",
      "result": "点预测校准不直接改善区域预测，标准方法无法完全消除不确定性差异",
      "contribution": "强调结合点与区域预测策略，提升医学AI系统可靠性"
    }
  },
  {
    "id": "6365fd9f4b36",
    "venue": "ICLR",
    "year": 2024,
    "title": "MedAgents: Large Language Models as Collaborators for Zero-shot Medical Reasoning",
    "url": "https://iclr.cc/virtual/2024/22218",
    "abstract": "Large Language Models (LLMs), despite their remarkable progress across various general domains, encounter significant barriers in medicine and healthcare. This field faces unique challenges such as domain-specific terminologies and reasoning over specialized knowledge. To address these issues, we propose a novel Multi-disciplinary Collaboration (MC) framework for the medical domain that leverages role-playing LLM-based agents who participate in a collaborative multi-round discussion, thereby enhancing LLM proficiency and reasoning capabilities. This training-free and interpretable framework encompasses five critical steps: gathering domain experts, proposing individual analyses, summarising these analyses into a report, iterating over discussions until a consensus is reached, and ultimately making a decision. Our work focuses on the zero-shot setting, which is applicable in real-world scenarios. Experimental results on nine datasets (MedQA, MedMCQA, PubMedQA, and six subtasks from MMLU) establish that our proposed MC framework excels at mining and harnessing the medical expertise within LLMs, as well as extending its reasoning abilities.",
    "summary_cn": "提出多学科协作框架，利用角色扮演LLM代理进行多轮讨论，提升零样本医学推理能力，在九个数据集上验证有效性。",
    "keywords": [
      "大型语言模型",
      "医学推理",
      "零样本学习",
      "多学科协作",
      "角色扮演代理",
      "框架设计"
    ],
    "triple": {
      "method": "多学科协作框架与角色扮演代理",
      "result": "在九个医学数据集上提升性能",
      "contribution": "增强LLM医学专业知识挖掘与推理能力"
    }
  },
  {
    "id": "0730257ecdce",
    "venue": "ICLR",
    "year": 2024,
    "title": "The Effect of Intrinsic Dataset Properties on Generalization: Unraveling Learning Differences Between Natural and Medical Images",
    "url": "https://proceedings.iclr.cc/paper_files/paper/2024/hash/5ef6b140c8b6cc2904854e30c6db78eb-Abstract-Conference.html",
    "abstract": "This paper investigates discrepancies in how neural networks learn from different imaging domains, which are commonly overlooked when adopting computer vision techniques from the domain of natural images to other specialized domains such as medical images. Recent works have found that the generalization error of a trained network typically increases with the intrinsic dimension ($d_{data}$) of its training set. Yet, the steepness of this relationship varies significantly between medical (radiological) and natural imaging domains, with no existing theoretical explanation. We address this gap in knowledge by establishing and empirically validating a generalization scaling law with respect to $d_{data}$, and propose that the substantial scaling discrepancy between the two considered domains may be at least partially attributed to the higher intrinsic ``label sharpness'' ($K_\\mathcal{F}$) of medical imaging datasets, a metric which we propose. Next, we demonstrate an additional benefit of measuring the label sharpness of a training set: it is negatively correlated with the trained model's adversarial robustness, which notably leads to models for medical images having a substantially higher vulnerability to adversarial attack. Finally, we extend our $d_{data}$ formalism to the related metric of learned representation intrinsic dimension ($d_{repr}$), derive a generalization scaling law with respect to $d_{repr}$, and show that $d_{data}$ serves as an upper bound for $d_{repr}$. Our theoretical results are supported by thorough experiments with six models and eleven natural and medical imaging datasets over a range of training set sizes. Our findings offer insights into the influence of intrinsic dataset properties on generalization, representation learning, and robustness in deep neural networks. *Code link: https://github.com/mazurowski-lab/intrinsic-properties*",
    "summary_cn": "研究神经网络在自然与医学图像学习中的差异，提出标签锐度解释泛化差距，并揭示医学图像对抗脆弱性更高。",
    "keywords": [
      "泛化误差",
      "内在维度",
      "标签锐度",
      "对抗鲁棒性",
      "医学图像",
      "神经网络"
    ],
    "triple": {
      "method": "提出标签锐度度量与泛化缩放定律",
      "result": "医学图像标签锐度高导致泛化差距大且对抗脆弱性强",
      "contribution": "揭示数据集内在属性对泛化与鲁棒性的影响"
    }
  },
  {
    "id": "b0cb0b995a81",
    "venue": "ICLR",
    "year": 2025,
    "title": "Large-scale and Fine-grained Vision-language Pre-training for Enhanced CT Image Understanding",
    "url": "https://iclr.cc/virtual/2025/poster/28403",
    "abstract": "Artificial intelligence (AI) shows great potential in assisting radiologists to improve the efficiency and accuracy of medical image interpretation and diagnosis. However, a versatile AI model requires large-scale data and comprehensive annotations, which are often impractical in medical settings. Recent studies leverage radiology reports as a naturally high-quality supervision for medical images, using contrastive language-image pre-training (CLIP) to develop language-informed models for radiological image interpretation. Nonetheless, these approaches typically contrast entire images with reports, neglecting the local associations between imaging regions and report sentences, which may undermine model performance and interoperability. In this paper, we propose a fine-grained vision-language model (fVLM) for anatomy-level CT image interpretation. Specifically, we explicitly match anatomical regions of CT images with corresponding descriptions in radiology reports and perform contrastive pre-training for each anatomy individually. Fine-grained alignment, however, faces considerable false-negative challenges, mainly from the abundance of anatomy-level healthy samples and similarly diseased abnormalities, leading to ambiguous patient-level pairings. To tackle this issue, we propose identifying false negatives of both normal and abnormal samples and calibrating contrastive learning from patient-level to disease-aware pairing. We curated the largest CT dataset to date, comprising imaging and report data from 69,086 patients, and conducted a comprehensive evaluation of 54 major and important disease (including several most deadly cancers) diagnosis tasks across 15 main anatomies. Experimental results demonstrate the substantial potential of fVLM in versatile medical image interpretation. In the zero-shot classification task, we achieved an average AUC of 81.3% on 54 diagnosis tasks, surpassing CLIP and supervised methods by 12.9% and 8.0%, respectively. Additionally, on the publicly available CT-RATE and Rad-ChestCT benchmarks, our fVLM outperformed the current state-of-the-art methods with absolute AUC gains of 7.4% and 4.8%, respectively.",
    "summary_cn": "提出细粒度视觉语言模型fVLM，通过解剖区域与报告句子对齐及假阴性校准，在大型CT数据集上实现零样本诊断性能显著提升。",
    "keywords": [
      "细粒度视觉语言模型",
      "CT图像理解",
      "对比学习",
      "解剖区域对齐",
      "零样本诊断",
      "假阴性校准"
    ],
    "triple": {
      "method": "解剖区域与报告句子对齐的对比预训练",
      "result": "零样本诊断AUC达81.3%，超越CLIP及监督方法",
      "contribution": "提升CT图像理解的细粒度对齐与泛化能力"
    }
  },
  {
    "id": "b1666a083690",
    "venue": "ICLR",
    "year": 2024,
    "title": "FairSeg: A Large-Scale Medical Image Segmentation Dataset for Fairness Learning Using Segment Anything Model with Fair Error-Bound Scaling",
    "url": "https://iclr.cc/virtual/2024/poster/17735",
    "abstract": "Fairness in artificial intelligence models has gained significantly more attention in recent years, especially in the area of medicine, as fairness in medical models is critical to people's well-being and lives. High-quality medical fairness datasets are needed to promote fairness learning research. Existing medical fairness datasets are all for classification tasks, and no fairness datasets are available for medical segmentation, while medical segmentation is an equally important clinical task as classifications, which can provide detailed spatial information on organ abnormalities ready to be assessed by clinicians. In this paper, we propose the first fairness dataset for medical segmentation named Harvard-FairSeg with 10,000 subject samples. In addition, we propose a fair error-bound scaling approach to reweight the loss function with the upper error-bound in each identity group, using the segment anything model (SAM). We anticipate that the segmentation performance equity can be improved by explicitly tackling the hard cases with high training errors in each identity group. To facilitate fair comparisons, we utilize a novel equity-scaled segmentation performance metric to compare segmentation metrics in the context of fairness, such as the equity-scaled Dice coefficient. Through comprehensive experiments, we demonstrate that our fair error-bound scaling approach either has superior or comparable fairness performance to the state-of-the-art fairness learning models. The dataset and code are publicly accessible via https://ophai.hms.harvard.edu/datasets/harvard-fairseg10k.",
    "summary_cn": "提出首个医学分割公平性数据集Harvard-FairSeg，包含1万样本，并基于SAM模型提出公平误差边界缩放方法，提升分割性能公平性。",
    "keywords": [
      "医学图像分割",
      "公平性学习",
      "数据集",
      "Segment Anything Model",
      "误差边界缩放",
      "性能公平性"
    ],
    "triple": {
      "method": "基于SAM的公平误差边界缩放方法",
      "result": "在公平性性能上优于或媲美现有模型",
      "contribution": "首创医学分割公平性数据集与公平性优化方法"
    }
  },
  {
    "id": "7afd9c0e050c",
    "venue": "ICLR",
    "year": 2025,
    "title": "Interpretable Bilingual Multimodal Large Language Model for Diverse Biomedical Tasks",
    "url": "https://iclr.cc/virtual/2025/poster/32076",
    "abstract": "Several medical Multimodal Large Languange Models (MLLMs) have been developed to address tasks involving visual images with textual instructions across various medical modalities, achieving impressive results. Most current medical generalist models are region-agnostic, treating the entire image as a holistic representation. However, they struggle to identify which specific regions they are focusing on when generating a sentence.To mimic the behavior of doctors, who typically begin by reviewing the entire image before concentrating on specific regions for a thorough evaluation, we aim to enhance the capability of medical MLLMs in understanding anatomical regions within entire medical scans.To achieve it, we first formulate \\textbf{Region-Centric tasks} and construct a \\textbf{large-scale dataset, MedRegInstruct,} to incorporate regional information into training. Combining our collected dataset with other medical multimodal corpora for training, we propose a \\textbf{Region-Aware medical MLLM, MedRegA}, which is the first bilingual generalist medical AI system to simultaneously handle image-level and region-level medical vision-language tasks across a broad range of modalities. Our MedRegA not only enables three region-centric tasks, but also achieves the best performance for visual question answering, report generation and medical image classification over 8 modalities, showcasing significant versatility. Experiments demonstrate that our model can not only accomplish powerful performance across various medical vision-language tasks in bilingual settings, but also recognize and detect structures in multimodal medical scans, boosting the interpretability and user interactivity of medical MLLMs. The codes and model will be made publicly available.",
    "summary_cn": "提出首个双语区域感知医学多模态大模型MedRegA，通过MedRegInstruct数据集增强区域理解，在多种医学视觉语言任务中表现优异，提升模型可解释性与交互性。",
    "keywords": [
      "医学多模态大模型",
      "区域感知",
      "双语处理",
      "可解释性",
      "MedRegInstruct数据集",
      "视觉语言任务"
    ],
    "triple": {
      "method": "构建MedRegInstruct数据集训练区域感知模型",
      "result": "在8种模态的视觉问答、报告生成等任务中取得最佳性能",
      "contribution": "提升医学MLLMs的区域理解与可解释性"
    }
  },
  {
    "id": "6d06f6aa95d7",
    "venue": "ICLR",
    "year": 2025,
    "title": "Prompt as Knowledge Bank: Boost Vision-language model via Structural Representation for  zero-shot medical detection",
    "url": "https://proceedings.iclr.cc/paper_files/paper/2025/hash/59a9cc95f046e9125d8816ef971873e7-Abstract-Conference.html",
    "abstract": "Zero-shot medical detection can further improve detection performance without relying on annotated medical images even upon the fine-tuned model, showing great clinical value. Recent studies leverage grounded vision-language models (GLIP) to achieve this by using detailed disease descriptions as prompts for the target disease name during the inference phase.  However, these methods typically treat prompts as equivalent context to the target name, making it difficult to assign specific disease knowledge based on visual information, leading to a coarse alignment between images and target descriptions. In this paper, we propose StructuralGLIP, which introduces an auxiliary branch to encode prompts into a latent knowledge bank layer-by-layer, enabling more context-aware and fine-grained alignment. Specifically, in each layer, we select highly similar features from both the image representation and the knowledge bank, forming structural representations that capture nuanced relationships between image patches and target descriptions. These features are then fused across modalities to further enhance detection performance.Extensive experiments demonstrate that StructuralGLIP achieves a +4.1\\% AP improvement over prior state-of-the-art methods across seven zero-shot medical detection benchmarks, and consistently improves fine-tuned models by +3.2\\% AP on endoscopy image datasets.",
    "summary_cn": "提出StructuralGLIP，通过分层编码提示构建知识库，实现图像与疾病描述的细粒度对齐，提升零样本医学检测性能。",
    "keywords": [
      "零样本医学检测",
      "视觉语言模型",
      "细粒度对齐",
      "知识库",
      "结构表示",
      "GLIP"
    ],
    "triple": {
      "method": "分层编码提示构建知识库，选择高相似特征形成结构表示",
      "result": "在七个基准上AP提升4.1%，内窥镜数据集上微调模型AP提升3.2%",
      "contribution": "增强图像与疾病描述的细粒度对齐，提升零样本检测性能"
    }
  },
  {
    "id": "79feccfa7820",
    "venue": "ICLR",
    "year": 2024,
    "title": "LLM-CXR: Instruction-Finetuned LLM for CXR Image Understanding and Generation",
    "url": "https://proceedings.iclr.cc/paper_files/paper/2024/hash/7f70331dbe58ad59d83941dfa7d975aa-Abstract-Conference.html",
    "abstract": "Following the impressive development of LLMs, vision-language alignment in LLMs is actively being researched to enable multimodal reasoning and visual input/output. This direction of research is particularly relevant to medical imaging because accurate medical image analysis and generation consist of a combination of reasoning based on visual features and prior knowledge. Many recent works have focused on training adapter networks that serve as an information bridge between image processing (encoding or generating) networks and LLMs; but presumably, in order to achieve maximum reasoning potential of LLMs on visual information as well, visual and language features should be allowed to interact more freely. This is especially important in the medical domain because understanding and generating medical images such as chest X-rays (CXR) require not only accurate visual and language-based reasoning but also a more intimate mapping between the two modalities. Thus, taking inspiration from previous work on the transformer and VQ-GAN combination for bidirectional image and text generation, we build upon this approach and develop a method for instruction-tuning an LLM pre-trained only on text to gain vision-language capabilities for medical images. Specifically, we leverage a pretrained LLM’s existing question-answering and instruction-following abilities to teach it to understand visual inputs by instructing it to answer questions about image inputs and, symmetrically, output both text and image responses appropriate to a given query by tuning the LLM with diverse tasks that encompass image-based text-generation and text-based image-generation. We show that our LLM-CXR trained in this approach shows better image-text alignment in both CXR understanding and generation tasks while being smaller in size compared to previously developed models that perform a narrower range of tasks.",
    "summary_cn": "本研究开发了LLM-CXR模型，通过指令微调使文本预训练大语言模型具备医学影像理解与生成能力，在胸部X光任务中实现更好的图文对齐。",
    "keywords": [
      "大语言模型",
      "医学影像",
      "指令微调",
      "胸部X光",
      "多模态学习",
      "图像生成"
    ],
    "triple": {
      "method": "指令微调预训练LLM",
      "result": "提升CXR图文对齐能力",
      "contribution": "开发轻量多功能医学多模态模型"
    }
  },
  {
    "id": "7df20a2d1505",
    "venue": "ICLR",
    "year": 2025,
    "title": "Dynamic Modeling of Patients, Modalities and Tasks via Multi-modal Multi-task Mixture of Experts",
    "url": "https://proceedings.iclr.cc/paper_files/paper/2025/hash/e3b82f4c7ba93a88025cf97dca9edc83-Abstract-Conference.html",
    "abstract": "Multi-modal multi-task learning holds significant promise in tackling complex diagnostic tasks and many significant medical imaging problems. It fulfills the needs in real-world diagnosis protocol to leverage information from different data sources and simultaneously perform mutually informative tasks. However, medical imaging domains introduce two key challenges: dynamic modality fusion and modality-task dependence. The quality and amount of task-related information from different modalities could vary significantly across patient samples, due to biological and demographic factors. Traditional fusion methods apply fixed combination strategies that fail to capture this dynamic relationship, potentially underutilizing modalities that carry stronger diagnostic signals for specific patients. Additionally, different clinical tasks may require dynamic feature selection and combination from various modalities, a phenomenon we term “modality-task dependence.” To address these issues, we propose M4oE, a novel Multi-modal Multi-task Mixture of Experts framework for precise Medical diagnosis. M4oE comprises Modality-Specific (MSoE) modules and a Modality-shared Modality-Task MoE (MToE) module. With collaboration from both modules, our model dynamically decomposes and learns distinct and shared information from different modalities and achieves dynamic fusion. MToE provides a joint probability model of modalities and tasks by using experts as a link and encourages experts to learn modality-task dependence via conditional mutual information loss. By doing so, M4oE offers sample and population-level interpretability of modality contributions. We evaluate M4oE on four public multi-modal medical benchmark datasets for solving two important medical diagnostic problems including breast cancer screening and retinal disease diagnosis. Results demonstrate our method's superiority over state-of-the-art methods under different metrics of classification and segmentation tasks like Accuracy, AUROC, AUPRC, and DICE.",
    "summary_cn": "提出M4oE框架，通过多模态多任务混合专家模型动态融合模态与任务信息，提升乳腺癌筛查和视网膜疾病诊断的准确性。",
    "keywords": [
      "多模态学习",
      "多任务学习",
      "混合专家模型",
      "动态融合",
      "医学诊断",
      "模态任务依赖"
    ],
    "triple": {
      "method": "多模态多任务混合专家框架（M4oE）",
      "result": "在分类和分割任务中超越现有方法",
      "contribution": "实现动态模态融合与模态任务依赖建模"
    }
  },
  {
    "id": "c11be9e6b738",
    "venue": "ICLR",
    "year": 2025,
    "title": "Synthetic Poisoning Attacks: The Impact of Poisoned MRI Image on U-Net Brain Tumor Segmentation",
    "url": "https://iclr.cc/virtual/2025/32191",
    "abstract": "Deep learning-based medical image segmentation models, such as U-Net, rely on high-quality annotated datasets to achieve accurate predictions. However, the increasing use of generative models for synthetic data augmentation introduces potential risks, particularly in the absence of rigorous quality control. In this paper, we investigate the impact of synthetic MRI data on the robustness and segmentation accuracy of U-Net models for brain tumor segmentation. Specifically, we generate synthetic T1-contrast-enhanced (T1-Ce) MRI scans using a GAN-based model with a shared encoding-decoding framework and shortest-path regularization. To quantify the effect of synthetic data contamination, we train U-Net models on progressively ``poisoned'' datasets, where synthetic data proportions range from 16.67\\% to 83.33\\%. Experimental results on a real MRI validation set reveal a significant performance degradation as synthetic data increases, with Dice coefficients dropping from 0.8937 (33.33\\% synthetic) to 0.7474 (83.33\\% synthetic). Accuracy and sensitivity exhibit similar downward trends, demonstrating the detrimental effect of synthetic data on segmentation robustness. These findings underscore the importance of quality control in synthetic data integration and highlight the risks of unregulated synthetic augmentation in medical image analysis. Our study provides critical insights for the development of more reliable and trustworthy AI-driven medical imaging systems.",
    "summary_cn": "研究合成MRI数据对U-Net脑肿瘤分割模型的影响，发现合成数据比例增加会导致分割性能显著下降，强调合成数据集成中质量控制的重要性。",
    "keywords": [
      "合成数据攻击",
      "U-Net分割",
      "脑肿瘤分割",
      "MRI图像",
      "生成对抗网络",
      "数据污染"
    ],
    "triple": {
      "method": "使用GAN生成合成MRI数据，逐步污染训练集",
      "result": "合成数据比例增加导致Dice系数从0.8937降至0.7474",
      "contribution": "揭示合成数据对分割模型的负面影响，强调质量控制"
    }
  },
  {
    "id": "e1304836cf3c",
    "venue": "ICLR",
    "year": 2024,
    "title": "Analog In-Memory Computing with Uncertainty Quantification for Efficient Edge-based Medical Imaging Segmentation",
    "url": "https://iclr.cc/virtual/2024/20891",
    "abstract": "This work investigates the role of the emerging Analog In-memory computing (AIMC) paradigm in enabling Medical AI analysis and improving the certainty of these models at the edge. It contrasts AIMC's efficiency with traditional digital computing's limitations in power, speed, and scalability. Our comprehensive evaluation focuses on brain tumor analysis, spleen segmentation, and nuclei detection. The study highlights the superior robustness of isotropic architectures, which exhibit a minimal accuracy drop (0.04) in analog-aware training, compared to significant drops (up to 0.15) in pyramidal structures. Additionally, the paper emphasizes IMC's effective data pipelining, reducing latency and increasing throughput as well as the exploitation of inherent noise within AIMC, strategically harnessed to augment model certainty.",
    "summary_cn": "研究模拟内存计算在边缘医疗影像分割中的应用，通过各向同性架构提升模型鲁棒性，利用固有噪声增强不确定性量化，实现高效低延迟分析。",
    "keywords": [
      "模拟内存计算",
      "边缘计算",
      "医疗影像分割",
      "不确定性量化",
      "各向同性架构",
      "模型鲁棒性"
    ],
    "triple": {
      "method": "模拟内存计算与各向同性架构",
      "result": "精度下降最小（0.04），延迟降低，吞吐量增加",
      "contribution": "提升边缘医疗AI效率与确定性"
    }
  },
  {
    "id": "86f00b1f2dcb",
    "venue": "ICLR",
    "year": 2025,
    "title": "Can Medical Vision-Language Pre-training Succeed with Purely Synthetic Data?",
    "url": "https://iclr.cc/virtual/2025/32770",
    "abstract": "Medical Vision-Language Pre-training (MedVLP) has made significant progress in enabling zero-shot tasks for medical image understanding. However, training MedVLP models typically requires large-scale datasets with paired, high-quality image-text data, which are scarce in the medical domain. Recent advancements in Large Language Models (LLMs) and diffusion models have made it possible to generate large-scale synthetic image-text pairs. This raises the question: Can MedVLP succeed using purely synthetic data? To address this, we use off-the-shelf generative models to create synthetic radiology reports and paired Chest X-ray (CXR) images, and propose an automated pipeline to build a diverse, high-quality synthetic dataset, enabling a rigorous study that isolates model and training settings, focusing entirely from the data perspective.Our results show that MedVLP models trained exclusively on synthetic data outperform those trained on real data by 3.8% in averaged AUC on zero-shot classification. Moreover, using a combination of synthetic and real data leads to a further improvement of 9.07% . Additionally, MedVLP models trained on synthetic or mixed data consistently outperform those trained on real data in zero-shot grounding, as well as in fine-tuned classification and segmentation tasks.Our analysis suggests MedVLP trained on well-designed synthetic data can outperform models trained on real datasets, which may be limited by low-quality samples and long-tailed distributions.",
    "summary_cn": "研究探讨仅用合成数据训练医学视觉语言预训练模型的可行性。结果显示，纯合成数据训练的模型在零样本分类上优于真实数据模型，混合数据进一步提升性能。",
    "keywords": [
      "医学视觉语言预训练",
      "合成数据",
      "零样本分类",
      "胸部X光",
      "生成模型",
      "数据增强"
    ],
    "triple": {
      "method": "使用生成模型创建合成胸部X光图像与报告对",
      "result": "纯合成数据训练模型在零样本分类AUC上优于真实数据3.8%，混合数据提升9.07%",
      "contribution": "证明高质量合成数据可克服真实数据限制，提升医学视觉语言模型性能"
    }
  },
  {
    "id": "03cb08a55e50",
    "venue": "ICLR",
    "year": 2025,
    "title": "Reasoning-Enhanced Healthcare Predictions with Knowledge Graph Community Retrieval",
    "url": "https://proceedings.iclr.cc/paper_files/paper/2025/hash/cb5a3b4589c1a16f14740396625cbfc8-Abstract-Conference.html",
    "abstract": "Large language models (LLMs) have demonstrated significant potential in clinical decision support. Yet LLMs still suffer from hallucinations and lack fine-grained contextual medical knowledge, limiting their high-stake healthcare applications such as clinical diagnosis. Traditional retrieval-augmented generation (RAG) methods attempt to address these limitations but frequently retrieve sparse or irrelevant information, undermining prediction accuracy. We introduce KARE, a novel framework that integrates knowledge graph (KG) community-level retrieval with LLM reasoning to enhance healthcare predictions. KARE constructs a comprehensive multi-source KG by integrating biomedical databases, clinical literature, and LLM-generated insights, and organizes it using hierarchical graph community detection and summarization for precise and contextually relevant information retrieval. Our key innovations include: (1) a dense medical knowledge structuring approach enabling accurate retrieval of relevant information; (2) a dynamic knowledge retrieval mechanism that enriches patient contexts with focused, multi-faceted medical insights; and (3) a reasoning-enhanced prediction framework that leverages these enriched contexts to produce both accurate and interpretable clinical predictions. Extensive experiments demonstrate that KARE outperforms leading models by up to 10.8-15.0\\% on MIMIC-III and 12.6-12.7\\% on MIMIC-IV for mortality and readmission predictions. In addition to its impressive prediction accuracy, our framework leverages the reasoning capabilities of LLMs, enhancing the trustworthiness of clinical predictions.",
    "summary_cn": "提出KARE框架，结合知识图谱社区检索与LLM推理，提升医疗预测准确性，在MIMIC数据集上表现优异。",
    "keywords": [
      "知识图谱",
      "检索增强生成",
      "临床预测",
      "大语言模型",
      "社区检测",
      "医疗推理"
    ],
    "triple": {
      "method": "知识图谱社区检索与LLM推理结合",
      "result": "预测准确率提升10.8-15.0%",
      "contribution": "增强临床预测准确性与可解释性"
    }
  },
  {
    "id": "20a389792a1f",
    "venue": "ICLR",
    "year": 2024,
    "title": "The Effect of Intrinsic Dataset Properties on Generalization: Unraveling Learning Differences Between Natural and Medical Images",
    "url": "https://iclr.cc/virtual/2024/poster/18044",
    "abstract": "The ICLR Logo above may be used on presentations. Right-click and choose\n                            download. It is a vector graphic and may be used at any scale.",
    "summary_cn": "研究自然与医学图像数据集内在属性对模型泛化能力的影响，揭示两者学习差异。",
    "keywords": [
      "医学图像",
      "自然图像",
      "泛化能力",
      "数据集属性",
      "机器学习",
      "深度学习"
    ],
    "triple": {
      "method": "分析数据集内在属性",
      "result": "揭示自然与医学图像学习差异",
      "contribution": "提升医学图像模型泛化理解"
    }
  },
  {
    "id": "bed044b22f24",
    "venue": "ICLR",
    "year": 2025,
    "title": "BoneMet: An Open Large-Scale Multi-Modal Murine Dataset for Breast Cancer Bone Metastasis Diagnosis and Prognosis",
    "url": "https://proceedings.iclr.cc/paper_files/paper/2025/hash/25dbf5a09fbf2553739a04645d3dfb2c-Abstract-Conference.html",
    "abstract": "Breast cancer bone metastasis (BCBM) affects women’s health globally, calling for the development of effective diagnosis and prognosis solutions. While deep learning has exhibited impressive capacities across various healthcare domains, its applicability in BCBM diseases is consistently hindered by the lack of an open, large-scale, deep learning-ready dataset. As such, we introduce the Bone Metastasis (BoneMet) dataset, the first large-scale, publicly available, high-resolution medical resource, which is derived from a well-accepted murine BCBM model. The unique advantage of BoneMet over existing human datasets is repeated sequential scans per subject over the entire disease development phases. The dataset consists of over 67 terabytes of multi-modal medical data, including 2D X-ray images, 3D CT scans, and detailed biological data (e.g., medical records and bone quantitative analysis), collected from more than five hundreds mice spanning from 2019 to 2024. Our BoneMet dataset is well-organized into six components, i.e., RotationX-Ray, Recon-CT, Seg-CT, Regist-CT, RoI-CT, and MiceMediRec. We further show that BoneMet can be readily adopted to build versatile, large-scale AI models for managing BCBM diseases in terms of diagnosis using 2D or 3D images, prognosis of bone deterioration, and sparse-angle 3D reconstruction for safe long-term disease monitoring. Our preliminary results demonstrate that BoneMet has the potentials to jump-start the development and fine-tuning of AI-driven solutions prior to their applications to human patients. To facilitate its easy access and wide dissemination, we have created the BoneMet package, providing three APIs that enable researchers to (i) flexibly process and download the BoneMet data filtered by specific time frames; and (ii) develop and train large-scale AI models for precise BCBM diagnosis and prognosis. The BoneMet dataset is officially available on Hugging Face Datasets at https://huggingface.co/datasets/BoneMet/BoneMet. The BoneMet package is available on the Python Package Index (PyPI) at https://pypi.org/project/BoneMet. Code and tutorials are available at https://github.com/Tiankuo528/BoneMet.",
    "summary_cn": "BoneMet是首个公开的大规模多模态小鼠乳腺癌骨转移数据集，包含超67TB影像与生物数据，支持AI模型开发用于诊断、预后与安全监测。",
    "keywords": [
      "乳腺癌骨转移",
      "多模态数据集",
      "小鼠模型",
      "AI诊断",
      "医学影像",
      "预后分析"
    ],
    "triple": {
      "method": "构建大规模多模态小鼠数据集",
      "result": "提供超67TB数据，支持AI模型开发",
      "contribution": "填补公开数据集空白，促进AI在BCBM的应用"
    }
  },
  {
    "id": "96bbe620809a",
    "venue": "ICLR",
    "year": 2025,
    "title": "MedTrinity-25M: A Large-scale Multimodal Dataset with Multigranular Annotations for Medicine",
    "url": "https://iclr.cc/virtual/2025/poster/30141",
    "abstract": "This paper introduces MedTrinity-25M, a comprehensive, large-scale multimodal dataset for medicine, covering over 25 million images across 10 modalities with multigranular annotations for more than 65 diseases. These multigranular annotations encompass both global information, such as modality and organ detection, and local information like ROI analysis, lesion texture, and region-wise correlations. Unlike the existing multimodal datasets, which are limited by the availability of image-text pairs, we have developed the first automated pipeline that scales up multimodal data by generating multigranular visual and textual annotations in the form of image-ROI-description triplets without the need for any paired text descriptions. Specifically, data from over 30 different sources have been collected, preprocessed, and grounded using domain-specific expert models to identify ROIs related to abnormal regions. We then build a comprehensive knowledge base and prompt multimodal large language models to perform retrieval-augmented generation with the identified ROIs as guidance, resulting in multigranular textual descriptions. Compared to existing datasets, MedTrinity-25M provides the most enriched annotations, supporting a comprehensive range of multimodal tasks such as captioning and report generation, as well as vision-centric tasks like classification and segmentation. We propose LLaVA-Tri by pretraining LLaVA on MedTrinity-25M, achieving state-of-the-art performance on VQA-RAD, SLAKE, and PathVQA, surpassing representative SOTA multimodal large language models. Furthermore, MedTrinity-25M can also be utilized to support large-scale pre-training of multimodal medical AI models, contributing to the development of future foundation models in the medical domain. We will make our dataset available. The dataset is publicly available at https://yunfeixie233.github.io/MedTrinity-25M/.",
    "summary_cn": "提出MedTrinity-25M大规模医学多模态数据集，含2500万图像及多粒度标注，通过自动化流程生成图像-ROI-描述三元组，支持多种AI任务，并基于此预训练LLaVA-Tri模型取得SOTA性能。",
    "keywords": [
      "多模态数据集",
      "医学图像",
      "多粒度标注",
      "自动化标注",
      "大语言模型",
      "预训练"
    ],
    "triple": {
      "method": "自动化流程生成图像-ROI-描述三元组",
      "result": "LLaVA-Tri在多个VQA基准上达到SOTA",
      "contribution": "提供大规模医学多模态数据集支持AI模型开发"
    }
  },
  {
    "id": "84e0431c8053",
    "venue": "ICLR",
    "year": 2024,
    "title": "Large-scale Training of Foundation Models for Wearable Biosignals",
    "url": "https://proceedings.iclr.cc/paper_files/paper/2024/hash/0d99a8c048befb6dd6e17d7684adacac-Abstract-Conference.html",
    "abstract": "Tracking biosignals is crucial for monitoring wellness and preempting the development of severe medical conditions. Today, wearable devices can conveniently record various biosignals, creating the opportunity to monitor health status without disruption to one's daily routine. Despite widespread use of wearable devices and existing digital biomarkers, the absence of curated data with annotated medical labels hinders the development of new biomarkers to measure common health conditions. In fact, medical datasets are usually small in comparison to other domains, which is an obstacle for developing neural network models for biosignals. To address this challenge, we have employed self-supervised learning using the unlabeled sensor data collected under informed consent from the large longitudinal Apple Heart and Movement Study (AHMS) to train foundation models for two common biosignals: photoplethysmography (PPG) and electrocardiogram (ECG) recorded on Apple Watch. We curated PPG and ECG datasets from AHMS that include data from ${\\sim} 141$K participants spanning ${\\sim} 3$ years. Our self-supervised learning framework includes participant level positive pair selection, stochastic augmentation module and a regularized contrastive loss optimized with momentum training, and generalizes well to both PPG and ECG modalities. We show that the pre-trained foundation models readily encode information regarding participants' demographics and health conditions. To the best of our knowledge, this is the first study that builds foundation models using large-scale PPG and ECG data collected via wearable consumer devices $\\textendash$ prior works have commonly used smaller-size datasets collected in clinical and experimental settings. We believe PPG and ECG foundation models can enhance future wearable devices by reducing the reliance on labeled data and hold the potential to help the users improve their health.",
    "summary_cn": "本研究利用苹果心脏与运动研究的大规模未标记PPG和ECG数据，通过自监督学习训练基础模型，有效编码用户人口统计和健康状况信息，减少对标注数据的依赖。",
    "keywords": [
      "可穿戴生物信号",
      "自监督学习",
      "基础模型",
      "光电容积描记",
      "心电图",
      "健康监测"
    ],
    "triple": {
      "method": "自监督学习框架，包括参与者级正对选择、随机增强和正则化对比损失",
      "result": "基础模型能编码参与者人口统计和健康状况信息",
      "contribution": "首次利用大规模可穿戴设备数据构建PPG和ECG基础模型，减少标注数据依赖"
    }
  },
  {
    "id": "5aca68ab5e50",
    "venue": "ICLR",
    "year": 2024,
    "title": "Graph Transformers on EHRs: Better Representation Improves Downstream Performance",
    "url": "https://proceedings.iclr.cc/paper_files/paper/2024/hash/a71c1931d3fb8ba564f7458d0657d0b1-Abstract-Conference.html",
    "abstract": "Following the success of transformer-based methods across various machine learning applications, their adoption for healthcare predictive tasks using electronic health records (EHRs)  has also expanded extensively. Similarly, graph-based methods have been shown to be very effective in capturing inherent graph-type relationships in EHRs, leading to improved downstream performance. Although integrating these two families of approaches seems like a natural next step, in practice, creating such a design is challenging and has not been done. This is partly due to known EHR problems, such as high sparsity, making extracting meaningful temporal representations of medical visits challenging. In this study, we propose GT-BEHRT, a new approach that leverages temporal visit embeddings extracted from a graph transformer and uses a BERT-based model to obtain more robust patient representations, especially on longer EHR sequences. The graph-based approach allows GT-BEHRT to implicitly capture the intrinsic graphical relationships between medical observations, while the BERT model extracts the temporal relationships between visits, loosely mimicking the clinicians' decision-making process. As part of our method, we also present a two-step pre-training strategy for learning better graphical and temporal representations. Our proposed method achieves state-of-the-art performance in a variety of standard medical predictive tasks, demonstrating the versatility of our approach.",
    "summary_cn": "本研究提出GT-BEHRT方法，结合图变换器和BERT模型，通过两步预训练策略提升电子健康记录的表征能力，在多项医疗预测任务中达到最优性能。",
    "keywords": [
      "图变换器",
      "电子健康记录",
      "BERT模型",
      "预训练策略",
      "医疗预测",
      "表征学习"
    ],
    "triple": {
      "method": "结合图变换器与BERT的两步预训练",
      "result": "在多项医疗预测任务中达到最优性能",
      "contribution": "提出GT-BEHRT方法，提升EHR表征与下游任务表现"
    }
  },
  {
    "id": "4c2f44ce7568",
    "venue": "ICLR",
    "year": 2025,
    "title": "Dynamic Modeling of Patients, Modalities and Tasks via Multi-modal Multi-task Mixture of Experts",
    "url": "https://iclr.cc/virtual/2025/poster/29884",
    "abstract": "Multi-modal multi-task learning holds significant promise in tackling complex diagnostic tasks and many significant medical imaging problems. It fulfills the needs in real-world diagnosis protocol to leverage information from different data sources and simultaneously perform mutually informative tasks. However, medical imaging domains introduce two key challenges: dynamic modality fusion and modality-task dependence. The quality and amount of task-related information from different modalities could vary significantly across patient samples, due to biological and demographic factors. Traditional fusion methods apply fixed combination strategies that fail to capture this dynamic relationship, potentially underutilizing modalities that carry stronger diagnostic signals for specific patients. Additionally, different clinical tasks may require dynamic feature selection and combination from various modalities, a phenomenon we term “modality-task dependence.” To address these issues, we propose M4oE, a novel Multi-modal Multi-task Mixture of Experts framework for precise Medical diagnosis. M4oE comprises Modality-Specific (MSoE) modules and a Modality-shared Modality-Task MoE (MToE) module. With collaboration from both modules, our model dynamically decomposes and learns distinct and shared information from different modalities and achieves dynamic fusion. MToE provides a joint probability model of modalities and tasks by using experts as a link and encourages experts to learn modality-task dependence via conditional mutual information loss. By doing so, M4oE offers sample and population-level interpretability of modality contributions. We evaluate M4oE on four public multi-modal medical benchmark datasets for solving two important medical diagnostic problems including breast cancer screening and retinal disease diagnosis. Results demonstrate our method's superiority over state-of-the-art methods under different metrics of classification and segmentation tasks like Accuracy, AUROC, AUPRC, and DICE.",
    "summary_cn": "提出M4oE框架，通过多模态多任务混合专家模型动态融合模态与任务信息，提升乳腺癌筛查和视网膜疾病诊断的准确性。",
    "keywords": [
      "多模态学习",
      "多任务学习",
      "混合专家模型",
      "动态融合",
      "医学诊断",
      "模态-任务依赖"
    ],
    "triple": {
      "method": "M4oE框架（含MSoE和MToE模块）",
      "result": "在分类和分割任务上超越现有方法",
      "contribution": "实现动态模态融合与模态-任务依赖建模"
    }
  },
  {
    "id": "d91d29889fdd",
    "venue": "ICLR",
    "year": 2025,
    "title": "TINY: Semantic-based Uncertainty Quantification in LLMS: A Case Study on Medical Explanation Generation Task.",
    "url": "https://iclr.cc/virtual/2025/32888",
    "abstract": "Given the often sensible and sometimes nonsensical outputs that modern Large Language Models (LLMs) generate, how should we interpret confident claims such as \"Strawberry has two 'r's\"? One tool that can be applied to such overconfident and hallucinatory claims is uncertainty quantification. In particular, this paper investigates the recently proposed semantic density framework to quantify uncertainty in LLM-generated medical explanations. Semantic density makes use of semantic similarity comparisons instead of lexical matching, and delivers per-response estimates of uncertainty. The results demonstrate that the semantic density framework remains performant when applied in specialized domains, and raises additional considerations around the utility of the ROUGE metric for semantic evaluations.",
    "summary_cn": "本文研究语义密度框架在大型语言模型生成医学解释任务中的不确定性量化，结果表明该框架在专业领域仍有效，并质疑ROUGE指标在语义评估中的实用性。",
    "keywords": [
      "不确定性量化",
      "语义密度",
      "大型语言模型",
      "医学解释生成",
      "ROUGE指标",
      "语义相似性"
    ],
    "triple": {
      "method": "应用语义密度框架",
      "result": "框架在医学领域有效，ROUGE指标适用性存疑",
      "contribution": "验证语义不确定性量化在专业任务中的性能"
    }
  },
  {
    "id": "66b4f531c438",
    "venue": "ICLR",
    "year": 2024,
    "title": "Rough Transformers for Continuous and Efficient Time-Series Modelling",
    "url": "https://iclr.cc/virtual/2024/23551",
    "abstract": "Time-series data in real-world medical settings typically exhibit long-range dependencies and are observed at non-uniform intervals. In such contents, traditional sequence-based recurrent models struggle. To overcome this, researchers replace recurrent architectures with Neural ODE-based models to model irregularly sampled data and use Transformer-based architectures to account for long-range dependencies. Despite the success of these two approaches, both incur very high computational costs for input sequences of moderate lengths and greater. To mitigate this, we introduce the Rough Transformer, a variation of the Transformer model which operates on continuous-time representations of input sequences and incurs significantly reduced computational costs, critical for addressing long-range dependencies common in medical contexts. In particular, we propose multi-view signature attention, which uses path signatures to augment vanilla attention and to capture both local and global dependencies in input data, while remaining robust to changes in the sequence length and sampling frequency. We find that Rough Transformers consistently outperform their vanilla attention counterparts while obtaining the benefits of Neural ODE-based models using a fraction of the computational time and memory resources on synthetic and real-world time-series tasks.",
    "summary_cn": "提出Rough Transformer，结合路径签名增强注意力，高效处理医疗时间序列的长程依赖和非均匀采样，计算成本显著降低。",
    "keywords": [
      "Rough Transformer",
      "时间序列建模",
      "路径签名",
      "长程依赖",
      "计算效率",
      "医疗数据"
    ],
    "triple": {
      "method": "多视图签名注意力",
      "result": "性能优于传统注意力，计算资源大幅减少",
      "contribution": "高效连续时间序列建模方法"
    }
  },
  {
    "id": "49be8405ec07",
    "venue": "ICLR",
    "year": 2025,
    "title": "Learning Causal Alignment for Reliable Disease Diagnosis",
    "url": "https://iclr.cc/virtual/2025/poster/28327",
    "abstract": "Aligning the decision-making process of machine learning algorithms with that of experienced radiologists is crucial for reliable diagnosis. While existing methods have attempted to align their prediction behaviors to those of radiologists reflected in the training data, this alignment is primarily associational rather than causal, resulting in pseudo-correlations that may not transfer well. In this paper, we propose a causality-based alignment framework towards aligning the model's decision process with that of experts. Specifically, we first employ counterfactual generation to identify the causal chain of model decisions. To align this causal chain with that of experts, we propose a causal alignment loss that enforces the model to focus on causal factors underlying each decision step in the whole causal chain. To optimize this loss that involves the counterfactual generator as an implicit function of the model's parameters, we employ the implicit function theorem equipped with the conjugate gradient method for efficient estimation. We demonstrate the effectiveness of our method on two medical diagnosis applications, showcasing faithful alignment to radiologists.",
    "summary_cn": "提出基于因果关系的对齐框架，通过反事实生成和因果对齐损失，使模型决策过程与放射科专家对齐，提升诊断可靠性。",
    "keywords": [
      "因果对齐",
      "反事实生成",
      "医学诊断",
      "机器学习",
      "放射科专家",
      "决策过程"
    ],
    "triple": {
      "method": "反事实生成与因果对齐损失",
      "result": "模型决策与专家因果链对齐",
      "contribution": "提升诊断可靠性与可迁移性"
    }
  },
  {
    "id": "1cb9970fc6a7",
    "venue": "ICLR",
    "year": 2025,
    "title": "TIMER: Temporal Instruction Modeling and Evaluation for Longitudinal Clinical Records",
    "url": "https://iclr.cc/virtual/2025/32173",
    "abstract": "Large language models (LLMs) have emerged as promising tools for assisting in medical tasks, yet processing Electronic Health Records (EHRs) presents unique challenges due to their longitudinal nature. While LLMs' capabilities to perform medical tasks continue to improve, their ability to reason over temporal dependencies across multiple patient visits and time frames remains unexplored. We introduce TIMER ( T emporal I nstruction M odeling and E valuation for Longitudinal Clinical R ecords), a synthetic data generation framework that incorporates temporal distribution of instructions as a critical dimension in both instruction evaluation and tuning for longitudinal clinical records. We develop TIMER-Bench, the first time-aware benchmark that evaluates temporal reasoning capabilities over longitudinal EHRs, as well as TIMER-Instruct, an instruction-tuning methodology for LLMs to learn reasoning over time. We demonstrate that models fine-tuned with TIMER-Instruct improve performance by 7.3% on human-generated benchmarks and 9.2% on TIMER-Bench, indicating that temporal instruction-tuning improves model performance for reasoning over EHR. Our code is available at TIMER .",
    "summary_cn": "TIMER框架通过生成合成数据和指令调优，提升大语言模型在纵向电子健康记录中的时序推理能力，性能显著提升。",
    "keywords": [
      "大语言模型",
      "电子健康记录",
      "时序推理",
      "指令调优",
      "基准评估",
      "合成数据"
    ],
    "triple": {
      "method": "合成数据生成与指令调优",
      "result": "模型性能提升7.3%-9.2%",
      "contribution": "提出首个时序感知基准与调优方法"
    }
  },
  {
    "id": "313e0d430f51",
    "venue": "ICLR",
    "year": 2025,
    "title": "A General Framework for Off-Policy Learning with Partially-Observed Reward",
    "url": "https://proceedings.iclr.cc/paper_files/paper/2025/hash/098491b37deebbe6c007e69815729e09-Abstract-Conference.html",
    "abstract": "Off-policy learning (OPL) in contextual bandits aims to learn a decision-making policy that maximizes the target rewards by using only historical interaction data collected under previously developed policies. Unfortunately, when rewards are only partially observed, the effectiveness of OPL degrades severely. Well-known examples of such partial rewards include explicit ratings in content recommendations, conversion signals on e-commerce platforms that are partial due to delay, and the issue of censoring in medical problems. One possible solution to deal with such partial rewards is to use secondary rewards, such as dwelling time, clicks, and medical indicators, which are more densely observed. However, relying solely on such secondary rewards can also lead to poor policy learning since they may not align with the target reward. Thus, this work studies a new and general problem of OPL where the goal is to learn a policy that maximizes the expected target reward by leveraging densely observed secondary rewards as supplemental data. We then propose a new method called Hybrid Policy Optimization for Partially-Observed Reward (HyPeR), which effectively uses the secondary rewards in addition to the partially observed target reward to achieve effective OPL despite the challenging scenario. We also discuss a case where we aim to optimize not only the expected target reward but also the expected secondary rewards to some extent; counter-intuitively, we will show that leveraging the two objectives is in fact advantageous also for the optimization of only the target reward. Along with statistical analysis of our proposed methods, empirical evaluations on both synthetic and real-world data show that HyPeR outperforms existing methods in various scenarios.",
    "summary_cn": "提出HyPeR方法，利用密集观测的次级奖励辅助部分观测的目标奖励，提升离线策略学习效果，在合成和真实数据中表现优异。",
    "keywords": [
      "离线策略学习",
      "部分观测奖励",
      "次级奖励",
      "HyPeR方法",
      "上下文赌博机",
      "优化目标"
    ],
    "triple": {
      "method": "HyPeR方法结合目标与次级奖励",
      "result": "在合成和真实数据中优于现有方法",
      "contribution": "提升部分观测奖励下的离线策略学习效果"
    }
  },
  {
    "id": "68f7d81ba7ca",
    "venue": "ICLR",
    "year": 2025,
    "title": "Learning General-purpose Biomedical Volume Representations using Randomized Synthesis",
    "url": "https://iclr.cc/virtual/2025/poster/27787",
    "abstract": "Current volumetric biomedical foundation models struggle to generalize as public 3D datasets are small and do not cover the broad diversity of medical procedures, conditions, anatomical regions, and imaging protocols. We address this by creating a representation learning method that instead anticipates strong domain shifts at training time itself. We first propose a data engine that synthesizes highly variable training samples that would enable generalization to new biomedical contexts. To then train a single 3D network for any voxel-level task, we develop a contrastive learning method that pretrains the network to be stable against nuisance imaging variation simulated by the data engine, a key inductive bias for generalization. This network's features can be used as robust representations of input images for downstream tasks and its weights provide a strong, dataset-agnostic initialization for finetuning on new datasets. As a result, we set new standards across both multimodality registration and few-shot segmentation, a first for any 3D biomedical vision model, all without (pre-)training on any existing dataset of real images.",
    "summary_cn": "提出一种通过随机合成数据训练通用3D生物医学体积表示的方法，无需真实图像数据集，在配准和分割任务中实现新标准。",
    "keywords": [
      "生物医学体积表示",
      "随机合成",
      "对比学习",
      "3D网络",
      "泛化能力",
      "少样本分割"
    ],
    "triple": {
      "method": "使用数据引擎合成变量样本，结合对比学习预训练",
      "result": "在配准和少样本分割任务中达到新标准",
      "contribution": "提出无需真实数据集的通用3D生物医学表示学习方法"
    }
  },
  {
    "id": "1ae7074d5a03",
    "venue": "ICLR",
    "year": 2025,
    "title": "Boltzmann Semantic Score: A Semantic Metric for Evaluating Large Vision Models Using Large Language Models",
    "url": "https://proceedings.iclr.cc/paper_files/paper/2025/hash/fc07feae9af49dd3f1a1e049b77f4e17-Abstract-Conference.html",
    "abstract": "Do Large Vision Models (LVMs) extract medically and semantically relevant features similar to those identified by human experts? Currently, only biased, qualitative approaches with limited, small-scale expert evaluations are available to answer this question. In this study, we propose the Boltzmann Semantic Score (BSS), a novel method inspired by state space modeling, to evaluate the encoding space of LVMs from medical images using the encoding space of Large Language Models (LLMs) from medical reports. Through extensive experimentation on 32 datasets from The Cancer Genome Atlas collection using five state-of-the-art LLMs, we first establish a baseline of LLMs' performance in digital pathology and show that LLMs' encoding can be linked to patient outcomes. Then, we compared seven LVMs with BSS and showed that LVMs suffer from poor semantic capability when compared with encoded expert knowledge from pathology reports.We also found statistically significant correlations between BSS (as a measure of structural similarity) and performance in two downstream tasks: information retrieval and survival prediction tasks. Our study also investigates the consensus among LLMs in evaluating LVMs using BSS, indicating that LLMs generally reach substantial consensus in rating LVMs, with some variation dependant on the cancer type. We believe the BSS metric proposed here holds significant potential for application in other domains with similar contexts. Data and code can be found in \\footnotesize \\url{ https://github.com/AIMLab-UBC/Boltzmann}",
    "summary_cn": "提出Boltzmann语义评分（BSS），利用大语言模型评估大视觉模型在医学图像中的语义特征提取能力，发现其与专家知识存在差距，并与下游任务性能相关。",
    "keywords": [
      "Boltzmann语义评分",
      "大视觉模型",
      "大语言模型",
      "医学图像分析",
      "语义评估",
      "癌症基因组图谱"
    ],
    "triple": {
      "method": "基于状态空间建模，比较LVMs与LLMs的编码空间",
      "result": "LVMs语义能力不足，BSS与下游任务性能显著相关",
      "contribution": "提出BSS量化评估指标，促进跨模型语义对齐"
    }
  },
  {
    "id": "8d6a80517e37",
    "venue": "ICLR",
    "year": 2025,
    "title": "Reasoning-Enhanced Healthcare Predictions with Knowledge Graph Community Retrieval",
    "url": "https://iclr.cc/virtual/2025/poster/30752",
    "abstract": "Large language models (LLMs) have demonstrated significant potential in clinical decision support. Yet LLMs still suffer from hallucinations and lack fine-grained contextual medical knowledge, limiting their high-stake healthcare applications such as clinical diagnosis. Traditional retrieval-augmented generation (RAG) methods attempt to address these limitations but frequently retrieve sparse or irrelevant information, undermining prediction accuracy. We introduce KARE, a novel framework that integrates knowledge graph (KG) community-level retrieval with LLM reasoning to enhance healthcare predictions. KARE constructs a comprehensive multi-source KG by integrating biomedical databases, clinical literature, and LLM-generated insights, and organizes it using hierarchical graph community detection and summarization for precise and contextually relevant information retrieval. Our key innovations include: (1) a dense medical knowledge structuring approach enabling accurate retrieval of relevant information; (2) a dynamic knowledge retrieval mechanism that enriches patient contexts with focused, multi-faceted medical insights; and (3) a reasoning-enhanced prediction framework that leverages these enriched contexts to produce both accurate and interpretable clinical predictions. Extensive experiments demonstrate that KARE outperforms leading models by up to 10.8-15.0\\% on MIMIC-III and 12.6-12.7\\% on MIMIC-IV for mortality and readmission predictions. In addition to its impressive prediction accuracy, our framework leverages the reasoning capabilities of LLMs, enhancing the trustworthiness of clinical predictions.",
    "summary_cn": "提出KARE框架，结合知识图谱社区检索与LLM推理，提升医疗预测准确性。在MIMIC数据集上，死亡率与再入院预测性能显著优于现有模型。",
    "keywords": [
      "知识图谱",
      "检索增强生成",
      "临床预测",
      "大语言模型",
      "社区检测",
      "医疗推理"
    ],
    "triple": {
      "method": "知识图谱社区检索与LLM推理结合",
      "result": "预测性能提升10.8-15.0%",
      "contribution": "提高预测准确性与可解释性"
    }
  },
  {
    "id": "afe6ee18b2e4",
    "venue": "ICLR",
    "year": 2025,
    "title": "pMoE: Prompting Diverse Experts Together Wins More in Visual Adaptation",
    "url": "https://proceedings.iclr.cc/paper_files/paper/2025/hash/c54a5c64fd929d05547dba74db0e7e8c-Abstract-Conference.html",
    "abstract": "Parameter-efficient fine-tuning has demonstrated promising results across various visual adaptation tasks, such as classification and segmentation. Typically, prompt tuning techniques have harnessed knowledge from a single pre-trained model, whether from a general or a specialized medical domain. However, this approach typically overlooks the potential synergies that could arise from integrating diverse domain knowledge within the same tuning process. In this work, we propose a novel Mixture-of-Experts prompt tuning method called pMoE, which leverages the strengths of multiple expert domains through expert-specialized prompt tokens and the learnable dispatcher, effectively combining their expertise in a unified model framework. Our pMoE introduces expert-specific prompt tokens and utilizes a dynamic token dispatching mechanism at various prompt layers to optimize the contribution of each domain expert during the adaptation phase. By incorporating both domain knowledge from diverse experts, the proposed pMoE significantly enhances the model's versatility and applicability to a broad spectrum of tasks. We conduct extensive experiments across 47 adaptation tasks, including both classification and segmentation in general and medical domains. The results demonstrate that our pMoE not only achieves superior performance with a large margin of improvements but also offers an optimal trade-off between computational efficiency and adaptation effectiveness compared to existing methods.",
    "summary_cn": "提出pMoE方法，通过专家专用提示令牌和动态调度器整合多领域知识，在47个视觉适应任务中显著提升性能与效率。",
    "keywords": [
      "pMoE",
      "视觉适应",
      "提示调优",
      "专家混合",
      "参数高效",
      "多领域知识"
    ],
    "triple": {
      "method": "专家专用提示令牌与动态调度器",
      "result": "在47个任务中性能大幅提升，平衡效率与效果",
      "contribution": "提出统一框架整合多领域专家知识"
    }
  },
  {
    "id": "f23cdc4beda6",
    "venue": "ICLR",
    "year": 2025,
    "title": "BoneMet: An Open Large-Scale Multi-Modal Murine Dataset for Breast Cancer Bone Metastasis Diagnosis and Prognosis",
    "url": "https://iclr.cc/virtual/2025/poster/29268",
    "abstract": "Breast cancer bone metastasis (BCBM) affects women’s health globally, calling for the development of effective diagnosis and prognosis solutions. While deep learning has exhibited impressive capacities across various healthcare domains, its applicability in BCBM diseases is consistently hindered by the lack of an open, large-scale, deep learning-ready dataset. As such, we introduce the Bone Metastasis (BoneMet) dataset, the first large-scale, publicly available, high-resolution medical resource, which is derived from a well-accepted murine BCBM model. The unique advantage of BoneMet over existing human datasets is repeated sequential scans per subject over the entire disease development phases. The dataset consists of over 67 terabytes of multi-modal medical data, including 2D X-ray images, 3D CT scans, and detailed biological data (e.g., medical records and bone quantitative analysis), collected from more than five hundreds mice spanning from 2019 to 2024. Our BoneMet dataset is well-organized into six components, i.e., RotationX-Ray, Recon-CT, Seg-CT, Regist-CT, RoI-CT, and MiceMediRec. We further show that BoneMet can be readily adopted to build versatile, large-scale AI models for managing BCBM diseases in terms of diagnosis using 2D or 3D images, prognosis of bone deterioration, and sparse-angle 3D reconstruction for safe long-term disease monitoring. Our preliminary results demonstrate that BoneMet has the potentials to jump-start the development and fine-tuning of AI-driven solutions prior to their applications to human patients. To facilitate its easy access and wide dissemination, we have created the BoneMet package, providing three APIs that enable researchers to (i) flexibly process and download the BoneMet data filtered by specific time frames; and (ii) develop and train large-scale AI models for precise BCBM diagnosis and prognosis. The BoneMet dataset is officially available on Hugging Face Datasets at https://huggingface.co/datasets/BoneMet/BoneMet. The BoneMet package is available on the Python Package Index (PyPI) at https://pypi.org/project/BoneMet. Code and tutorials are available at https://github.com/Tiankuo528/BoneMet.",
    "summary_cn": "BoneMet是首个公开的大规模多模态小鼠乳腺癌骨转移数据集，包含X光、CT及生物数据，支持AI模型开发用于诊断、预后和3D重建。",
    "keywords": [
      "乳腺癌骨转移",
      "多模态数据集",
      "小鼠模型",
      "AI诊断",
      "医学影像",
      "公开数据"
    ],
    "triple": {
      "method": "构建大规模多模态小鼠数据集",
      "result": "提供超67TB数据，支持AI模型开发",
      "contribution": "填补公开数据空白，促进AI在BCBM应用"
    }
  },
  {
    "id": "edc7683fcea9",
    "venue": "ICLR",
    "year": 2024,
    "title": "Uncertainty of Thoughts: Uncertainty-Aware Planning Enhances Information Seeking in Large Language Models",
    "url": "https://iclr.cc/virtual/2024/22184",
    "abstract": "In the face of uncertainty, the ability to seek information is of fundamental importance. In many practical applications, such as medical diagnosis and troubleshooting, the information needed to solve the task is not initially given, and has to be actively sought by asking follow-up questions (for example, a doctor asking a patient for more details about their symptoms). In this work, we introduce Uncertainty of Thoughts (UoT), an algorithm to augment large language models with the ability to actively seek information by asking effective questions. UoT combines 1) an uncertainty-aware simulation approach which enables the model to simulate possible future scenarios and how likely they are to occur, 2) uncertainty-based rewards motivated by information gain which incentivizes the model to seek information, and 3) a reward propagation scheme to select the optimal question to ask in a way that maximizes the expected reward. In experiments on medical diagnosis, troubleshooting and the `20 Questions' game, UoT achieves an average performance improvement of 57.8% in the rate of successful task completion across multiple LLMs compared with direct prompting, and also improves efficiency (i.e., the number of questions needed to complete the task).",
    "summary_cn": "提出UoT算法，通过不确定性感知模拟、信息增益奖励和奖励传播，增强大语言模型主动提问以寻求信息的能力，在医疗诊断等任务中显著提升成功率与效率。",
    "keywords": [
      "不确定性感知",
      "信息寻求",
      "大语言模型",
      "主动提问",
      "医疗诊断",
      "奖励机制"
    ],
    "triple": {
      "method": "不确定性感知模拟与奖励传播",
      "result": "任务成功率平均提升57.8%，效率提高",
      "contribution": "增强LLM主动信息寻求能力"
    }
  },
  {
    "id": "b3024c9cfaf9",
    "venue": "ICLR",
    "year": 2024,
    "title": "EHRAgent: Code Empowers Large Language Models for Few-shot Complex Tabular Reasoning on Electronic Health Records",
    "url": "https://iclr.cc/virtual/2024/22183",
    "abstract": "Large language models (LLMs) have demonstrated exceptional capabilities in planning and tool utilization as autonomous agents, but few have been developed for medical problem-solving. We propose EHRAgent, an LLM agent empowered with a code interface, to autonomously generate and execute code for complex clinical tasks within electronic health records (EHRs). First, we formulate an EHR question-answering task into a tool-use planning process, efficiently decomposing a complicated task into a sequence of manageable actions. By integrating interactive coding and execution feedback, EHRAgent learns from error messages and improves the originally generated code through iterations. Furthermore, we enhance the LLM agent by incorporating long-term memory, which allows EHRAgent to effectively select and build upon the most relevant successful cases from past experiences. Experiments on three real-world multi-tabular EHR datasets show that EHRAgent outperforms the strongest baseline by up to 29.60%. EHRAgent leverages the emerging few-shot learning capabilities of LLMs, enabling autonomous code generation and execution to tackle complex clinical tasks. Our implementation of EHRAgent is available at https://anonymous.4open.science/r/EHRAgent24-95C0.",
    "summary_cn": "EHRAgent 是一个基于大语言模型的智能体，通过代码接口自主生成和执行代码，以解决电子健康记录中的复杂临床任务，在少样本学习下性能显著提升。",
    "keywords": [
      "电子健康记录",
      "大语言模型",
      "代码生成",
      "少样本学习",
      "临床推理",
      "自主代理"
    ],
    "triple": {
      "method": "代码接口与长时记忆增强",
      "result": "在三个真实 EHR 数据集上性能提升达 29.60%",
      "contribution": "实现少样本复杂临床任务自主推理"
    }
  },
  {
    "id": "5f9681e52b1e",
    "venue": "ICLR",
    "year": 2024,
    "title": "GraphCare: Enhancing Healthcare Predictions with Personalized Knowledge Graphs",
    "url": "https://iclr.cc/virtual/2024/poster/17612",
    "abstract": "Clinical predictive models often rely on patients’ electronic health records (EHR), but integrating medical knowledge to enhance predictions and decision-making is challenging. This is because personalized predictions require personalized knowledgegraphs (KGs), which are difficult to generate from patient EHR data. To address this, we propose GraphCare, an open-world framework that uses external KGs to improve EHR-based predictions. Our method extracts knowledge from large language models (LLMs) and external biomedical KGs to build patient-specific KGs, which are then used to train our proposed Bi-attention AugmenTed(BAT) graph neural network (GNN) for healthcare predictions. On two public datasets, MIMIC-III and MIMIC-IV, GraphCare surpasses baselines in four vital healthcare prediction tasks: mortality, readmission, length of stay (LOS), and drug recommendation. On MIMIC-III, it boosts AUROC by 17.6% and 6.6% for mortality and readmission, and F1-score by 7.9% and 10.8% for LOS and drug recommendation, respectively. Notably, GraphCare demonstrates a substantial edge in scenarios with limited data availability. Our findings highlight the potential of using external KGs in healthcare prediction tasks and demonstrate the promise of GraphCare in generating personalized KGs for promoting personalized medicine.",
    "summary_cn": "GraphCare框架利用外部知识图谱和LLM构建个性化知识图谱，结合BAT图神经网络提升医疗预测性能，在MIMIC数据集上显著优于基线模型。",
    "keywords": [
      "个性化知识图谱",
      "图神经网络",
      "医疗预测",
      "电子健康记录",
      "大语言模型",
      "数据增强"
    ],
    "triple": {
      "method": "基于外部知识图谱和LLM构建个性化知识图谱，采用BAT图神经网络",
      "result": "在MIMIC数据集上，死亡率预测AUROC提升17.6%，再入院预测提升6.6%",
      "contribution": "提出开放世界框架GraphCare，增强医疗预测的个性化和准确性"
    }
  },
  {
    "id": "ae055e609e2f",
    "venue": "ICLR",
    "year": 2025,
    "title": "Closing The Modality Gap Enables Novel Multimodal Learning Applications",
    "url": "https://iclr.cc/virtual/2025/36848",
    "abstract": "In multimodal learning, CLIP has emerged as the de facto approach for mapping different modalities into a shared latent space by bringing semantically similar representations closer while pushing apart dissimilar ones. However, CLIP-based contrastive losses exhibit unintended behaviors that negatively impact true semantic alignment, leading to sparse and fragmented latent spaces. This phenomenon, known as the modality gap, has been partially mitigated for standard text and image pairs, but remains unresolved in more complex multimodal settings, especially when integrating three or more modalities.In this work, we propose a modality-agnostic framework that definitively closes the modality gap across multiple modalities, ensuring that semantically related representations are perfectly aligned, regardless of their source modality. Beyond theoretical improvements, we demonstrate that closing the modality gap has profound implications for real-world applications. In semantic communication, our approach enables the transmission of a single compact representation per semantic concept, drastically reducing bandwidth requirements while preserving multimodal reconstruction quality. In medical multimodal learning, our method enhances alignment between radiology images and clinical text, improving cross-modal retrieval and image captioning. We show that our approach not only closes the modality gap permanently but also unlocks new capabilities in downstream applications that were previously limited by poor cross-modal alignment.",
    "summary_cn": "提出一种模态无关框架，彻底消除多模态学习中的模态鸿沟，实现语义表示的完美对齐，显著提升语义通信和医学多模态应用性能。",
    "keywords": [
      "模态鸿沟",
      "多模态学习",
      "语义对齐",
      "对比学习",
      "医学应用",
      "语义通信"
    ],
    "triple": {
      "method": "模态无关框架",
      "result": "消除模态鸿沟，提升跨模态检索与图像描述",
      "contribution": "解锁新应用能力"
    }
  },
  {
    "id": "a6090f41a61f",
    "venue": "ICLR",
    "year": 2024,
    "title": "Graph Transformers on EHRs: Better Representation Improves Downstream Performance",
    "url": "https://iclr.cc/virtual/2024/poster/17772",
    "abstract": "Following the success of transformer-based methods across various machine learning applications, their adoption for healthcare predictive tasks using electronic health records (EHRs)  has also expanded extensively. Similarly, graph-based methods have been shown to be very effective in capturing inherent graph-type relationships in EHRs, leading to improved downstream performance. Although integrating these two families of approaches seems like a natural next step, in practice, creating such a design is challenging and has not been done. This is partly due to known EHR problems, such as high sparsity, making extracting meaningful temporal representations of medical visits challenging. In this study, we propose GT-BEHRT, a new approach that leverages temporal visit embeddings extracted from a graph transformer and uses a BERT-based model to obtain more robust patient representations, especially on longer EHR sequences. The graph-based approach allows GT-BEHRT to implicitly capture the intrinsic graphical relationships between medical observations, while the BERT model extracts the temporal relationships between visits, loosely mimicking the clinicians' decision-making process. As part of our method, we also present a two-step pre-training strategy for learning better graphical and temporal representations. Our proposed method achieves state-of-the-art performance in a variety of standard medical predictive tasks, demonstrating the versatility of our approach.",
    "summary_cn": "本研究提出GT-BEHRT方法，结合图变换器和BERT模型，通过两步预训练策略提升电子健康记录的表征能力，在多项医疗预测任务中达到最优性能。",
    "keywords": [
      "图变换器",
      "电子健康记录",
      "BERT模型",
      "预训练策略",
      "医疗预测",
      "时序表征"
    ],
    "triple": {
      "method": "结合图变换器与BERT的两步预训练",
      "result": "在多项医疗预测任务中达到最优性能",
      "contribution": "提升EHR表征能力，模拟临床决策过程"
    }
  },
  {
    "id": "698c024528b8",
    "venue": "ICLR",
    "year": 2025,
    "title": "A General Framework for Off-Policy Learning with Partially-Observed Reward",
    "url": "https://iclr.cc/virtual/2025/poster/28461",
    "abstract": "Off-policy learning (OPL) in contextual bandits aims to learn a decision-making policy that maximizes the target rewards by using only historical interaction data collected under previously developed policies. Unfortunately, when rewards are only partially observed, the effectiveness of OPL degrades severely. Well-known examples of such partial rewards include explicit ratings in content recommendations, conversion signals on e-commerce platforms that are partial due to delay, and the issue of censoring in medical problems. One possible solution to deal with such partial rewards is to use secondary rewards, such as dwelling time, clicks, and medical indicators, which are more densely observed. However, relying solely on such secondary rewards can also lead to poor policy learning since they may not align with the target reward. Thus, this work studies a new and general problem of OPL where the goal is to learn a policy that maximizes the expected target reward by leveraging densely observed secondary rewards as supplemental data. We then propose a new method called Hybrid Policy Optimization for Partially-Observed Reward (HyPeR), which effectively uses the secondary rewards in addition to the partially observed target reward to achieve effective OPL despite the challenging scenario. We also discuss a case where we aim to optimize not only the expected target reward but also the expected secondary rewards to some extent; counter-intuitively, we will show that leveraging the two objectives is in fact advantageous also for the optimization of only the target reward. Along with statistical analysis of our proposed methods, empirical evaluations on both synthetic and real-world data show that HyPeR outperforms existing methods in various scenarios.",
    "summary_cn": "本文提出HyPeR方法，利用密集观测的次级奖励辅助部分观测的目标奖励，提升离线策略学习效果，在合成和真实数据上优于现有方法。",
    "keywords": [
      "离线策略学习",
      "部分观测奖励",
      "次级奖励",
      "HyPeR方法",
      "上下文赌博机",
      "优化目标"
    ],
    "triple": {
      "method": "HyPeR方法结合目标与次级奖励",
      "result": "在合成和真实数据中表现优于现有方法",
      "contribution": "提出通用框架优化部分观测奖励的离线学习"
    }
  },
  {
    "id": "b1a75193d68b",
    "venue": "ICLR",
    "year": 2025,
    "title": "Anyprefer: An Agentic Framework for Preference Data Synthesis",
    "url": "https://iclr.cc/virtual/2025/poster/29342",
    "abstract": "High-quality preference data is essential for aligning foundation models with human values through preference learning. However, manual annotation of such data is often time-consuming and costly. Recent methods often adopt a self-rewarding approach, where the target model generates and annotates its own preference data, but this can lead to inaccuracies since the reward model shares weights with the target model, thereby amplifying inherent biases. To address these issues, we propose Anyprefer, a framework designed to synthesize high-quality preference data for aligning the target model. Anyprefer frames the data synthesis process as a cooperative two-player Markov Game, where the target model and the judge model collaborate together. Here, a series of external tools are introduced to assist the judge model in accurately rewarding the target model’s responses, mitigating biases in the rewarding process. In addition, a feedback mechanism is introduced to optimize prompts for both models, enhancing collaboration and improving data quality. The synthesized data is compiled into a new preference dataset, Anyprefer-V1, consisting of 58K high-quality preference pairs. Extensive experiments show that Anyprefer significantly improves model alignment performance across four main applications, covering 21 datasets, achieving average improvements of 18.55% in five natural language generation datasets, 3.66% in nine vision-language understanding datasets, 30.05% in three medical image analysis datasets, and 16.00% in four visuo-motor control tasks.",
    "summary_cn": "提出Anyprefer框架，通过双智能体协作与外部工具辅助，合成高质量偏好数据，提升基础模型对齐性能。",
    "keywords": [
      "偏好数据合成",
      "模型对齐",
      "双智能体协作",
      "外部工具",
      "反馈机制",
      "Anyprefer-V1数据集"
    ],
    "triple": {
      "method": "双智能体协作与外部工具辅助",
      "result": "合成58K高质量偏好对，提升多领域模型性能",
      "contribution": "提出Anyprefer框架，缓解偏好数据合成偏差"
    }
  },
  {
    "id": "ae2c6f6a52a1",
    "venue": "ICLR",
    "year": 2024,
    "title": "Adversarial Feature Map Pruning for Backdoor",
    "url": "https://iclr.cc/virtual/2024/poster/18966",
    "abstract": "Deep neural networks have been widely used in many critical applications, such as autonomous vehicles and medical diagnosis. However, their security is threatened by backdoor attacks, which are achieved by adding artificial patterns to specific training data. Existing defense strategies primarily focus on using reverse engineering to reproduce the backdoor trigger generated by attackers and subsequently repair the DNN model by adding the trigger into inputs and fine-tuning the model with ground truth labels. However, once the trigger generated by the attackers is complex and invisible, the defender cannot reproduce the trigger successfully then the DNN model will not be repaired, as the trigger is not effectively removed. In this work, we propose Adversarial Feature Map Pruning for Backdoor (FMP) to mitigate backdoor from the DNN. Unlike existing defense strategies, which focus on reproducing backdoor triggers, FMP attempts to prune backdoor feature maps, which are trained to extract backdoor information from inputs. After pruning these backdoor feature maps, FMP will fine-tune the model with a secure subset of training data. Our experiments demonstrate that, compared to existing defense strategies, FMP can effectively reduce the Attack Success Rate (ASR) even against the most complex and invisible attack triggers (e.g., FMP decreases the ASR to 2.86% in CIFAR10, which is 19.2% to 65.41% lower than baselines). Second, unlike conventional defense methods that tend to exhibit low robust accuracy (that is, the accuracy of the model on poisoned data), FMP achieves a higher RA, indicating its superiority in maintaining model performance while mitigating the effects of backdoor attacks (e.g., FMP obtains 87.40% RA in CIFAR10). Our code is publicly available at: https://github.com/hku-systems/FMP.",
    "summary_cn": "提出对抗性特征图剪枝方法，通过剪枝后门特征图并微调，有效降低复杂隐形后门攻击成功率，同时保持模型性能。",
    "keywords": [
      "后门攻击防御",
      "特征图剪枝",
      "深度学习安全",
      "对抗性训练",
      "模型微调",
      "攻击成功率"
    ],
    "triple": {
      "method": "剪枝后门特征图并微调",
      "result": "攻击成功率降至2.86%，鲁棒准确率达87.40%",
      "contribution": "高效防御复杂隐形后门攻击"
    }
  },
  {
    "id": "1486b885629d",
    "venue": "ICLR",
    "year": 2024,
    "title": "BEYOND FINE-TUNING: LORA MODULES BOOST NEAR- OOD DETECTION AND LLM SECURITY",
    "url": "https://iclr.cc/virtual/2024/23846",
    "abstract": "Under resource constraints, LLMs are usually fine-tuned with additional knowl- edge using Parameter Efficient Fine-Tuning (PEFT), using Low-Rank Adaptation (LoRA) modules. In fact, LoRA injects a new set of small trainable matrices to adapt an LLM to a new task, while keeping the latter frozen. At deployment, LoRA weights are subsequently merged with the LLM weights to speed up inference. In this work, we show how to exploit the unmerged LoRA’s embedding to boost the performance of Out-Of-Distribution (OOD) detectors, especially in the more challenging near-OOD scenarios. Accordingly, we demonstrate how improving OOD detection also helps in characterizing wrong predictions in downstream tasks, a fundamental aspect to improve the reliability of LLMs. Moreover, we will present a use-case in which the sensitivity of LoRA modules and OOD detection are em- ployed together to alert stakeholders about new model updates. This scenario is particularly important when LLMs are out-sourced. Indeed, test functions should be applied as soon as the model changes the version in order to adapt prompts in the downstream applications. In order to validate our method, we performed tests on Multiple Choice Question Answering datasets, by focusing on the medical domain as a fine-tuning task. Our results motivate the use of LoRA modules even after deployment, since they provide strong features for OOD detection for fine-tuning tasks and can be employed to improve the security of LLMs.",
    "summary_cn": "研究利用未合并的LoRA模块提升大语言模型的分布外检测性能，尤其在近分布外场景，增强模型可靠性和安全性。",
    "keywords": [
      "LoRA模块",
      "分布外检测",
      "大语言模型安全",
      "参数高效微调",
      "近分布外",
      "模型可靠性"
    ],
    "triple": {
      "method": "利用未合并LoRA模块特征",
      "result": "提升近分布外检测性能，改善错误预测表征",
      "contribution": "增强LLM可靠性与安全更新"
    }
  },
  {
    "id": "12a1c1f382a0",
    "venue": "ICML",
    "year": 2025,
    "title": "Multimodal Medical Code Tokenizer",
    "url": "https://icml.cc/virtual/2025/poster/45110",
    "abstract": "Foundation models trained on patient electronic health records (EHRs) require tokenizing medical data into sequences of discrete vocabulary items. Existing tokenizers treat medical codes from EHRs as isolated textual tokens. However, each medical code is defined by its textual description, its position in ontological hierarchies, and its relationships to other codes, such as disease co-occurrences and drug-treatment associations. Medical vocabularies contain more than 600,000 codes with critical information for clinical reasoning.  We introduce MedTok, a multimodal medical code tokenizer that uses the text descriptions and relational context of codes. MedTok processes text using a language model encoder and encodes the relational structure with a graph encoder. It then quantizes both modalities into a unified token space, preserving modality-specific and cross-modality information.  We integrate MedTok into five EHR models and evaluate it on operational and clinical tasks across in-patient and out-patient datasets, including outcome prediction, diagnosis classification, drug recommendation, and risk stratification. Swapping standard EHR tokenizers with MedTok improves AUPRC across all EHR models, by 4.10% on MIMIC-III, 4.78% on MIMIC-IV, and 11.32% on EHRShot, with the largest gains in drug recommendation. Beyond EHR modeling, we demonstrate using MedTok tokenizer with medical QA systems. Our results demonstrate the potential of MedTok as a unified tokenizer for medical codes, improving tokenization for medical foundation models.",
    "summary_cn": "MedTok多模态医学代码分词器结合文本描述与关系结构，提升电子健康记录模型性能，在临床任务中表现优异。",
    "keywords": [
      "多模态分词器",
      "医学代码",
      "电子健康记录",
      "图编码",
      "临床任务",
      "性能提升"
    ],
    "triple": {
      "method": "结合文本与图编码量化",
      "result": "AUPRC提升4.10%-11.32%",
      "contribution": "统一医学代码分词器"
    }
  },
  {
    "id": "23baa49cb9e8",
    "venue": "ICML",
    "year": 2025,
    "title": "Evaluating Multimodal Approaches to Medical Report Generation and Introducing a Better Metric for Generated Medical Reports",
    "url": "https://icml.cc/virtual/2025/50690",
    "abstract": "As a result of recent advancements in foundation models, including large vision-language models, several researchers have explored methods of combining multiple modalities of data as inputs for visual question answering. One key application of visual question answering in the context of the healthcare domain is automated medical report generation, where X-ray images and text-based symptom data for a patient might be provided as inputs, with the intention of generating a relevant medical report as an output. However, very few studies analyze the performance of these models alongside unimodal fine-tuned LLMs, and even fewer compare the performance of these multimodal models depending on whether they are provided symptom information as an input. In this paper, we compare the performance of a variety of approaches for generating medical reports on a dataset of Chest X-Ray medical reports, including a unimodal fine-tuned medical LLM, a multimodal model without symptom data, and a multimodal model with symptom data. Second, we introduce a new metric for evaluating the similarity between generated and reference medical reports, which we call \"sentence pairs\". Our results show that multimodal approaches to medical report generation far outperform unimodal approaches, and providing symptom data slightly improves accuracy for generated medical reports. We also find that our \"sentence pairs\" evaluation metric more closely measures similarity between generated and reference medical reports than standard techniques.",
    "summary_cn": "本研究比较了单模态与多模态方法在胸部X光报告生成中的表现，发现多模态方法显著更优，症状数据可轻微提升准确性，并提出了更优的评估指标“句子对”。",
    "keywords": [
      "医学报告生成",
      "多模态模型",
      "症状数据",
      "评估指标",
      "胸部X光",
      "视觉问答"
    ],
    "triple": {
      "method": "比较单模态与多模态模型，引入新评估指标",
      "result": "多模态方法优于单模态，症状数据提升准确性，新指标更有效",
      "contribution": "验证多模态优势，提出改进评估方法"
    }
  },
  {
    "id": "4d8f95bee42d",
    "venue": "ICML",
    "year": 2025,
    "title": "HealthGPT: A Medical Large Vision-Language Model for Unifying Comprehension and Generation via Heterogeneous Knowledge Adaptation",
    "url": "https://icml.cc/virtual/2025/poster/45007",
    "abstract": "We present HealthGPT , a powerful Medical Large Vision-Language Model (Med-LVLM) that integrates medical visual comprehension and generation capabilities within a unified autoregressive paradigm. Our bootstrapping philosophy is to progressively adapt heterogeneous comprehension and generation knowledge to pre-trained Large Language Models (LLMs). This is achieved through a novel heterogeneous low-rank adaptation (H-LoRA) technique, which is complemented by a tailored hierarchical visual perception (HVP) approach and a three-stage learning strategy (TLS) . To effectively learn the HealthGPT, we devise a comprehensive medical domain-specific comprehension and generation dataset called VL-Health . Experimental results demonstrate exceptional performance and scalability of HealthGPT in medical visual unified tasks. Our project can be accessed at https://github.com/DCDmllm/HealthGPT.",
    "summary_cn": "HealthGPT是医疗视觉语言模型，通过异构知识适应统一理解与生成，在医学视觉任务中表现卓越。",
    "keywords": [
      "HealthGPT",
      "医疗视觉语言模型",
      "异构知识适应",
      "H-LoRA",
      "VL-Health数据集",
      "统一任务"
    ],
    "triple": {
      "method": "异构低秩适应(H-LoRA)与三阶段学习",
      "result": "在医学视觉统一任务中实现卓越性能",
      "contribution": "提出首个统一医学视觉理解与生成的模型"
    }
  },
  {
    "id": "5859c409c7e7",
    "venue": "ICML",
    "year": 2025,
    "title": "Raptor: Scalable Train-Free Embeddings for 3D Medical Volumes Leveraging Pretrained 2D Foundation Models",
    "url": "https://icml.cc/virtual/2025/poster/46452",
    "abstract": "Current challenges in developing foundational models for volumetric imaging data, such as magnetic resonance imaging (MRI), stem from the computational complexity of state-of-the-art architectures in high dimensions and curating sufficiently large datasets of volumes.To address these challenges, we introduce Raptor (Random Planar Tensor Reduction), a train-free method for generating semantically rich embeddings for volumetric data. Raptor leverages a frozen 2D foundation model, pretrained on natural images, to extract visual tokens from individual cross-sections of medical volumes. These tokens are then spatially compressed using random projections, significantly reducing computational complexity while retaining rich semantic information. Extensive experiments on 10 diverse medical volume tasks verify the superior performance of Raptor over state-of-the-art methods, including those pretrained exclusively on medical volumes (+3 SuPreM, +6 MISFM, +10 Merlin, +13 VoCo, and +14 SLIViT), while entirely bypassing the need for costly training. Our results highlight Raptor's effectiveness and versatility as a foundation for advancing deep learning-based methods for medical volumes (code: github.com/sriramlab/raptor).",
    "summary_cn": "Raptor提出一种无需训练的3D医学影像嵌入方法，利用预训练2D模型提取切片特征并通过随机投影压缩，在10个任务中超越现有方法，避免昂贵训练。",
    "keywords": [
      "3D医学影像",
      "无需训练嵌入",
      "随机投影",
      "预训练2D模型",
      "语义特征提取",
      "计算效率"
    ],
    "triple": {
      "method": "利用预训练2D模型提取切片特征，结合随机投影压缩",
      "result": "在10个医学任务中性能超越现有方法，无需额外训练",
      "contribution": "提供高效、可扩展的3D医学影像基础嵌入方案"
    }
  },
  {
    "id": "780f47da2d10",
    "venue": "ICML",
    "year": 2025,
    "title": "Position: Medical Large Language Model Benchmarks Should Prioritize Construct Validity",
    "url": "https://icml.cc/virtual/2025/poster/40129",
    "abstract": "Medical large language models (LLMs) research often makes bold claims, from encoding clinical knowledge to reasoning like a physician. These claims are usually backed by evaluation on competitive benchmarks—a tradition inherited from mainstream machine learning. But how do we separate real progress from a leaderboard flex? Medical LLM benchmarks, much like those in other fields, are arbitrarily constructed using medical licensing exam questions. For these benchmarks to truly measure progress, they must accurately capture the real-world tasks they aim to represent. In this position paper, we argue that medical LLM benchmarks should—and indeed can—be empirically evaluated for their construct validity. In the psychological testing literature, “construct validity” refers to the ability of a test to measure an underlying “construct”, that is the actual conceptual target of evaluation. By drawing an analogy between LLM benchmarks and psychological tests, we explain how frameworks from this field can provide empirical foundations for validating benchmarks. To put these ideas into practice, we use real-world clinical data in proof-of-concept experiments to evaluate popular medical LLM benchmarks and report significant gaps in their construct validity. Finally, we outline a vision for a new ecosystem of medical LLM evaluation centered around the creation of valid benchmarks.",
    "summary_cn": "本文主张医学大语言模型基准应优先考虑结构效度，通过心理学测试框架评估现有基准，发现其与现实临床任务存在显著差距，并展望构建有效基准的新评估生态。",
    "keywords": [
      "医学大语言模型",
      "基准评估",
      "结构效度",
      "临床数据",
      "心理学测试",
      "模型验证"
    ],
    "triple": {
      "method": "类比心理学测试框架评估基准",
      "result": "发现现有基准与现实临床任务存在显著差距",
      "contribution": "提出以结构效度为中心的医学LLM评估新生态"
    }
  },
  {
    "id": "92f583bb9bd2",
    "venue": "ICML",
    "year": 2025,
    "title": "MedRAX: Medical Reasoning Agent for Chest X-ray",
    "url": "https://icml.cc/virtual/2025/poster/45678",
    "abstract": "Chest X-rays (CXRs) play an integral role in driving critical decisions in disease management and patient care. While recent innovations have led to specialized models for various CXR interpretation tasks, these solutions often operate in isolation, limiting their practical utility in clinical practice. We present MedRAX, the first versatile AI agent that seamlessly integrates state-of-the-art  CXR analysis tools and multimodal large language models into a unified framework. MedRAX dynamically leverages these models to address complex medical queries without requiring additional training. To rigorously evaluate its capabilities, we introduce ChestAgentBench, a comprehensive benchmark containing 2,500 complex medical queries across 7 diverse categories. Our experiments demonstrate that MedRAX achieves state-of-the-art performance compared to both open-source and proprietary models, representing a significant step toward the practical deployment of automated CXR interpretation systems. Data and code have been publicly available at https://github.com/bowang-lab/MedRAX",
    "summary_cn": "MedRAX是首个整合先进CXR分析工具与多模态大语言模型的AI代理，无需额外训练即可处理复杂医疗查询，在综合基准测试中表现优异。",
    "keywords": [
      "MedRAX",
      "胸部X光",
      "AI代理",
      "多模态大语言模型",
      "医疗推理",
      "ChestAgentBench"
    ],
    "triple": {
      "method": "整合CXR分析工具与多模态大语言模型",
      "result": "在ChestAgentBench基准上达到最优性能",
      "contribution": "推动自动化CXR解读系统实际部署"
    }
  },
  {
    "id": "605cf434d96f",
    "venue": "ICML",
    "year": 2025,
    "title": "BoxLM: Unifying Structures and Semantics of Medical Concepts for Diagnosis Prediction in Healthcare",
    "url": "https://icml.cc/virtual/2025/poster/44310",
    "abstract": "Language Models (LMs) have advanced diagnosis prediction by leveraging the semantic understanding of medical concepts in Electronic Health Records (EHRs). Despite these advancements, existing LM-based methods often fail to capture the structures of medical concepts (e.g., hierarchy structure from domain knowledge). In this paper, we propose BoxLM, a novel framework that unifies the structures and semantics of medical concepts for diagnosis prediction. Specifically, we propose a structure-semantic fusion mechanism via box embeddings, which integrates both ontology-driven and EHR-driven hierarchical structures with LM-based semantic embeddings, enabling interpretable medical concept representations.Furthermore, in the box-aware diagnosis prediction module, an evolve-and-memorize patient box learning mechanism is proposed to model the temporal dynamics of patient visits, and a volume-based similarity measurement is proposed to enable accurate diagnosis prediction. Extensive experiments demonstrate that BoxLM consistently outperforms state-of-the-art baselines, especially achieving strong performance in few-shot learning scenarios, showcasing its practical utility in real-world clinical settings.",
    "summary_cn": "BoxLM框架通过盒嵌入统一医学概念的结构与语义，结合本体和电子病历层次结构，提升诊断预测性能，尤其在少样本学习中表现优异。",
    "keywords": [
      "BoxLM",
      "诊断预测",
      "盒嵌入",
      "结构语义融合",
      "少样本学习",
      "电子病历"
    ],
    "triple": {
      "method": "盒嵌入融合结构与语义",
      "result": "超越现有基线，少样本学习表现强",
      "contribution": "提出统一框架提升诊断预测"
    }
  },
  {
    "id": "3f031141ca49",
    "venue": "ICML",
    "year": 2025,
    "title": "Modeling Cognitive and Implicit Biases in Multi-Agent Medical Systems for Clinical Diagnoses",
    "url": "https://icml.cc/virtual/2025/49339",
    "abstract": "Biases in large language models threaten diagnostic equity and patient safety, and their downstream effects on multi-agent medical systems have only recently been explored. Using a multi-agent framework that mirrored patient-physician conversations, we simulated implicit  and cognitive biases within approximately 1,700 MedQA and NEJM encounters and measured downstream effects on diagnostic accuracy, ancillary test utilization, and diagnostic breadth. We found that implicit and cognitive biases could lower diagnostic accuracy by up to 24% and 32%, respectively, with variable effects on test ordering behavior and diagnostic considerations. Our findings expand on current notions of how cognitive and implicit biases may adversely affect multi-agent medical systems, draw parallels between biases in multi-agent systems and real-world clinical contexts, and highlight the need for equitable safeguards in the development of medical agents for clinical decision-making.",
    "summary_cn": "研究模拟多智能体医疗系统中认知与隐性偏见，发现其显著降低诊断准确性，并影响测试使用与诊断广度，强调需开发公平保障措施。",
    "keywords": [
      "多智能体系统",
      "认知偏见",
      "隐性偏见",
      "诊断准确性",
      "医疗公平",
      "临床决策"
    ],
    "triple": {
      "method": "多智能体框架模拟医患对话",
      "result": "偏见降低诊断准确性达24%-32%",
      "contribution": "揭示偏见对医疗系统影响，呼吁公平保障"
    }
  },
  {
    "id": "febe95ddf485",
    "venue": "ICML",
    "year": 2025,
    "title": "Modalities Contribute Unequally: Enhancing Medical Multi-modal Learning through Adaptive Modality Token Re-balancing",
    "url": "https://icml.cc/virtual/2025/poster/45524",
    "abstract": "Motivation: Medical research and care often rely on combining different modalities (like medical images, genetic information, and patient records). However, these “multimodal” datasets face a critical challenge: data quality varies widely between different types of data and between patients. Traditional methods struggle to handle these inconsistencies, especially in complex medical scenarios where data types (like genes and pathology scans) are very different and their relevance can change. Key Insight: Not all data types (“modalities”) are equally useful for every patient or task. Some modalities might be highly informative for a specific case, while others are unreliable or irrelevant. Instead of treating all modalities the same, we need to dynamically weigh their importance and focus on the most trustworthy information. Our Solution: We propose a new approach called AMC to address these challenges. 1. Assess Modality Importance (“Top” Step): First, the model evaluates how useful each type of data (e.g., MRI scans vs. blood test results) is for the specific task, like cancer diagnosis. It does this by identifying which data types contain the clearest, most relevant information for the patient or condition at hand.2. Replace Unreliable Data with Useful Insights (“Down” Step): For each data type, the model then filters out uninformative or noisy parts and replaces them with insights from more reliable data types. For example, if a patient’s genetic data is of poor quality, the model might rely more on their imaging data instead. This “re-balancing” ensures the model focuses on the most trustworthy information from all available sources.3. Smart Fusion with a Customized Model: We design a flexible, efficient model (similar to those used in language apps like chatbots) to combine the re-balanced data. This model includes features to improve accuracy and interpretability, making it suitable for medical use where trust and clarity are essential. Impact: Tests on real-world medical datasets showed that AMC performs better than traditional methods when data quality varies. Key benefits include: More Reliable Diagnoses and Predictions: By focusing on high-quality data, the model makes more accurate decisions, which is critical for treatments and personalized care. Flexibility Across Medical Fields: AMC works well for diverse tasks, from Alzheimer’s research to cancer subtype analysis, showing its broad utility in healthcare. Interpretability: The model’s ability to quantify which data types are most important helps doctors and researchers understand why it makes certain decisions, building trust in AI-driven medical tools.This approach could pave the way for more robust, adaptable AI in healthcare, improving how we use diverse data to better understand and treat diseases.",
    "summary_cn": "提出AMC方法，通过自适应模态令牌重平衡动态评估并融合多模态医疗数据，提升数据质量不均时的诊断准确性与可解释性。",
    "keywords": [
      "多模态学习",
      "自适应重平衡",
      "医疗AI",
      "数据质量不均",
      "模态重要性评估",
      "可解释性"
    ],
    "triple": {
      "method": "自适应模态令牌重平衡（AMC）",
      "result": "在真实医疗数据上优于传统方法，提升诊断准确性",
      "contribution": "增强多模态医疗AI的鲁棒性与可解释性"
    }
  },
  {
    "id": "1e4da45741a3",
    "venue": "ICML",
    "year": 2025,
    "title": "Position: Retrieval-augmented systems can be dangerous medical communicators",
    "url": "https://icml.cc/virtual/2025/poster/40149",
    "abstract": "Patients have long sought health information online, and increasingly, they are turning to generative AI to answer their health-related queries. Given the high stakes of the medical domain, techniques like retrieval-augmented generation and citation grounding have been widely promoted as methods to reduce hallucinations and improve the accuracy of AI-generated responses and have been widely adopted into search engines. However, we argue that even when these methods produce literally accurate content drawn from source documents sans hallucinations, they can still be highly misleading. Patients may derive significantly different interpretations from AI-generated outputs than they would from reading the original source material, let alone consulting a knowledgeable clinician. Through a large-scale query analysis on topics including disputed diagnoses and procedure safety, we support our argument with quantitative and qualitative evidence of the suboptimal answers resulting from current systems. In particular, we highlight how these models tend to decontextualize facts, omit critical relevant sources, and reinforce patient misconceptions or biases. We propose a series of recommendations---such as the incorporation of communication pragmatics and enhanced comprehension of source documents---that could help mitigate these issues and extend beyond the medical domain.",
    "summary_cn": "检索增强系统在医疗沟通中可能误导患者，即使内容准确，也可能因去语境化、遗漏关键信息而强化误解。",
    "keywords": [
      "检索增强生成",
      "医疗沟通",
      "误导风险",
      "去语境化",
      "患者误解",
      "准确性"
    ],
    "triple": {
      "method": "大规模查询分析",
      "result": "系统产生次优答案，去语境化事实并遗漏关键来源",
      "contribution": "揭示检索增强系统在医疗领域的潜在危险，提出改进建议"
    }
  },
  {
    "id": "9d92d171b121",
    "venue": "ICML",
    "year": 2024,
    "title": "Unlocking the Power of Spatial and Temporal Information in Medical Multimodal Pre-training",
    "url": "https://icml.cc/virtual/2024/poster/34857",
    "abstract": "Medical vision-language pre-training methods mainly leverage the correspondence between paired medical images and radiological reports. Although multi-view spatial images and temporal sequences of image-report pairs are available in off-the-shelf multi-modal medical datasets, most existing methods have not thoroughly tapped into such extensive supervision signals. In this paper, we introduce the Med-ST framework for fine-grained spatial and temporal modeling to exploit information from multiple spatial views of chest radiographs and temporal historical records. For spatial modeling, Med-ST employs the Mixture of View Expert (MoVE) architecture to integrate different visual features from both frontal and lateral views. To achieve a more comprehensive alignment, Med-ST not only establishes the global alignment between whole images and texts but also introduces modality-weighted local alignment between text tokens and spatial regions of images. For temporal modeling, we propose a novel cross-modal bidirectional cycle consistency objective by forward mapping classification (FMC) and reverse mapping regression (RMR). By perceiving temporal information from simple to complex, Med-ST can learn temporal semantics. Experimental results across four distinct tasks demonstrate the effectiveness of Med-ST, especially in temporal classification tasks. Our code and model are available at https://github.com/SVT-Yang/MedST.",
    "summary_cn": "提出Med-ST框架，通过多视图空间建模和跨模态时序一致性目标，利用医学多模态数据中的空间和时间信息，提升医学视觉-语言预训练性能。",
    "keywords": [
      "医学多模态预训练",
      "空间建模",
      "时序建模",
      "视图专家混合",
      "跨模态对齐",
      "循环一致性"
    ],
    "triple": {
      "method": "多视图空间建模与跨模态时序一致性目标",
      "result": "在四项任务中有效，尤其在时序分类任务表现突出",
      "contribution": "开发Med-ST框架，充分利用医学数据的空间和时间信息"
    }
  },
  {
    "id": "e1da6bd99ddb",
    "venue": "ICML",
    "year": 2025,
    "title": "GraphCL: Graph-based Clustering for Semi-Supervised Medical Image Segmentation",
    "url": "https://icml.cc/virtual/2025/poster/45355",
    "abstract": "Semi-supervised learning (SSL) has made notable advancements in medical image segmentation (MIS), particularly in scenarios with limited labeled data and significantly enhancing data utilization efficiency. Previous methods primarily focus on complex training strategies to utilize unlabeled data but neglect the importance of graph structural information. Different from existing methods, we propose a graph-based clustering for semi-supervised medical image segmentation (GraphCL) by jointly modeling graph data structure in a unified deep model. The proposed GraphCL model enjoys several advantages. Firstly, to the best of our knowledge, this is the first work to model the data structure information for semi-supervised medical image segmentation (SSMIS). Secondly, to get the clustered features across different graphs, we integrate both pairwise affinities between local image features and raw features as inputs. Extensive experimental results on three standard benchmarks show that the proposed GraphCL algorithm outperforms state-of-the-art semi-supervised medical image segmentation methods.",
    "summary_cn": "提出GraphCL模型，首次在图结构中建模数据信息，用于半监督医学图像分割，通过整合局部特征亲和度与原始特征提升性能，在三个基准测试中优于现有方法。",
    "keywords": [
      "半监督学习",
      "医学图像分割",
      "图聚类",
      "数据建模",
      "特征亲和度",
      "深度学习"
    ],
    "triple": {
      "method": "图聚类建模数据结构",
      "result": "在三个基准测试中优于现有方法",
      "contribution": "首次将图结构信息引入半监督医学图像分割"
    }
  },
  {
    "id": "54a6584f7729",
    "venue": "ICML",
    "year": 2025,
    "title": "iDPA: Instance Decoupled Prompt Attention for Incremental Medical Object Detection",
    "url": "https://icml.cc/virtual/2025/poster/44405",
    "abstract": "Existing prompt-based approaches have demonstrated impressive performance in continual learning, leveraging pre-trained large-scale models for classification tasks; however, the tight coupling between foreground-background information and the coupled attention between prompts and image-text tokens present significant challenges in incremental medical object detection tasks, due to the conceptual gap between medical and natural domains. To overcome these challenges, we introduce the iDPA framework, which comprises two main components: 1) Instance-level Prompt Generation (IPG), which decouples fine-grained instance-level knowledge from images and generates prompts that focus on dense predictions, and 2) Decoupled Prompt Attention (DPA), which decouples the original prompt attention, enabling a more direct and efficient transfer of prompt information while reducing memory usage and mitigating catastrophic forgetting. We collect 13 clinical, cross-modal, multi-organ, and multi-category datasets, referred to as ODinM-13, and experiments demonstrate that iDPA outperforms existing SOTA methods, with FAP improvements of f 5.44%, 4.83%, 12.88%, and 4.59% in full data, 1-shot, 10-shot, and 50-shot settings, respectively.",
    "summary_cn": "提出iDPA框架，通过实例级提示生成与解耦提示注意力，解决增量医学目标检测中信息耦合与灾难性遗忘问题，在ODinM-13数据集上性能优于现有方法。",
    "keywords": [
      "增量学习",
      "医学目标检测",
      "提示学习",
      "灾难性遗忘",
      "实例解耦",
      "多模态数据"
    ],
    "triple": {
      "method": "实例级提示生成与解耦提示注意力",
      "result": "在ODinM-13数据集上FAP指标提升显著",
      "contribution": "提升增量医学目标检测性能并减少遗忘"
    }
  },
  {
    "id": "d9923a68b63a",
    "venue": "ICML",
    "year": 2025,
    "title": "ClinicalFMamba: Mamba-based Multimodal Medical Image Fusion for Enhanced Clinical Diagnosis",
    "url": "https://icml.cc/virtual/2025/50490",
    "abstract": "The ICML Logo above may be used on presentations. Right-click and choose\n                            download. It is a vector graphic and may be used at any scale.",
    "summary_cn": "提出ClinicalFMamba模型，基于Mamba架构融合多模态医学图像，提升临床诊断效果。",
    "keywords": [
      "医学图像融合",
      "Mamba架构",
      "多模态",
      "临床诊断",
      "深度学习"
    ],
    "triple": {
      "method": "基于Mamba的多模态融合",
      "result": "增强诊断效果",
      "contribution": "提出新融合模型"
    }
  },
  {
    "id": "fea47b68af29",
    "venue": "ICML",
    "year": 2025,
    "title": "MMedPO: Aligning Medical Vision-Language Models with Clinical-Aware Multimodal Preference Optimization",
    "url": "https://icml.cc/virtual/2025/poster/44599",
    "abstract": "The advancement of Large Vision-Language Models (LVLMs) has propelled their application in the medical field. However, Medical LVLMs (Med-LVLMs) encounter factuality challenges due to modality misalignment, where the models prioritize textual knowledge over visual input, leading to hallucinations that contradict information in medical images. Previous attempts to enhance modality alignment in Med-LVLMs through preference optimization have inadequately addressed clinical relevance in preference data, making these samples easily distinguishable and reducing alignment effectiveness. In response, we propose MMedPO, a novel multimodal medical preference optimization approach that considers the clinical relevance of preference samples to enhance Med-LVLM alignment. MMedPO curates multimodal preference data by introducing two types of dispreference: (1) plausible hallucinations injected through target Med-LVLMs or GPT-4o to produce medically inaccurate responses, and (2) lesion region neglect achieved through local lesion-noising, disrupting visual understanding of critical areas. We then calculate clinical relevance for each sample based on scores from multiple Med-LLMs and visual tools, enabling effective alignment. Our experiments demonstrate that MMedPO significantly enhances factual accuracy in Med-LVLMs, achieving substantial improvements over existing preference optimization methods by 14.2% and 51.7% on the Med-VQA and report generation tasks, respectively. Our code are available in https://github.com/aiming-lab/MMedPO}{https://github.com/aiming-lab/MMedPO.",
    "summary_cn": "提出MMedPO方法，通过引入临床相关性的多模态偏好优化，提升医学视觉语言模型的事实准确性，减少幻觉。",
    "keywords": [
      "医学视觉语言模型",
      "多模态偏好优化",
      "临床相关性",
      "事实准确性",
      "幻觉减少",
      "模态对齐"
    ],
    "triple": {
      "method": "引入临床相关性的多模态偏好数据优化",
      "result": "在Med-VQA和报告生成任务上分别提升14.2%和51.7%",
      "contribution": "增强医学模型模态对齐与事实准确性"
    }
  },
  {
    "id": "9cb38039bb9b",
    "venue": "ICML",
    "year": 2025,
    "title": "MedXpertQA: Benchmarking Expert-Level Medical Reasoning and Understanding",
    "url": "https://icml.cc/virtual/2025/poster/45718",
    "abstract": "We introduce MedXpertQA, a highly challenging and comprehensive benchmark to evaluate expert-level medical knowledge and advanced reasoning. MedXpertQA includes 4,460 questions spanning 17 specialties and 11 body systems. It includes two subsets, Text for text evaluation and MM for multimodal evaluation. Notably, MM introduces expert-level exam questions with diverse images and rich clinical information, including patient records and examination results, setting it apart from traditional medical multimodal benchmarks with simple QA pairs generated from image captions. MedXpertQA applies rigorous filtering and augmentation to address the insufficient difficulty of existing benchmarks like MedQA, and incorporates specialty board questions to improve clinical relevance and comprehensiveness. We perform data synthesis to mitigate data leakage risk and conduct multiple rounds of expert reviews to ensure accuracy and reliability. We evaluate 18 leading models on MedXpertQA. Moreover, medicine is deeply connected to real-world decision-making, providing a rich and representative setting for assessing reasoning abilities beyond mathematics and code. To this end, we develop a reasoning-oriented subset to facilitate the assessment of o1-like models.",
    "summary_cn": "MedXpertQA是一个评估专家级医学知识与推理的基准，包含4,460个问题，涵盖17个专科和11个身体系统，分为文本和多模态子集，通过严格过滤和专家审核确保质量。",
    "keywords": [
      "医学基准",
      "专家级推理",
      "多模态评估",
      "临床相关性",
      "数据合成",
      "模型评估"
    ],
    "triple": {
      "method": "数据合成与专家审核",
      "result": "构建包含文本和多模态子集的基准",
      "contribution": "提升医学AI评估的难度与可靠性"
    }
  },
  {
    "id": "9679e0296393",
    "venue": "ICML",
    "year": 2025,
    "title": "Evaluating LLMs Across Multi-Cognitive Levels: From Medical Knowledge Mastery to Scenario-Based Problem Solving",
    "url": "https://icml.cc/virtual/2025/poster/43822",
    "abstract": "Large language models (LLMs) have demonstrated remarkable performance on various medical benchmarks, but their capabilities across different cognitive levels remain underexplored. Inspired by Bloom's Taxonomy, we propose a multi-cognitive-level evaluation framework for assessing LLMs in the medical domain in this study. The framework integrates existing medical datasets and introduces tasks targeting three cognitive levels: preliminary knowledge grasp, comprehensive knowledge application, and scenario-based problem solving. Using this framework, we systematically evaluate state-of-the-art general and medical LLMs from six prominent families: Llama, Qwen, Gemma, Phi, GPT, and DeepSeek. Our findings reveal a significant performance decline as cognitive complexity increases across evaluated models, with model size playing a more critical role in performance at higher cognitive levels. Our study highlights the need to enhance LLMs' medical capabilities at higher cognitive levels and provides insights for developing LLMs suited to real-world medical applications.",
    "summary_cn": "研究基于布鲁姆分类法提出多认知层次评估框架，系统评估主流大语言模型在医学领域的表现，发现模型性能随认知复杂度增加而下降，模型规模在高认知层次中更为关键。",
    "keywords": [
      "大语言模型",
      "医学评估",
      "认知层次",
      "布鲁姆分类法",
      "模型性能",
      "场景问题解决"
    ],
    "triple": {
      "method": "多认知层次评估框架",
      "result": "模型性能随认知复杂度增加而下降",
      "contribution": "揭示模型规模在高认知层次的关键作用"
    }
  },
  {
    "id": "41b508a83252",
    "venue": "ICML",
    "year": 2024,
    "title": "Medical Unlearnable Examples: Securing Medical Data from Unauthorized Traning via Sparsity-Aware Local Masking",
    "url": "https://icml.cc/virtual/2024/37554",
    "abstract": "The rapid expansion of AI in healthcare has led to a surge in medical data generation and storage, boosting medical AI development. However, fears of unauthorized use, like training commercial AI models, hinder researchers from sharing their valuable datasets. To encourage data sharing, one promising solution is to introduce imperceptible noise into the data. This method aims to safeguard the data against unauthorized training by inducing degradation in the generalization ability of the trained model. However, they are not effective and efficient when applied to medical data, mainly due to the ignorance of the sparse nature of medical images. To address this problem, we propose the Sparsity-Aware Local Masking (SALM) method, a novel approach that selectively perturbs significant pixel regions rather than the entire image as previously. This simple yet effective approach, by focusing on local areas, significantly narrows down the search space for disturbances and fully leverages the characteristics of sparsity. Our extensive experiments across various datasets and model architectures demonstrate that SALM effectively prevents unauthorized training of different models and outperforms previous SoTA data protection methods.",
    "summary_cn": "提出SALM方法，通过稀疏感知局部掩蔽在医学图像中引入噪声，防止未经授权的AI模型训练，提升数据保护效果。",
    "keywords": [
      "医学数据安全",
      "不可学习示例",
      "稀疏感知",
      "局部掩蔽",
      "AI模型保护",
      "数据共享"
    ],
    "triple": {
      "method": "稀疏感知局部掩蔽(SALM)",
      "result": "有效阻止不同模型未经授权训练，优于现有方法",
      "contribution": "提升医学数据保护效率与效果"
    }
  },
  {
    "id": "8d2a91793997",
    "venue": "ICML",
    "year": 2025,
    "title": "Multi-Modal Medical Image Augmentation for Controlled Heterogeneity and Fair Outcomes",
    "url": "https://icml.cc/virtual/2025/50903",
    "abstract": "Limited data in medical imaging exacerbate class imbalance and fairness gaps, undermining deep-learning across diverse patient subgroups. GAN- and diffusion-based augmenters can expand datasets but often lack precise control over multiple clinical attributes and fail to cover the full range of real-world variability. We introduce a four-step augmentation pipeline. First, an automated scoring function identifies which classes or regions most urgently need synthetic examples. Second, we construct sketch–image–text triplets from real scans, embedding age, sex, and disease labels. Third, we fine-tune a sketch-conditioned diffusion network for reliable sketch-to-image synthesis and boost variability by generating multiple, similarity-penalized sketches per case. Fourth, we propose a novel diversity metric that simultaneously measures semantic feature-space coverage and pixel-level dispersion—unlike FID or IS, it captures intra-class spread and boundary sharpness without human annotations. Experiments on chest X-rays show our pipeline delivers high-fidelity, diverse images aligned with user-specified conditions, substantially improving fairness and generalizability.",
    "summary_cn": "提出四步医学图像增强流程，通过自动评分、多模态嵌入、扩散网络和多样性度量，生成可控合成图像，提升公平性和泛化能力。",
    "keywords": [
      "医学图像增强",
      "扩散模型",
      "公平性",
      "多样性度量",
      "多模态嵌入",
      "合成数据"
    ],
    "triple": {
      "method": "四步增强流程（评分、嵌入、扩散合成、多样性度量）",
      "result": "生成高保真、多样化的合成图像，改善公平性和泛化性",
      "contribution": "提出可控多模态增强方法，提升数据覆盖和模型公平性"
    }
  },
  {
    "id": "375d3aeb639e",
    "venue": "ICML",
    "year": 2025,
    "title": "Distribution-aware Fairness Learning in Medical Image Segmentation From A Control-Theoretic Perspective",
    "url": "https://icml.cc/virtual/2025/poster/46110",
    "abstract": "Ensuring fairness in medical image segmentation is critical due to biases in imbalanced clinical data acquisition caused by demographic attributes (e.g., age, sex, race) and clinical factors (e.g., disease severity). To address these challenges, we introduce Distribution-aware Mixture of Experts (dMoE), inspired by optimal control theory. We provide a comprehensive analysis of its underlying mechanisms and clarify dMoE's role in adapting to heterogeneous distributions in medical image segmentation. Furthermore, we integrate dMoE into multiple network architectures, demonstrating its broad applicability across diverse medical image analysis tasks. By incorporating demographic and clinical factors, dMoE achieves state-of-the-art performance on two 2D benchmark datasets and a 3D in-house dataset. Our results highlight the effectiveness of dMoE in mitigating biases from imbalanced distributions, offering a promising approach to bridging control theory and medical image segmentation within fairness learning paradigms. The source code is available at https://github.com/tvseg/dMoE.",
    "summary_cn": "提出分布感知专家混合模型（dMoE），基于最优控制理论解决医学图像分割中的公平性问题，有效缓解数据不平衡带来的偏见，在多个数据集上实现先进性能。",
    "keywords": [
      "医学图像分割",
      "公平性学习",
      "分布感知",
      "专家混合模型",
      "最优控制理论",
      "数据不平衡"
    ],
    "triple": {
      "method": "分布感知专家混合模型（dMoE）",
      "result": "在2D基准和3D内部数据集上实现先进性能",
      "contribution": "结合控制理论缓解医学图像分割中的分布偏见"
    }
  },
  {
    "id": "bd803ee31367",
    "venue": "ICML",
    "year": 2024,
    "title": "Contextualized Policy Recovery: Modeling and Interpreting Medical Decisions with Adaptive Imitation Learning",
    "url": "https://icml.cc/virtual/2024/poster/33778",
    "abstract": "The ICML Logo above may be used on presentations. Right-click and choose\n                            download. It is a vector graphic and may be used at any scale.",
    "summary_cn": "提出情境化策略恢复方法，通过自适应模仿学习建模和解释医疗决策，提升决策可解释性。",
    "keywords": [
      "情境化策略恢复",
      "自适应模仿学习",
      "医疗决策建模",
      "决策解释",
      "机器学习",
      "医疗人工智能"
    ],
    "triple": {
      "method": "自适应模仿学习",
      "result": "建模和解释医疗决策",
      "contribution": "提升决策可解释性"
    }
  },
  {
    "id": "8f66504aa1f3",
    "venue": "ICML",
    "year": 2025,
    "title": "Visual and Domain Knowledge for Professional-level Graph-of-Thought Medical Reasoning",
    "url": "https://icml.cc/virtual/2025/poster/43761",
    "abstract": "Medical Visual Question Answering (MVQA) requires AI models to answer questions related to medical images, offering significant potential to assist medical professionals in evaluating and diagnosing diseases, thereby improving early interventions. However, existing MVQA datasets primarily focus on basic questions regarding visual perception and pattern recognition, without addressing the more complex questions that are critical in clinical diagnosis and decision-making. This paper introduces a new benchmark designed for professional-level medical reasoning, simulating the decision-making process. We achieve this by collecting MRI and clinical data related to Hypoxic-Ischemic Encephalopathy, enriched with expert annotations and insights. Building on this data, we generate clinical question-answer pairs and MRI interpretations to enable comprehensive diagnosis, interpretation, and prediction of neurocognitive outcomes. Our evaluation of current large vision-language models (LVLMs) shows limited performance on this benchmark, highlighting both the challenges of the task and the importance of this benchmark for advancing medical AI. Furthermore, we propose a novel ``Clinical Graph of Thoughts\" model, which integrates domain-specific medical knowledge and clinical reasoning processes with the interpretive abilities of LVLMs. The model demonstrates promising results, achieving around 15\\% absolute gain on the most important neurocognitive outcome task, while the benchmark still reveals substantial opportunities for further research innovation.",
    "summary_cn": "本文提出专业级医学推理新基准，针对缺氧缺血性脑病，结合MRI与临床数据生成问答对。现有大视觉语言模型表现有限，而新提出的临床思维图模型集成领域知识，在神经认知结果任务上提升约15%。",
    "keywords": [
      "医学视觉问答",
      "专业级推理",
      "缺氧缺血性脑病",
      "大视觉语言模型",
      "临床思维图",
      "神经认知预测"
    ],
    "triple": {
      "method": "构建基准与临床思维图模型",
      "result": "新模型在关键任务上提升约15%",
      "contribution": "推动医学AI专业推理发展"
    }
  },
  {
    "id": "d7c3769cee53",
    "venue": "ICML",
    "year": 2025,
    "title": "TinyMIG: Transferring Generalization from Vision Foundation Models to Single-Domain Medical Imaging",
    "url": "https://icml.cc/virtual/2025/poster/45472",
    "abstract": "Medical imaging faces significant challenges in single-domain generalization (SDG) due to the diversity of imaging devices and the variability among data collection centers. To address these challenges, we propose \\textbf{TinyMIG}, a framework designed to transfer generalization capabilities from vision foundation models to medical imaging SDG. TinyMIG aims to enable lightweight specialized models to mimic the strong generalization capabilities of foundation models in terms of both global feature distribution and local fine-grained details during training. Specifically, for global feature distribution, we propose a Global Distribution Consistency Learning strategy that mimics the prior distributions of the foundation model layer by layer. For local fine-grained details, we further design a Localized Representation Alignment method, which promotes semantic alignment and generalization distillation between the specialized model and the foundation model. These mechanisms collectively enable the specialized model to achieve robust performance in diverse medical imaging scenarios. Extensive experiments on large-scale benchmarks demonstrate that TinyMIG, with extremely low computational cost, significantly outperforms state-of-the-art models, showcasing its superior SDG capabilities. All the code and model weights will be publicly available.",
    "summary_cn": "TinyMIG框架将视觉基础模型的泛化能力迁移至医学影像单域泛化，通过全局分布一致性和局部表示对齐，以极低计算成本显著提升性能。",
    "keywords": [
      "单域泛化",
      "医学影像",
      "基础模型",
      "迁移学习",
      "轻量模型",
      "泛化能力"
    ],
    "triple": {
      "method": "全局分布一致性学习与局部表示对齐",
      "result": "在基准测试中显著超越现有模型",
      "contribution": "实现轻量模型高效单域泛化"
    }
  },
  {
    "id": "e904e4da428a",
    "venue": "ICML",
    "year": 2025,
    "title": "LangDAug: Langevin Data Augmentation for Multi-Source Domain Generalization in Medical Image Segmentation",
    "url": "https://icml.cc/virtual/2025/poster/45607",
    "abstract": "Medical image segmentation models often struggle to generalize across different domains due to various reasons. Domain Generalization (DG) methods overcome this either through representation learning or data augmentation (DA). While representation learning methods seek domain-invariant features, they often rely on ad-hoc techniques and lack formal guarantees. DA methods, which enrich model representations through synthetic samples, have shown comparable or superior performance to representation learning approaches. We propose LangDAug, a novel Lang evin D ata Aug mentation for multi-source domain generalization in 2D medical image segmentation. LangDAug leverages Energy-Based Models (EBMs) trained via contrastive divergence to traverse between source domains, generating intermediate samples through Langevin dynamics. Theoretical analysis shows that LangDAug induces a regularization effect, and for GLMs, it upper-bounds the Rademacher complexity by the intrinsic dimensionality of the data manifold. Through extensive experiments on Fundus segmentation and 2D MRI prostate segmentation benchmarks, we show that LangDAug outperforms state-of-the-art domain generalization methods and effectively complements existing domain-randomization approaches. The codebase for our method is available at https://github.com/backpropagator/LangDAug.",
    "summary_cn": "提出LangDAug方法，利用基于能量的模型和Langevin动力学生成中间样本，增强医学图像分割的跨域泛化能力，在眼底和MRI前列腺分割基准测试中表现优异。",
    "keywords": [
      "医学图像分割",
      "域泛化",
      "数据增强",
      "基于能量的模型",
      "Langevin动力学",
      "多源域"
    ],
    "triple": {
      "method": "基于能量的模型与Langevin动力学生成中间样本",
      "result": "在眼底和MRI前列腺分割基准上超越现有域泛化方法",
      "contribution": "提出理论正则化框架并补充现有域随机化方法"
    }
  },
  {
    "id": "5cf30f000ea7",
    "venue": "ICML",
    "year": 2024,
    "title": "MH-pFLID: Model Heterogeneous personalized Federated Learning via Injection and Distillation for Medical Data Analysis",
    "url": "https://icml.cc/virtual/2024/poster/34360",
    "abstract": "Federated learning is widely used in medical applications for training global models without needing local data access, but varying computational capabilities and network architectures (system heterogeneity) across clients pose significant challenges in effectively aggregating information from non-independently and identically distributed (non-IID) data (statistic heterogeneity). Current federated learning methods using knowledge distillation require public datasets, raising privacy and data collection issues. Additionally, these datasets require additional local computing and storage resources, which is a burden for medical institutions with limited hardware conditions. In this paper, we introduce a novel federated learning paradigm, named Model Heterogeneous personalized Federated Learning via Injection and Distillation (MH-pFLID). Our framework leverages a lightweight messenger model, eliminating the need for public datasets and reducing the training cost for each client. We also develops receiver and transmitter modules for each client to separate local biases from generalizable information, reducing biased data collection and mitigating client drift. Our experiments on various medical tasks including image classification, image segmentation, and time-series classification, show MH-pFLID outperforms state-of-the-art methods in all these areas and has good generalizability.",
    "summary_cn": "提出MH-pFLID框架，通过注入与蒸馏实现模型异构的个性化联邦学习，无需公共数据集，降低客户端负担，在多项医疗任务中表现优异。",
    "keywords": [
      "联邦学习",
      "模型异构",
      "知识蒸馏",
      "医疗数据分析",
      "个性化学习",
      "隐私保护"
    ],
    "triple": {
      "method": "轻量信使模型与收发模块",
      "result": "多项医疗任务性能超越现有方法",
      "contribution": "无需公共数据集，降低计算负担，提升泛化能力"
    }
  },
  {
    "id": "53b99b710e18",
    "venue": "ICML",
    "year": 2025,
    "title": "Efficient Graph Neural Architecture Search for Medical Imaging in Real-World Clinical Settings",
    "url": "https://icml.cc/virtual/2025/50548",
    "abstract": "Deploying deep learning in clinical settings requires balancing accuracy with limited computational resources. This is especially challenging in multitask medical imaging, where shared encoders reduce redundancy but task-specific heads remain memory-intensive. We propose Efficient Graph Neural Architecture Search (EGNAS), a gradient-based method that explores a graph-structured space to find compact, task-specific predictors. EGNAS jointly optimizes accuracy and model size using a Pareto-efficient strategy. Evaluated on six MedNIST tasks, it reduces head size by 2.1x on average without performance loss. We further validate EGNAS in a real-world deployment on a low-resource clinical laptop in Algeria, demonstrating its practical utility for resource-constrained healthcare.",
    "summary_cn": "提出EGNAS方法，通过图结构搜索优化多任务医学影像模型，在保持精度的同时平均减少2.1倍头部尺寸，适用于资源受限的临床环境。",
    "keywords": [
      "图神经架构搜索",
      "多任务医学影像",
      "资源受限部署",
      "梯度优化",
      "临床实用性",
      "模型压缩"
    ],
    "triple": {
      "method": "基于梯度的图神经架构搜索",
      "result": "头部尺寸减少2.1倍且性能无损",
      "contribution": "实现资源受限临床环境的高效部署"
    }
  },
  {
    "id": "752aa3614e80",
    "venue": "ICML",
    "year": 2024,
    "title": "CARES: A Comprehensive Benchmark of Trustworthiness in Medical Vision Language Models",
    "url": "https://icml.cc/virtual/2024/39446",
    "abstract": "Artificial intelligence has significantly impacted medical applications, particularly with the advent of Medical Large Vision Language Models (Med-LVLMs), sparking optimism for the future of automated and personalized healthcare. However, the trustworthiness of Med-LVLMs remains unverified, posing significant risks for future model deployment. In this paper, we introduce CARES and aim to comprehensively evaluate the Trustworthiness of Med-LVLMs across the medical domain. We assess the trustworthiness of Med-LVLMs across five dimensions, including trustfulness, fairness, safety, privacy, and robustness. CARES comprises about 41K question-answer pairs in both closed and open-ended formats, covering 16 medical image modalities and 27 anatomical regions. Our analysis reveals that the models consistently exhibit concerns regarding trustworthiness, often displaying factual inaccuracies and failing to maintain fairness across different demographic groups. Furthermore, they are vulnerable to attacks and demonstrate a lack of privacy awareness.",
    "summary_cn": "CARES基准全面评估医疗视觉语言模型的可信度，涵盖五个维度，揭示模型存在事实错误、公平性不足、易受攻击及隐私意识薄弱等问题。",
    "keywords": [
      "医疗视觉语言模型",
      "可信度评估",
      "公平性",
      "安全性",
      "隐私保护",
      "基准测试"
    ],
    "triple": {
      "method": "构建多维度评估基准CARES",
      "result": "模型存在可信度问题，如事实错误和公平性不足",
      "contribution": "提供首个全面医疗模型可信度评估框架"
    }
  },
  {
    "id": "1d5ff1b3c65e",
    "venue": "ICML",
    "year": 2024,
    "title": "Towards Safe Large Language Models for Medicine",
    "url": "https://icml.cc/virtual/2024/37543",
    "abstract": "As large language models (LLMs) develop ever-improving capabilities and are applied in real-world settings, it is important to understand their safety. While initial steps have been taken to evaluate the safety of general-knowledge LLMs, exposing some weaknesses, the safety of medical LLMs has not been sufficiently evaluated despite their high risks to personal health and safety, public health and safety, patient rights, and human rights. To address this gap, we conduct, to our knowledge, the first study of its kind to evaluate and improve the safety of medical LLMs. We find that 1) current medical LLMs do not meet standards of general or medical safety, as they readily comply with harmful requests and that 2) fine-tuning medical LLMs on safety demonstrations significantly improves their safety. Along the way, we also present a definition of medical safety for LLMs and develop a benchmark dataset to evaluate and train for medical safety in LLMs. At the intersection of research on machine learning safety and medical machine learning, this work casts light on the status quo of the safety of medical LLMs and motivates future work in this area, mitigating the risks of harm of LLMs in medicine.",
    "summary_cn": "本研究首次评估并提升医疗大语言模型的安全性，发现现有模型存在安全隐患，但通过安全演示微调可显著改善。",
    "keywords": [
      "医疗大语言模型",
      "安全性评估",
      "微调",
      "基准数据集",
      "医学机器学习",
      "风险缓解"
    ],
    "triple": {
      "method": "安全演示微调",
      "result": "模型安全性显著提升",
      "contribution": "定义医疗安全并创建评估基准"
    }
  },
  {
    "id": "4e5a6cc4278d",
    "venue": "ICML",
    "year": 2024,
    "title": "Towards Dynamic Feature Acquisition on Medical Time Series by Maximizing Conditional Mutual Information",
    "url": "https://icml.cc/virtual/2024/36140",
    "abstract": "Knowing which features of a multivariate time series to measure and when is a key task in medicine, wearables, and robotics. Better acquisition policies can reduce costs while maintaining or even improving the performance of downstream predictors. Inspired by the maximization of conditional mutual information, we propose an approach to train acquirers end-to-end using only the downstream loss. We show that our method outperforms random acquisition policy, matches a model with an unrestrained budget, but does not yet overtake a static acquisition strategy. We highlight the assumptions and outline avenues for future work.",
    "summary_cn": "本文提出一种基于条件互信息最大化的动态特征获取方法，用于医学时间序列，旨在降低成本并保持预测性能。",
    "keywords": [
      "动态特征获取",
      "条件互信息",
      "医学时间序列",
      "端到端训练",
      "成本降低",
      "预测性能"
    ],
    "triple": {
      "method": "条件互信息最大化",
      "result": "优于随机策略，匹配无限制预算模型",
      "contribution": "提出端到端动态获取方法"
    }
  },
  {
    "id": "24aa432dd6a5",
    "venue": "ICML",
    "year": 2025,
    "title": "Breaking the Barrier of Hard Samples: A Data-Centric Approach to Synthetic Data for Medical Tasks",
    "url": "https://icml.cc/virtual/2025/poster/45228",
    "abstract": "Data scarcity and quality issues remain significant barriers to developing robust predictive models in medical research. Traditional reliance on real-world data often leads to biased models with poor generalizability across diverse patient populations. Synthetic data generation has emerged as a promising solution, yet challenges related to these sample's representativeness and effective utilization persist. This paper introduces Profile2Gen, a novel data-centric framework designed to guide the generation and refinement of synthetic data, focusing on addressing hard-to-learn samples in regression tasks. We conducted approximately 18,000 experiments to validate its effectiveness across six medical datasets, utilizing seven state-of-the-art generative models. Results demonstrate that refined synthetic samples can reduce predictive errors and enhance model reliability. Additionally, we generalize the DataIQ framework to support regression tasks, enabling its application in broader contexts. Statistical analyses confirm that our approach achieves equal or superior performance compared to models trained exclusively on real data.",
    "summary_cn": "本文提出Profile2Gen框架，通过生成和优化合成数据解决医学回归任务中难样本问题，在六个数据集上验证其能降低预测误差并提升模型可靠性。",
    "keywords": [
      "合成数据",
      "医学回归",
      "难样本",
      "数据生成",
      "模型可靠性",
      "Profile2Gen"
    ],
    "triple": {
      "method": "Profile2Gen框架与DataIQ扩展",
      "result": "合成数据降低预测误差，性能等同或优于纯真实数据",
      "contribution": "提出数据中心方法优化合成数据，提升模型泛化能力"
    }
  },
  {
    "id": "55b84559d750",
    "venue": "ICML",
    "year": 2025,
    "title": "The Boundary versus The Core: A Quantitative Comparison of TotalSegmentator and MOOSE Performance in Medical Image Segmentation",
    "url": "https://icml.cc/virtual/2025/50452",
    "abstract": "The rise of total-body PET/CT imaging necessitates robust automated segmentation tools, fueling a debate between using general-purpose \"foundation\" models and domain-specific \"specialised\" models. This paper presents a rigorous, quantitative comparison between TotalSegmentator, a foundation model trained on over 1,200 CT scans, and MOOSE, a specialised tool designed for PET/CT analysis. We evaluated both models on public datasets for spleen and vertebrae segmentation, using a comprehensive suite of metrics covering both volumetric overlap (Dice, Jaccard) and boundary precision (Hausdorff Distance). Our results reveal a critical trade-off: TotalSegmentator delivers significantly superior boundary accuracy for all tested structures, making it the premier choice for precision-critical tasks, such as radiotherapy planning. Conversely, MOOSE achieves higher volumetric overlap for vertebrae, suggesting better performance in capturing the core anatomical volume. However, this comes at the cost of severe and frequent boundary errors. We conclude that TotalSegmentator has better performance in Total-body PET/CT images segmentation. The optimal choice is application-dependent, requiring a careful balance between the need for precise surface delineation and accurate core volume assessment. Our findings provide evidence-based guidance for researchers and clinicians in selecting the appropriate AI tool for their specific quantitative imaging needs.",
    "summary_cn": "本研究对比了通用模型TotalSegmentator与专用模型MOOSE在医学图像分割中的表现。TotalSegmentator在边界精度上更优，适合放疗规划；MOOSE在椎体体积重叠上更好，但边界误差严重。选择取决于具体应用需求。",
    "keywords": [
      "医学图像分割",
      "TotalSegmentator",
      "MOOSE",
      "边界精度",
      "体积重叠",
      "PET/CT"
    ],
    "triple": {
      "method": "使用Dice、Jaccard和Hausdorff距离等指标对比模型",
      "result": "TotalSegmentator边界精度更高，MOOSE体积重叠更好但边界误差大",
      "contribution": "为临床选择AI工具提供基于证据的指导"
    }
  },
  {
    "id": "46979724a95e",
    "venue": "ICML",
    "year": 2025,
    "title": "Introducing 3D Representation for Dense Volume-to-Volume Translation via Score Fusion",
    "url": "https://icml.cc/virtual/2025/poster/45130",
    "abstract": "In volume-to-volume translations in medical images, existing models often struggle to capture the inherent volumetric distribution using 3D voxel-space representations, due to high computational dataset demands. We present Score-Fusion, a novel volumetric translation model that effectively learns 3D representations by ensembling perpendicularly trained 2D diffusion models in score function space. By carefully initializing our model to start with an average of 2D models as in existing models, we reduce 3D training to a fine-tuning process, mitigating computational and data demands. Furthermore, we explicitly design the 3D model's hierarchical layers to learn ensembles of 2D features, further enhancing efficiency and performance. Moreover, Score-Fusion naturally extends to multi-modality settings by fusing diffusion models conditioned on different inputs for flexible, accurate integration. We demonstrate that 3D representation is essential for better performance in downstream recognition tasks, such as tumor segmentation, where most segmentation models are based on 3D representation. Extensive experiments demonstrate that Score-Fusion achieves superior accuracy and volumetric fidelity in 3D medical image super-resolution and modality translation. Additionally, we extend Score-Fusion to video super-resolution by integrating 2D diffusion models on time-space slices with a spatial-temporal video diffusion backbone, highlighting its potential for general-purpose volume translation and providing broader insight into learning-based approaches for score function fusion.",
    "summary_cn": "提出Score-Fusion模型，通过融合垂直训练的2D扩散模型学习3D表示，降低计算需求，提升医学图像超分辨率和模态转换的精度与体积保真度。",
    "keywords": [
      "3D表示",
      "体积转换",
      "扩散模型",
      "分数融合",
      "医学图像",
      "超分辨率"
    ],
    "triple": {
      "method": "融合2D扩散模型分数函数",
      "result": "提升3D图像转换精度与效率",
      "contribution": "降低3D训练计算需求并扩展至多模态"
    }
  },
  {
    "id": "d7eb4fd1756f",
    "venue": "ICML",
    "year": 2025,
    "title": "Breaking Barriers in Hard Samples: Guiding the Generation of Synthetic Data for Medical Tasks with Data-centric Approach",
    "url": "https://icml.cc/virtual/2025/46748",
    "abstract": "Data scarcity and quality issues remain significant barriers to developing robust predictive models in medical research. Traditional reliance on real-world data often leads to biased models with poor generalizability across diverse patient populations. Synthetic data generation has emerged as a promising solution, yet challenges related to these sample's representativeness and effective utilization persist. This paper introduces Profile2Gen, a novel data-centric framework designed to guide the generation and refinement of synthetic data, focusing on addressing hard-to-learn samples in regression tasks. We conducted approximately 18,000 experiments to validate its effectiveness across six medical datasets, utilizing seven state-of-the-art generative models. Results demonstrate that refined synthetic samples can reduce predictive errors and enhance model reliability. Additionally, we generalize the DataIQ framework to support regression tasks, enabling its application in broader contexts. Statistical analyses confirm that our approach achieves equal or superior performance compared to models trained exclusively on real data.",
    "summary_cn": "本文提出Profile2Gen框架，通过数据驱动方法指导合成数据生成，针对医学回归任务中的难样本，提升模型预测精度与可靠性。",
    "keywords": [
      "合成数据生成",
      "数据驱动方法",
      "医学回归任务",
      "难样本处理",
      "模型可靠性",
      "Profile2Gen框架"
    ],
    "triple": {
      "method": "Profile2Gen框架指导合成数据生成与精炼",
      "result": "减少预测误差，性能等同或优于仅用真实数据训练的模型",
      "contribution": "推广DataIQ至回归任务，提升模型在医学数据中的泛化能力"
    }
  },
  {
    "id": "9d72d6d651d0",
    "venue": "ICML",
    "year": 2024,
    "title": "DiffusionBlend: Learning 3D Image Prior through Position-aware Diffusion Score Blending for 3D Computed Tomography Reconstruction",
    "url": "https://icml.cc/virtual/2024/37056",
    "abstract": "The ICML Logo above may be used on presentations. Right-click and choose\n                            download. It is a vector graphic and may be used at any scale.",
    "summary_cn": "提出DiffusionBlend方法，通过位置感知扩散分数融合学习3D图像先验，用于3D CT重建，提升重建质量。",
    "keywords": [
      "3D CT重建",
      "扩散模型",
      "图像先验",
      "位置感知",
      "分数融合",
      "深度学习"
    ],
    "triple": {
      "method": "位置感知扩散分数融合",
      "result": "提升3D CT重建质量",
      "contribution": "学习3D图像先验"
    }
  },
  {
    "id": "a260af1692ed",
    "venue": "ICML",
    "year": 2025,
    "title": "CLIMB: Data Foundations for Large Scale Multimodal Clinical Foundation Models",
    "url": "https://icml.cc/virtual/2025/poster/45167",
    "abstract": "Recent advances in clinical AI have enabled remarkable progress across many clinical domains. However, existing benchmarks and models are primarily limited to a small set of modalities and tasks, which hinders the development of large-scale multimodal methods that can make holistic assessments of patient health and well-being. To bridge this gap, we introduce Clinical Large-scale Integrative Multimodal Benchmark (CLIMB), a comprehensive clinical benchmark unifying diverse clinical data across imaging, language, temporal, and graph modalities. CLIMB comprises 4.51 million patient samples totaling 19.01 terabytes distributed across 2D imaging, 3D video, time series, graphs, and multimodal data. Through extensive empirical evaluation, we demonstrate that multitask pretraining significantly improves performance on understudied domains, achieving up to 29% improvement in ultrasound and 23% in ECG analysis over single-task learning. Pretraining on CLIMB also effectively improves models' generalization capability to new tasks, and strong unimodal encoder performance translates well to multimodal performance when paired with task-appropriate fusion strategies. Our findings provide a foundation for new architecture designs and pretraining strategies to adavance clinical AI research. Code is released at https://github.com/DDVD233/climb.",
    "summary_cn": "CLIMB是一个大规模多模态临床基准数据集，整合了影像、语言、时序和图数据，包含451万患者样本。研究表明，多任务预训练显著提升超声和心电图分析性能，并增强模型泛化能力。",
    "keywords": [
      "多模态基准",
      "临床AI",
      "预训练",
      "泛化能力",
      "数据整合",
      "医学影像"
    ],
    "triple": {
      "method": "构建CLIMB多模态临床基准数据集并进行多任务预训练",
      "result": "超声和心电图分析性能提升达29%和23%，模型泛化能力增强",
      "contribution": "为临床AI研究提供大规模数据基础和架构设计指导"
    }
  },
  {
    "id": "b24412955c3f",
    "venue": "ICML",
    "year": 2024,
    "title": "Training-free Design of Augmentations with Data-centric Principles",
    "url": "https://icml.cc/virtual/2024/36733",
    "abstract": "The remarkable advancements in Artificial Intelligence (AI) and Deep Learning owe significantly to the evolution of informative datasets. With the emerging concept of ``Data-centric AI'', there has been a shift in focus from developing deep neural networks (DNNs) to crafting high-quality training datasets. However, current data-centric approaches predominantly rely on empirics or heavy DNN training costs, lacking established design principles. Our work concentrates on data augmentation, a key technique for enhancing data quality. Grounded by the recent development of deep learning theory, we discover principled metrics that effectively gauge both data quality and its interaction with DNNs. Crucially, these principles can be calculated without the need for extensive DNN training, enabling training-free augmentation design with minimal computation costs. Comprehensive experiments validate that our principles are strongly aligned with optimal choices of augmentations used in practice. Our method is particularly beneficial in domain-specific fields like medical image analysis, where the optimal augmentation strategy and the data's inductive bias are often unclear. Our results demonstrate consistent improvements over existing state-of-the-art segmentation methods across various medical imaging datasets.We attach our code at: https://anonymous.4open.science/r/240523 anonymous repo-C828/.",
    "summary_cn": "本研究提出无需训练的数据增强设计原则，通过理论推导的指标评估数据质量与模型交互，在医学图像分割中验证了其有效性。",
    "keywords": [
      "数据增强",
      "数据为中心AI",
      "无训练设计",
      "医学图像分析",
      "深度学习理论",
      "数据质量评估"
    ],
    "triple": {
      "method": "基于深度学习理论推导原则性指标",
      "result": "增强策略与最优选择高度一致，提升医学图像分割性能",
      "contribution": "实现低计算成本的无训练数据增强设计"
    }
  },
  {
    "id": "8dbf93d41bda",
    "venue": "ICML",
    "year": 2025,
    "title": "OrthoGraphRAG: Enhancing Clinical Decision Making with Multi-Level Knowledge Graphs",
    "url": "https://icml.cc/virtual/2025/51234",
    "abstract": "Large Language Models (LLMs) face accuracy and complex reasoning challenges in specialized medical domains like orthopedics. We introduce OrthoGraphRAG, a multi-level Graph Retrieval-Augmented Generation (GraphRAG) framework, to address these issues. OrthoGraphRAG constructs a novel multi-level knowledge graph linking private clinical knowledge with public UMLS data, building on recent medical GraphRAG advancements. The framework retrieves query-entity-based subgraphs, augments them with clinical note text, allowing an LLM to synthesize informed responses from combined graph and textual evidence. Evaluated on real-world orthopedic clinic letters with diverse query complexities, OrthoGraphRAG demonstrated effectiveness, particularly in contextual reasoning integrating private patient data with broader medical knowledge. This multi-level GraphRAG approach offers a promising path to safer, more capable, and contextually aware LLMs for specialized clinical applications. Our code is released at: https://github.com/venkateshtata/OrthoGraphRAG.",
    "summary_cn": "OrthoGraphRAG提出多级知识图谱增强生成框架，结合私有临床数据与公共UMLS知识，提升骨科领域LLM的准确性和复杂推理能力。",
    "keywords": [
      "GraphRAG",
      "知识图谱",
      "临床决策",
      "骨科",
      "LLM增强",
      "多级检索"
    ],
    "triple": {
      "method": "多级知识图谱构建与检索增强生成",
      "result": "在骨科临床信件评估中有效提升上下文推理能力",
      "contribution": "为专业医疗应用提供更安全、上下文感知的LLM框架"
    }
  },
  {
    "id": "b2c6b6e52eae",
    "venue": "ICML",
    "year": 2024,
    "title": "Exploiting Negative Samples: A Catalyst for Cohort Discovery in Healthcare Analytics",
    "url": "https://icml.cc/virtual/2024/poster/33276",
    "abstract": "In healthcare analytics, addressing binary diagnosis or prognosis tasks presents unique challenges due to the inherent asymmetry between positive and negative samples. While positive samples, indicating patients with a disease, are defined based on stringent medical criteria, negative samples are defined in an open-ended manner and remain underexplored in prior research. To bridge this gap, we propose an innovative approach to facilitate cohort discovery within negative samples, leveraging a Shapley-based exploration of interrelationships between these samples, which holds promise for uncovering valuable insights concerning the studied disease, and related comorbidity and complications. We quantify each sample’s contribution using data Shapley values, subsequently constructing the Negative Sample Shapley Field to model the distribution of all negative samples. Next, we transform this field through manifold learning, preserving the essential data structure information while imposing an isotropy constraint in data Shapley values. Within this transformed space, we pinpoint cohorts of medical interest via density-based clustering. We empirically evaluate the effectiveness of our approach on the real-world electronic medical records from National University Hospital in Singapore, yielding clinically valuable insights aligned with existing knowledge, and benefiting medical research and clinical decision-making.",
    "summary_cn": "本研究提出一种利用Shapley值和流形学习分析医疗负样本的方法，通过密度聚类发现临床相关队列，在新加坡国立大学医院电子病历上验证有效，为疾病研究提供新见解。",
    "keywords": [
      "负样本分析",
      "Shapley值",
      "流形学习",
      "密度聚类",
      "队列发现",
      "医疗分析"
    ],
    "triple": {
      "method": "Shapley值结合流形学习与密度聚类",
      "result": "从负样本中识别出临床相关队列，验证与现有知识一致",
      "contribution": "为医疗分析提供负样本探索新方法，辅助临床决策"
    }
  },
  {
    "id": "59636bebccc9",
    "venue": "ICML",
    "year": 2024,
    "title": "Unsupervised Domain Adaptation for Anatomical Structure Detection in Ultrasound Images",
    "url": "https://icml.cc/virtual/2024/poster/33163",
    "abstract": "The ICML Logo above may be used on presentations. Right-click and choose\n                            download. It is a vector graphic and may be used at any scale.",
    "summary_cn": "该研究提出一种无监督域自适应方法，用于超声图像中的解剖结构检测，旨在解决跨域数据分布差异问题，提升模型在目标域上的检测性能。",
    "keywords": [
      "无监督域自适应",
      "超声图像",
      "解剖结构检测",
      "跨域学习",
      "医学影像分析"
    ],
    "triple": {
      "method": "无监督域自适应方法",
      "result": "提升目标域检测性能",
      "contribution": "解决跨域数据差异问题"
    }
  },
  {
    "id": "75fc49042ffd",
    "venue": "ICML",
    "year": 2024,
    "title": "Detecting Any instruction-to-answer interaction relationship:Universal Instruction-to-Answer Navigator for Med-VQA",
    "url": "https://icml.cc/virtual/2024/poster/35018",
    "abstract": "Medical Visual Question Answering (Med-VQA) interprets complex medical imagery using user instructions for precise diagnostics, yet faces challenges due to diverse, inadequately annotated images. In this paper, we introduce the Universal Instruction-Vision Navigator (Uni-Med) framework for extracting instruction-to-answer relationships, facilitating the understanding of visual evidence behind responses. Specifically, we design the Instruct-to-Answer Clues Interpreter (IAI) to generate visual explanations based on the answers and mark the core part of instructions with \"real intent\" labels. The IAI-Med VQA dataset, produced using IAI, is now publicly available to advance Med-VQA research. Additionally, our Token-Level Cut-Mix module dynamically aligns visual explanations with image patches, ensuring answers are traceable and learnable. We also implement intention-guided attention to minimize non-core instruction interference, sharpening focus on 'real intent'. Extensive experiments on SLAKE datasets show Uni-Med’s superior accuracies (87.52% closed, 86.12% overall), outperforming MedVInT-PMC-VQA by 1.22% and 0.92%. Code and dataset are available at: https://github.com/zhongzee/Uni-Med-master.",
    "summary_cn": "提出Uni-Med框架，通过指令-答案线索解释器和Token-Level Cut-Mix模块，提升医学视觉问答的准确性与可解释性，在SLAKE数据集上达到87.52%的封闭准确率。",
    "keywords": [
      "医学视觉问答",
      "指令-答案关系",
      "视觉解释",
      "可解释性",
      "数据集",
      "注意力机制"
    ],
    "triple": {
      "method": "指令-答案线索解释器与Token-Level Cut-Mix模块",
      "result": "SLAKE数据集上封闭准确率87.52%，超越基准模型",
      "contribution": "提升Med-VQA准确性与可解释性，公开数据集IAI-Med VQA"
    }
  },
  {
    "id": "fd6793b98487",
    "venue": "ICML",
    "year": 2024,
    "title": "Generalizing Orthogonalization for Models with Non-Linearities",
    "url": "https://icml.cc/virtual/2024/poster/33057",
    "abstract": "The complexity of black-box algorithms can lead to various challenges, including the introduction of biases. These biases present immediate risks in the algorithms’ application. It was, for instance, shown that neural networks can deduce racial information solely from a patient's X-ray scan, a task beyond the capability of medical experts. If this fact is not known to the medical expert, automatic decision-making based on this algorithm could lead to prescribing a treatment (purely) based on racial information. While current methodologies allow for the \"orthogonalization\" or \"normalization\" of neural networks with respect to such information, existing approaches are grounded in linear models. Our paper advances the discourse by introducing corrections for non-linearities such as ReLU activations. Our approach also encompasses scalar and tensor-valued predictions, facilitating its integration into neural network architectures. Through extensive experiments, we validate our method's effectiveness in safeguarding sensitive data in generalized linear models, normalizing convolutional neural networks for metadata, and rectifying pre-existing embeddings for undesired attributes.",
    "summary_cn": "本文提出一种非线性正交化方法，用于消除神经网络中的敏感信息偏差，如从X射线推断种族，通过实验验证其在保护隐私和纠正嵌入方面的有效性。",
    "keywords": [
      "正交化",
      "非线性模型",
      "神经网络",
      "偏差消除",
      "敏感数据保护",
      "元数据归一化"
    ],
    "triple": {
      "method": "引入非线性正交化校正",
      "result": "有效保护敏感数据并纠正嵌入属性",
      "contribution": "扩展正交化方法至非线性模型"
    }
  },
  {
    "id": "42bd45f85ebd",
    "venue": "ICML",
    "year": 2025,
    "title": "TANGO: Clustering with Typicality-Aware Nonlocal Mode-Seeking and Graph-Cut Optimization",
    "url": "https://icml.cc/virtual/2025/poster/43507",
    "abstract": "Density-based mode-seeking methods generate a density-ascending dependency from low-density points towards higher-density neighbors.Current mode-seeking methods identify modes by breaking some dependency connections, but relying heavily on local data characteristics, requiring case-by-case threshold settings or human intervention to be effective for different datasets. To address this issue, we introduce a novel concept called typicality, by exploring the locally defined dependency from a global perspective, to quantify how confident a point would be a mode. We devise an algorithm that effectively and efficiently identifies modes with the help of the global-view typicality. To implement and validate our idea, we design a clustering method called TANGO, which not only leverages typicality to detect modes, but also utilizes graph-cut with an improved path-based similarity to aggregate data into the final clusters. Moreover, this paper also provides some theoretical analysis on the proposed algorithm. Experimental results on several synthetic and extensive real-world datasets demonstrate the effectiveness and superiority of TANGO. The code is available at https://github.com/SWJTU-ML/TANGO_code.",
    "summary_cn": "TANGO提出典型性概念，从全局视角改进密度模式搜索，结合图割优化实现高效聚类，在合成和真实数据集上表现优异。",
    "keywords": [
      "典型性",
      "模式搜索",
      "图割",
      "聚类",
      "全局视角",
      "密度估计"
    ],
    "triple": {
      "method": "典型性感知非局部模式搜索与图割优化",
      "result": "在合成和真实数据集上表现有效且优越",
      "contribution": "提出典型性概念，改进模式检测与聚类方法"
    }
  },
  {
    "id": "8d54213eac6c",
    "venue": "ICML",
    "year": 2025,
    "title": "AutoAL: Automated Active Learning with Differentiable Query Strategy Search",
    "url": "https://icml.cc/virtual/2025/poster/45158",
    "abstract": "As deep learning continues to evolve, the need for data efficiency becomes increasingly important. Considering labeling large datasets is both time-consuming and expensive, active learning (AL) provides a promising solution to this challenge by iteratively selecting the most informative subsets of examples to train deep neural networks, thereby reducing the labeling cost. However, the effectiveness of different AL algorithms can vary significantly across data scenarios, and determining which AL algorithm best fits a given task remains a challenging problem. This work presents the first differentiable AL strategy search method, named AutoAL, which is designed on top of existing AL sampling strategies. AutoAL consists of two neural nets, named SearchNet and FitNet, which are optimized concurrently under a differentiable bi-level optimization framework. For any given task, SearchNet and FitNet are iteratively co-optimized using the labeled data, learning how well a set of candidate AL algorithms perform on that task. With the optimal AL strategies identified, SearchNet selects a small subset from the unlabeled pool for querying their annotations, enabling efficient training of the task model. Experimental results demonstrate that AutoAL consistently achieves superior accuracy compared to all candidate AL algorithms and other selective AL approaches, showcasing its potential for adapting and integrating multiple existing AL methods across diverse tasks and domains.",
    "summary_cn": "AutoAL提出首个可微分的主动学习策略搜索方法，通过SearchNet和FitNet协同优化，自动选择最优策略，在多种任务中实现更高准确率，降低标注成本。",
    "keywords": [
      "主动学习",
      "可微分搜索",
      "策略优化",
      "数据效率",
      "深度学习",
      "标注成本"
    ],
    "triple": {
      "method": "可微分双层优化框架",
      "result": "优于所有候选AL算法",
      "contribution": "自适应集成多种AL方法"
    }
  },
  {
    "id": "10393c5b9136",
    "venue": "ICML",
    "year": 2025,
    "title": "On the Vulnerability of Applying Retrieval-Augmented Generation within Knowledge-Intensive Application Domains",
    "url": "https://icml.cc/virtual/2025/poster/45226",
    "abstract": "Retrieval-Augmented Generation (RAG) has been empirically shown to enhance the performance of large language models (LLMs) in knowledge-intensive domains such as healthcare, finance, and legal contexts. Given a query, RAG retrieves relevant documents from a corpus and integrates them into the LLMs’ generation process. In this study, we investigate the adversarial robustness of RAG, focusing specifically on examining the retrieval system. First, across 225 different setup combinations of corpus, retriever, query, and targeted information, we show that retrieval systems are vulnerable to universal poisoning attacks in medical Q&A. In such attacks, adversaries generate poisoned documents containing a broad spectrum of targeted information, such as personally identifiable information. When these poisoned documents are inserted into a corpus, they can be accurately retrieved by any users, as long as attacker-specified queries are used. To understand this vulnerability, we discovered that the deviation from the query’s embedding to that of the poisoned document tends to follow a pattern in which the high similarity between the poisoned document and the query is retained, thereby enabling precise retrieval. Based on these findings, we develop a new detection-based defense to ensure the safe use of RAG. Through extensive experiments spanning various Q&A domains, we observed that our proposed method consistently achieves excellent detection rates in nearly all cases.",
    "summary_cn": "研究发现检索增强生成（RAG）在医疗问答等领域易受通用投毒攻击，攻击者通过插入含目标信息的文档影响检索结果。基于此，提出了一种检测防御方法，实验证明其有效性。",
    "keywords": [
      "检索增强生成",
      "投毒攻击",
      "医疗问答",
      "对抗鲁棒性",
      "检测防御",
      "知识密集型领域"
    ],
    "triple": {
      "method": "实验分析投毒攻击模式",
      "result": "RAG检索系统易受攻击，新防御方法检测率高",
      "contribution": "揭示RAG脆弱性并提出有效防御策略"
    }
  },
  {
    "id": "c6cd093fcdc4",
    "venue": "ICML",
    "year": 2024,
    "title": "BLO-SAM: Bi-level Optimization Based Finetuning of the Segment Anything Model for Overfitting-Preventing Semantic Segmentation",
    "url": "https://icml.cc/virtual/2024/poster/32990",
    "abstract": "The Segment Anything Model (SAM), a foundation model pretrained on millions of images and segmentation masks, has significantly advanced semantic segmentation, a fundamental task in computer vision. Despite its strengths, SAM encounters two major challenges. Firstly, it struggles with segmenting specific objects autonomously, as it relies on users to manually input prompts like points or bounding boxes to identify targeted objects. Secondly, SAM faces challenges in excelling at specific downstream tasks, like medical imaging, due to a disparity between the distribution of its pretraining data, which predominantly consists of general-domain images, and the data used in downstream tasks. Current solutions to these problems, which involve finetuning SAM, often lead to overfitting, a notable issue in scenarios with very limited data, like in medical imaging. To overcome these limitations, we introduce BLO-SAM, which finetunes SAM based on bi-level optimization (BLO). Our approach allows for automatic image segmentation without the need for manual prompts, by optimizing a learnable prompt embedding. Furthermore, it significantly reduces the risk of overfitting by training the model's weight parameters and the prompt embedding on two separate subsets of the training dataset, each at a different level of optimization. We apply BLO-SAM to diverse semantic segmentation tasks in general and medical domains. The results demonstrate BLO-SAM's superior performance over various state-of-the-art image semantic segmentation methods. The code of BLO-SAM is available at https://github.com/importZL/BLO-SAM.",
    "summary_cn": "BLO-SAM通过双层优化微调SAM模型，实现无手动提示的自动图像分割，有效防止过拟合，在通用和医学图像分割任务中表现优异。",
    "keywords": [
      "双层优化",
      "SAM模型",
      "语义分割",
      "过拟合预防",
      "医学图像",
      "自动分割"
    ],
    "triple": {
      "method": "双层优化微调SAM",
      "result": "性能优于现有方法",
      "contribution": "实现无提示自动分割并防过拟合"
    }
  },
  {
    "id": "c1954741a0e8",
    "venue": "ICML",
    "year": 2025,
    "title": "\"Why Is There a Tumor?\": Tell Me the Reason, Show Me the Evidence",
    "url": "https://icml.cc/virtual/2025/poster/43910",
    "abstract": "Medical AI models excel at tumor detection and segmentation. However, their latent representations often lack explicit ties to clinical semantics, producing outputs less trusted in clinical practice. Most of the existing models generate either segmentation masks/labels (localizing where without why) or textual justifications (explaining why without where), failing to ground clinical concepts in spatially localized evidence. To bridge this gap, we propose to develop models that can justify the segmentation or detection using clinically relevant terms and point to visual evidence. We address two core challenges: First, we curate a rationale dataset to tackle the lack of paired images, annotations, and textual rationales for training. The dataset includes 180K image-mask-rationale triples with quality evaluated by expert radiologists. Second, we design rationale-informed optimization that disentangles and localizes fine-grained clinical concepts in a self-supervised manner without requiring pixel-level concept annotations. Experiments across medical benchmarks show our model demonstrates superior performance in segmentation, detection, and beyond. The anonymous link to our code.",
    "summary_cn": "提出一种医学AI模型，结合分割与文本解释，用临床术语定位肿瘤并提供视觉证据，提升临床可信度。",
    "keywords": [
      "医学AI",
      "肿瘤分割",
      "临床解释",
      "视觉证据",
      "自监督学习",
      "数据集构建"
    ],
    "triple": {
      "method": "自监督优化与数据集构建",
      "result": "在分割与检测任务中表现优异",
      "contribution": "实现临床概念的空间定位与解释"
    }
  },
  {
    "id": "75fc49042ffd",
    "venue": "ICML",
    "year": 2024,
    "title": "Detecting Any instruction-to-answer interaction relationship:Universal Instruction-to-Answer Navigator for Med-VQA",
    "url": "https://icml.cc/virtual/2024/poster/35018",
    "abstract": "Medical Visual Question Answering (Med-VQA) interprets complex medical imagery using user instructions for precise diagnostics, yet faces challenges due to diverse, inadequately annotated images. In this paper, we introduce the Universal Instruction-Vision Navigator (Uni-Med) framework for extracting instruction-to-answer relationships, facilitating the understanding of visual evidence behind responses. Specifically, we design the Instruct-to-Answer Clues Interpreter (IAI) to generate visual explanations based on the answers and mark the core part of instructions with \"real intent\" labels. The IAI-Med VQA dataset, produced using IAI, is now publicly available to advance Med-VQA research. Additionally, our Token-Level Cut-Mix module dynamically aligns visual explanations with image patches, ensuring answers are traceable and learnable. We also implement intention-guided attention to minimize non-core instruction interference, sharpening focus on 'real intent'. Extensive experiments on SLAKE datasets show Uni-Med’s superior accuracies (87.52% closed, 86.12% overall), outperforming MedVInT-PMC-VQA by 1.22% and 0.92%. Code and dataset are available at: https://github.com/zhongzee/Uni-Med-master.",
    "summary_cn": "提出Uni-Med框架，通过指令-答案线索解释器和Token-Level Cut-Mix模块，提升医学视觉问答的准确性和可解释性，在SLAKE数据集上达到87.52%的封闭准确率。",
    "keywords": [
      "医学视觉问答",
      "指令-答案关系",
      "可解释性",
      "视觉解释",
      "数据集",
      "注意力机制"
    ],
    "triple": {
      "method": "指令-答案线索解释器与Token-Level Cut-Mix模块",
      "result": "SLAKE数据集上封闭准确率87.52%，超越基准模型",
      "contribution": "提升Med-VQA准确性与可解释性，公开数据集IAI-Med VQA"
    }
  },
  {
    "id": "9d92d171b121",
    "venue": "ICML",
    "year": 2024,
    "title": "Unlocking the Power of Spatial and Temporal Information in Medical Multimodal Pre-training",
    "url": "https://icml.cc/virtual/2024/poster/34857",
    "abstract": "Medical vision-language pre-training methods mainly leverage the correspondence between paired medical images and radiological reports. Although multi-view spatial images and temporal sequences of image-report pairs are available in off-the-shelf multi-modal medical datasets, most existing methods have not thoroughly tapped into such extensive supervision signals. In this paper, we introduce the Med-ST framework for fine-grained spatial and temporal modeling to exploit information from multiple spatial views of chest radiographs and temporal historical records. For spatial modeling, Med-ST employs the Mixture of View Expert (MoVE) architecture to integrate different visual features from both frontal and lateral views. To achieve a more comprehensive alignment, Med-ST not only establishes the global alignment between whole images and texts but also introduces modality-weighted local alignment between text tokens and spatial regions of images. For temporal modeling, we propose a novel cross-modal bidirectional cycle consistency objective by forward mapping classification (FMC) and reverse mapping regression (RMR). By perceiving temporal information from simple to complex, Med-ST can learn temporal semantics. Experimental results across four distinct tasks demonstrate the effectiveness of Med-ST, especially in temporal classification tasks. Our code and model are available at https://github.com/SVT-Yang/MedST.",
    "summary_cn": "提出Med-ST框架，通过多视图空间建模与跨模态时序一致性目标，利用医学多模态数据中的空间和时间信息，提升医学视觉-语言预训练性能。",
    "keywords": [
      "医学多模态预训练",
      "空间建模",
      "时序建模",
      "跨模态对齐",
      "胸部X光",
      "视觉-语言学习"
    ],
    "triple": {
      "method": "多视图专家混合架构与跨模态双向循环一致性目标",
      "result": "在四个任务中表现有效，尤其在时序分类任务中突出",
      "contribution": "开发了Med-ST框架，充分利用医学数据的空间和时间信息进行细粒度建模"
    }
  },
  {
    "id": "fea47b68af29",
    "venue": "ICML",
    "year": 2025,
    "title": "MMedPO: Aligning Medical Vision-Language Models with Clinical-Aware Multimodal Preference Optimization",
    "url": "https://icml.cc/virtual/2025/poster/44599",
    "abstract": "The advancement of Large Vision-Language Models (LVLMs) has propelled their application in the medical field. However, Medical LVLMs (Med-LVLMs) encounter factuality challenges due to modality misalignment, where the models prioritize textual knowledge over visual input, leading to hallucinations that contradict information in medical images. Previous attempts to enhance modality alignment in Med-LVLMs through preference optimization have inadequately addressed clinical relevance in preference data, making these samples easily distinguishable and reducing alignment effectiveness. In response, we propose MMedPO, a novel multimodal medical preference optimization approach that considers the clinical relevance of preference samples to enhance Med-LVLM alignment. MMedPO curates multimodal preference data by introducing two types of dispreference: (1) plausible hallucinations injected through target Med-LVLMs or GPT-4o to produce medically inaccurate responses, and (2) lesion region neglect achieved through local lesion-noising, disrupting visual understanding of critical areas. We then calculate clinical relevance for each sample based on scores from multiple Med-LLMs and visual tools, enabling effective alignment. Our experiments demonstrate that MMedPO significantly enhances factual accuracy in Med-LVLMs, achieving substantial improvements over existing preference optimization methods by 14.2% and 51.7% on the Med-VQA and report generation tasks, respectively. Our code are available in https://github.com/aiming-lab/MMedPO}{https://github.com/aiming-lab/MMedPO.",
    "summary_cn": "提出MMedPO方法，通过引入临床相关性的多模态偏好优化，提升医学视觉语言模型的事实准确性，减少幻觉。",
    "keywords": [
      "医学视觉语言模型",
      "多模态对齐",
      "偏好优化",
      "临床相关性",
      "事实准确性",
      "幻觉减少"
    ],
    "triple": {
      "method": "基于临床相关性的多模态偏好优化",
      "result": "在Med-VQA和报告生成任务上分别提升14.2%和51.7%",
      "contribution": "增强医学模型事实准确性，减少幻觉"
    }
  },
  {
    "id": "752aa3614e80",
    "venue": "ICML",
    "year": 2024,
    "title": "CARES: A Comprehensive Benchmark of Trustworthiness in Medical Vision Language Models",
    "url": "https://icml.cc/virtual/2024/39446",
    "abstract": "Artificial intelligence has significantly impacted medical applications, particularly with the advent of Medical Large Vision Language Models (Med-LVLMs), sparking optimism for the future of automated and personalized healthcare. However, the trustworthiness of Med-LVLMs remains unverified, posing significant risks for future model deployment. In this paper, we introduce CARES and aim to comprehensively evaluate the Trustworthiness of Med-LVLMs across the medical domain. We assess the trustworthiness of Med-LVLMs across five dimensions, including trustfulness, fairness, safety, privacy, and robustness. CARES comprises about 41K question-answer pairs in both closed and open-ended formats, covering 16 medical image modalities and 27 anatomical regions. Our analysis reveals that the models consistently exhibit concerns regarding trustworthiness, often displaying factual inaccuracies and failing to maintain fairness across different demographic groups. Furthermore, they are vulnerable to attacks and demonstrate a lack of privacy awareness.",
    "summary_cn": "CARES基准全面评估医疗视觉语言模型的可信度，涵盖五个维度，揭示模型存在事实错误、公平性不足、易受攻击及隐私意识薄弱等问题。",
    "keywords": [
      "医疗视觉语言模型",
      "可信度评估",
      "公平性",
      "安全性",
      "隐私保护",
      "基准测试"
    ],
    "triple": {
      "method": "构建多维度评估基准",
      "result": "模型存在可信度缺陷",
      "contribution": "提供全面可信度分析框架"
    }
  },
  {
    "id": "12a1c1f382a0",
    "venue": "ICML",
    "year": 2025,
    "title": "Multimodal Medical Code Tokenizer",
    "url": "https://icml.cc/virtual/2025/poster/45110",
    "abstract": "Foundation models trained on patient electronic health records (EHRs) require tokenizing medical data into sequences of discrete vocabulary items. Existing tokenizers treat medical codes from EHRs as isolated textual tokens. However, each medical code is defined by its textual description, its position in ontological hierarchies, and its relationships to other codes, such as disease co-occurrences and drug-treatment associations. Medical vocabularies contain more than 600,000 codes with critical information for clinical reasoning.  We introduce MedTok, a multimodal medical code tokenizer that uses the text descriptions and relational context of codes. MedTok processes text using a language model encoder and encodes the relational structure with a graph encoder. It then quantizes both modalities into a unified token space, preserving modality-specific and cross-modality information.  We integrate MedTok into five EHR models and evaluate it on operational and clinical tasks across in-patient and out-patient datasets, including outcome prediction, diagnosis classification, drug recommendation, and risk stratification. Swapping standard EHR tokenizers with MedTok improves AUPRC across all EHR models, by 4.10% on MIMIC-III, 4.78% on MIMIC-IV, and 11.32% on EHRShot, with the largest gains in drug recommendation. Beyond EHR modeling, we demonstrate using MedTok tokenizer with medical QA systems. Our results demonstrate the potential of MedTok as a unified tokenizer for medical codes, improving tokenization for medical foundation models.",
    "summary_cn": "提出MedTok多模态医学代码分词器，结合文本描述与关系结构，提升电子健康记录模型在临床任务中的性能。",
    "keywords": [
      "多模态分词器",
      "医学代码",
      "电子健康记录",
      "图编码",
      "临床任务",
      "性能提升"
    ],
    "triple": {
      "method": "结合语言模型与图编码量化多模态信息",
      "result": "在多个数据集上提升AUPRC，最高达11.32%",
      "contribution": "为医学基础模型提供统一分词器，改善临床推理"
    }
  },
  {
    "id": "9cb38039bb9b",
    "venue": "ICML",
    "year": 2025,
    "title": "MedXpertQA: Benchmarking Expert-Level Medical Reasoning and Understanding",
    "url": "https://icml.cc/virtual/2025/poster/45718",
    "abstract": "We introduce MedXpertQA, a highly challenging and comprehensive benchmark to evaluate expert-level medical knowledge and advanced reasoning. MedXpertQA includes 4,460 questions spanning 17 specialties and 11 body systems. It includes two subsets, Text for text evaluation and MM for multimodal evaluation. Notably, MM introduces expert-level exam questions with diverse images and rich clinical information, including patient records and examination results, setting it apart from traditional medical multimodal benchmarks with simple QA pairs generated from image captions. MedXpertQA applies rigorous filtering and augmentation to address the insufficient difficulty of existing benchmarks like MedQA, and incorporates specialty board questions to improve clinical relevance and comprehensiveness. We perform data synthesis to mitigate data leakage risk and conduct multiple rounds of expert reviews to ensure accuracy and reliability. We evaluate 18 leading models on MedXpertQA. Moreover, medicine is deeply connected to real-world decision-making, providing a rich and representative setting for assessing reasoning abilities beyond mathematics and code. To this end, we develop a reasoning-oriented subset to facilitate the assessment of o1-like models.",
    "summary_cn": "MedXpertQA是一个评估专家级医学知识与推理的基准，包含4460个问题，涵盖17个专科和11个身体系统，分为文本和多模态子集，通过严格过滤和专家审核提升难度与可靠性。",
    "keywords": [
      "医学基准",
      "专家级推理",
      "多模态评估",
      "数据合成",
      "专科问题",
      "模型评估"
    ],
    "triple": {
      "method": "数据合成与专家审核",
      "result": "构建高难度医学基准并评估18个领先模型",
      "contribution": "提升医学AI推理评估的临床相关性和全面性"
    }
  },
  {
    "id": "0c03922bf93f",
    "venue": "ICML",
    "year": 2025,
    "title": "Open-Det: An Efficient Learning Framework for Open-Ended Detection",
    "url": "https://icml.cc/virtual/2025/poster/45000",
    "abstract": "Open-Ended object Detection (OED) is a novel and challenging task that detects objects and generates their category names in a free-form manner, without requiring additional vocabularies during inference. However, the existing OED models, such as GenerateU, require large-scale datasets for training, suffer from slow convergence, and exhibit limited performance. To address these issues, we present a novel and efficient Open-Det framework, consisting of four collaborative parts. Specifically, Open-Det accelerates model training in both the bounding box and object name generation process by reconstructing the Object Detector and the Object Name Generator. To bridge the semantic gap between Vision and Language modalities, we propose a Vision-Language Aligner with V-to-L and L-to-V alignment mechanisms, incorporating with the Prompts Distiller to transfer knowledge from the VLM into VL-prompts, enabling accurate object name generation for the LLM. In addition, we design a Masked Alignment Loss to eliminate contradictory supervision and introduce a Joint Loss to enhance classification, resulting in more efficient training. Compared to GenerateU, Open-Det, using only 1.5% of the training data (0.077M vs. 5.077M), 20.8% of the training epochs (31 vs. 149), and fewer GPU resources (4 V100 vs. 16 A100), achieves even higher performance (+1.0% in APr). The source codes are available at: https://github.com/Med-Process/Open-Det.",
    "summary_cn": "提出Open-Det框架，通过重构检测器和名称生成器，结合视觉-语言对齐与提示蒸馏，显著提升开放目标检测的训练效率和性能。",
    "keywords": [
      "开放目标检测",
      "视觉-语言对齐",
      "提示蒸馏",
      "高效训练",
      "多模态学习",
      "目标名称生成"
    ],
    "triple": {
      "method": "重构检测器与生成器，结合视觉-语言对齐与提示蒸馏",
      "result": "使用更少数据和资源，性能超越GenerateU（APr提升1.0%）",
      "contribution": "提出高效开放检测框架，解决训练慢、数据需求大的问题"
    }
  },
  {
    "id": "4d8f95bee42d",
    "venue": "ICML",
    "year": 2025,
    "title": "HealthGPT: A Medical Large Vision-Language Model for Unifying Comprehension and Generation via Heterogeneous Knowledge Adaptation",
    "url": "https://icml.cc/virtual/2025/poster/45007",
    "abstract": "We present HealthGPT , a powerful Medical Large Vision-Language Model (Med-LVLM) that integrates medical visual comprehension and generation capabilities within a unified autoregressive paradigm. Our bootstrapping philosophy is to progressively adapt heterogeneous comprehension and generation knowledge to pre-trained Large Language Models (LLMs). This is achieved through a novel heterogeneous low-rank adaptation (H-LoRA) technique, which is complemented by a tailored hierarchical visual perception (HVP) approach and a three-stage learning strategy (TLS) . To effectively learn the HealthGPT, we devise a comprehensive medical domain-specific comprehension and generation dataset called VL-Health . Experimental results demonstrate exceptional performance and scalability of HealthGPT in medical visual unified tasks. Our project can be accessed at https://github.com/DCDmllm/HealthGPT.",
    "summary_cn": "HealthGPT是统一医学视觉理解与生成的医疗大视觉语言模型，通过异构知识适应和低秩适配技术实现，在医学视觉任务中表现卓越。",
    "keywords": [
      "医疗大视觉语言模型",
      "异构知识适应",
      "低秩适配",
      "视觉感知",
      "医学数据集",
      "统一任务"
    ],
    "triple": {
      "method": "异构低秩适配与三阶段学习",
      "result": "在医学视觉统一任务中表现优异",
      "contribution": "提出首个统一医学视觉理解与生成的模型"
    }
  },
  {
    "id": "92f583bb9bd2",
    "venue": "ICML",
    "year": 2025,
    "title": "MedRAX: Medical Reasoning Agent for Chest X-ray",
    "url": "https://icml.cc/virtual/2025/poster/45678",
    "abstract": "Chest X-rays (CXRs) play an integral role in driving critical decisions in disease management and patient care. While recent innovations have led to specialized models for various CXR interpretation tasks, these solutions often operate in isolation, limiting their practical utility in clinical practice. We present MedRAX, the first versatile AI agent that seamlessly integrates state-of-the-art  CXR analysis tools and multimodal large language models into a unified framework. MedRAX dynamically leverages these models to address complex medical queries without requiring additional training. To rigorously evaluate its capabilities, we introduce ChestAgentBench, a comprehensive benchmark containing 2,500 complex medical queries across 7 diverse categories. Our experiments demonstrate that MedRAX achieves state-of-the-art performance compared to both open-source and proprietary models, representing a significant step toward the practical deployment of automated CXR interpretation systems. Data and code have been publicly available at https://github.com/bowang-lab/MedRAX",
    "summary_cn": "MedRAX是首个整合先进CXR分析工具与多模态大语言模型的AI代理，无需额外训练即可处理复杂医疗查询，在综合基准测试中表现优异。",
    "keywords": [
      "MedRAX",
      "胸部X光",
      "AI代理",
      "多模态大语言模型",
      "医疗推理",
      "ChestAgentBench"
    ],
    "triple": {
      "method": "整合CXR分析工具与多模态大语言模型",
      "result": "在ChestAgentBench基准上达到最优性能",
      "contribution": "推动自动化CXR解读系统实际部署"
    }
  },
  {
    "id": "c13943ee5036",
    "venue": "ICML",
    "year": 2025,
    "title": "NeuronTune: Towards Self-Guided Spurious Bias Mitigation",
    "url": "https://icml.cc/virtual/2025/poster/43955",
    "abstract": "Deep neural networks often develop spurious bias, reliance on correlations between non-essential features and classes for predictions. For example, a model may identify objects based on frequently co-occurring backgrounds rather than intrinsic features, resulting in degraded performance on data lacking these correlations. Existing mitigation approaches typically depend on external annotations of spurious correlations, which may be difficult to obtain and are not relevant to the spurious bias in a model. In this paper, we take a step towards self-guided mitigation of spurious bias by proposing NeuronTune, a post hoc method that directly intervenes in a model's internal decision process. Our method probes in a model's latent embedding space to identify and regulate neurons that lead to spurious prediction behaviors. We theoretically justify our approach and show that it brings the model closer to an unbiased one. Unlike previous methods, NeuronTune operates without requiring spurious correlation annotations, making it a practical and effective tool for improving model robustness. Experiments across different architectures and data modalities demonstrate that our method significantly mitigates spurious bias in a self-guided way.",
    "summary_cn": "NeuronTune提出一种自引导后处理方法，通过干预模型内部决策过程，无需外部标注即可显著减轻虚假偏差，提升模型鲁棒性。",
    "keywords": [
      "虚假偏差",
      "自引导缓解",
      "后处理方法",
      "神经元调控",
      "模型鲁棒性",
      "内部决策干预"
    ],
    "triple": {
      "method": "在潜在嵌入空间识别并调控导致虚假预测的神经元",
      "result": "显著减轻虚假偏差，提升模型在不同架构和数据模态上的鲁棒性",
      "contribution": "提出无需外部标注的自引导后处理方法，直接干预模型内部决策过程"
    }
  },
  {
    "id": "a260af1692ed",
    "venue": "ICML",
    "year": 2025,
    "title": "CLIMB: Data Foundations for Large Scale Multimodal Clinical Foundation Models",
    "url": "https://icml.cc/virtual/2025/poster/45167",
    "abstract": "Recent advances in clinical AI have enabled remarkable progress across many clinical domains. However, existing benchmarks and models are primarily limited to a small set of modalities and tasks, which hinders the development of large-scale multimodal methods that can make holistic assessments of patient health and well-being. To bridge this gap, we introduce Clinical Large-scale Integrative Multimodal Benchmark (CLIMB), a comprehensive clinical benchmark unifying diverse clinical data across imaging, language, temporal, and graph modalities. CLIMB comprises 4.51 million patient samples totaling 19.01 terabytes distributed across 2D imaging, 3D video, time series, graphs, and multimodal data. Through extensive empirical evaluation, we demonstrate that multitask pretraining significantly improves performance on understudied domains, achieving up to 29% improvement in ultrasound and 23% in ECG analysis over single-task learning. Pretraining on CLIMB also effectively improves models' generalization capability to new tasks, and strong unimodal encoder performance translates well to multimodal performance when paired with task-appropriate fusion strategies. Our findings provide a foundation for new architecture designs and pretraining strategies to adavance clinical AI research. Code is released at https://github.com/DDVD233/climb.",
    "summary_cn": "CLIMB是一个大规模多模态临床基准数据集，整合了影像、语言、时序和图数据，包含451万患者样本。多任务预训练显著提升超声和心电图分析性能，并增强模型泛化能力。",
    "keywords": [
      "临床AI",
      "多模态基准",
      "预训练",
      "泛化能力",
      "数据整合",
      "医疗影像"
    ],
    "triple": {
      "method": "构建CLIMB多模态临床基准数据集并进行多任务预训练",
      "result": "超声和心电图分析性能提升达29%和23%，模型泛化能力增强",
      "contribution": "为大规模多模态临床AI研究提供数据基础和架构设计指导"
    }
  },
  {
    "id": "c1954741a0e8",
    "venue": "ICML",
    "year": 2025,
    "title": "\"Why Is There a Tumor?\": Tell Me the Reason, Show Me the Evidence",
    "url": "https://icml.cc/virtual/2025/poster/43910",
    "abstract": "Medical AI models excel at tumor detection and segmentation. However, their latent representations often lack explicit ties to clinical semantics, producing outputs less trusted in clinical practice. Most of the existing models generate either segmentation masks/labels (localizing where without why) or textual justifications (explaining why without where), failing to ground clinical concepts in spatially localized evidence. To bridge this gap, we propose to develop models that can justify the segmentation or detection using clinically relevant terms and point to visual evidence. We address two core challenges: First, we curate a rationale dataset to tackle the lack of paired images, annotations, and textual rationales for training. The dataset includes 180K image-mask-rationale triples with quality evaluated by expert radiologists. Second, we design rationale-informed optimization that disentangles and localizes fine-grained clinical concepts in a self-supervised manner without requiring pixel-level concept annotations. Experiments across medical benchmarks show our model demonstrates superior performance in segmentation, detection, and beyond. The anonymous link to our code.",
    "summary_cn": "提出一种医学AI模型，结合肿瘤分割与临床语义解释，通过自监督学习定位视觉证据，提升临床可信度。",
    "keywords": [
      "医学AI",
      "肿瘤分割",
      "临床解释",
      "自监督学习",
      "视觉证据",
      "数据集构建"
    ],
    "triple": {
      "method": "自监督优化与数据集构建",
      "result": "分割与检测性能提升",
      "contribution": "实现临床语义与视觉证据的联合定位"
    }
  },
  {
    "id": "5859c409c7e7",
    "venue": "ICML",
    "year": 2025,
    "title": "Raptor: Scalable Train-Free Embeddings for 3D Medical Volumes Leveraging Pretrained 2D Foundation Models",
    "url": "https://icml.cc/virtual/2025/poster/46452",
    "abstract": "Current challenges in developing foundational models for volumetric imaging data, such as magnetic resonance imaging (MRI), stem from the computational complexity of state-of-the-art architectures in high dimensions and curating sufficiently large datasets of volumes.To address these challenges, we introduce Raptor (Random Planar Tensor Reduction), a train-free method for generating semantically rich embeddings for volumetric data. Raptor leverages a frozen 2D foundation model, pretrained on natural images, to extract visual tokens from individual cross-sections of medical volumes. These tokens are then spatially compressed using random projections, significantly reducing computational complexity while retaining rich semantic information. Extensive experiments on 10 diverse medical volume tasks verify the superior performance of Raptor over state-of-the-art methods, including those pretrained exclusively on medical volumes (+3 SuPreM, +6 MISFM, +10 Merlin, +13 VoCo, and +14 SLIViT), while entirely bypassing the need for costly training. Our results highlight Raptor's effectiveness and versatility as a foundation for advancing deep learning-based methods for medical volumes (code: github.com/sriramlab/raptor).",
    "summary_cn": "Raptor提出一种无需训练的3D医学影像嵌入方法，利用预训练2D模型提取切片特征并通过随机投影压缩，在10个任务中超越现有方法，降低计算成本。",
    "keywords": [
      "3D医学影像",
      "无需训练",
      "随机投影",
      "预训练模型",
      "语义嵌入",
      "计算效率"
    ],
    "triple": {
      "method": "利用预训练2D模型提取切片特征，随机投影压缩",
      "result": "在10个任务中性能超越现有方法",
      "contribution": "提供高效、无需训练的3D医学影像嵌入基础"
    }
  },
  {
    "id": "ddbf32ff1d8c",
    "venue": "ICML",
    "year": 2025,
    "title": "SITCOM: Step-wise Triple-Consistent Diffusion Sampling For Inverse Problems",
    "url": "https://icml.cc/virtual/2025/poster/46601",
    "abstract": "Diffusion models (DMs) are a class of generative models that allow sampling from a distribution learned over a training set. When applied to solving inverse problems, the reverse sampling steps are modified to approximately sample from a measurement-conditioned distribution. However, these modifications may be unsuitable for certain settings (e.g., presence of measurement noise) and non-linear tasks, as they often struggle to correct errors from earlier steps and generally require a large number of optimization and/or sampling steps. To address these challenges, we state three conditions for achieving measurement-consistent diffusion trajectories. Building on these conditions, we propose a new optimization-based sampling method that not only enforces standard data manifold measurement consistency and forward diffusion consistency, as seen in previous studies, but also incorporates our proposed step-wise and network-regularized backward diffusion consistency that maintains a diffusion trajectory by optimizing over the input of the pre-trained model at every sampling step. By enforcing these conditions (implicitly or explicitly), our sampler requires significantly fewer reverse steps. Therefore, we refer to our method as S tep-w i se T riple- Co nsistent Sa m pling ( SITCOM ). Compared to SOTA baselines, our experiments across several linear and non-linear tasks (with natural and medical images) demonstrate that SITCOM achieves competitive or superior results in terms of standard similarity metrics and run-time.",
    "summary_cn": "SITCOM提出一种三步一致性扩散采样方法，用于解决逆问题，通过优化预训练模型输入，减少反向步骤，在多种任务中实现高效且性能优越的结果。",
    "keywords": [
      "扩散模型",
      "逆问题",
      "一致性采样",
      "优化方法",
      "医学图像",
      "非线性任务"
    ],
    "triple": {
      "method": "三步一致性扩散采样",
      "result": "减少反向步骤，提升性能与运行效率",
      "contribution": "提出SITCOM方法，解决逆问题中的噪声和非线性挑战"
    }
  },
  {
    "id": "c877eb58760e",
    "venue": "ICML",
    "year": 2025,
    "title": "Trusted Multi-View Classification  with Expert Knowledge Constraints",
    "url": "https://icml.cc/virtual/2025/poster/45140",
    "abstract": "Multi-view classification (MVC) based on the Dempster-Shafer theory has gained significant recognition for its reliability in safety-critical applications. However, existing methods predominantly focus on providing confidence levels for decision outcomes without explaining the reasoning behind these decisions. Moreover, the reliance on first-order statistical magnitudes of belief masses often inadequately capture the intrinsic uncertainty within the evidence.  To address these limitations, we propose a novel framework termed Trusted Multi-view Classification Constrained with Expert Knowledge (TMCEK). TMCEK integrates expert knowledge to enhance feature-level interpretability and introduces a distribution-aware subjective opinion mechanism to derive more reliable and realistic confidence estimates. The theoretical superiority of the proposed uncertainty measure over conventional approaches is rigorously established. Extensive experiments conducted on three multi-view datasets for sleep stage classification demonstrate that TMCEK achieves state-of-the-art performance while offering interpretability at both the feature and decision levels. These results position TMCEK as a robust and interpretable solution for MVC in safety-critical domains. The code is available at https://github.com/jie019/TMCEK_ICML2025.",
    "summary_cn": "提出TMCEK框架，集成专家知识和分布感知主观意见机制，提升多视图分类的可解释性与置信度估计，在睡眠分期任务中实现最优性能。",
    "keywords": [
      "多视图分类",
      "专家知识",
      "可解释性",
      "置信度估计",
      "睡眠分期",
      "不确定性建模"
    ],
    "triple": {
      "method": "集成专家知识与分布感知主观意见机制",
      "result": "在睡眠分期任务中达到最优性能",
      "contribution": "提升多视图分类的可解释性与置信度估计"
    }
  },
  {
    "id": "b539374643ce",
    "venue": "ICML",
    "year": 2025,
    "title": "Quantifying Treatment Effects: Estimating Risk Ratios via Observational Studies",
    "url": "https://icml.cc/virtual/2025/poster/45747",
    "abstract": "The Risk Difference (RD), an absolute measure of effect, is widely used and well-studied in both randomized controlled trials (RCTs) and observational studies. Complementary to the RD, the Risk Ratio (RR), as a relative measure, is critical for a comprehensive understanding of intervention effects: RD can downplay small absolute changes, while RR can highlight them. Despite its significance, the theoretical study of RR has received less attention, particularly in observational settings. This paper addresses this gap by tackling the estimation of RR in observational data. We propose several RR estimators and establish their theoretical properties, including asymptotic normality and confidence intervals. Through analyses on simulated and real-world datasets, we evaluate the performance of these estimators in terms of bias, efficiency, and robustness to generative data models. We also examine the coverage and length of the associated confidence intervals. Due to the non-linear nature of RR, influence function theory yields two distinct efficient estimators with different convergence assumptions. Based on theoretical and empirical insights, we recommend, among all estimators, one of the two doubly-robust estimators, which, intriguingly, challenges conventional expectations.",
    "summary_cn": "本文针对观察性研究中风险比估计的理论不足，提出多种估计器并评估其性能，推荐一种双重稳健估计器以提升效果评估。",
    "keywords": [
      "风险比",
      "观察性研究",
      "估计器",
      "双重稳健",
      "理论性质",
      "置信区间"
    ],
    "triple": {
      "method": "提出多种风险比估计器",
      "result": "评估性能并推荐双重稳健估计器",
      "contribution": "填补观察性研究中风险比估计的理论空白"
    }
  },
  {
    "id": "70bd253dd9fc",
    "venue": "ICML",
    "year": 2025,
    "title": "Training Flexible Models of Genetic Variant Effects from Functional Annotations using Accelerated Linear Algebra",
    "url": "https://icml.cc/virtual/2025/poster/44064",
    "abstract": "To understand how genetic variants in human genomes manifest in phenotypes - traits like height or diseases like asthma - geneticists have sequenced and measured hundreds of thousands of individuals. Geneticists use this data to build models that predict how a genetic variant impacts phenotype given genomic features of the variant, like DNA accessibility or the presence of nearby DNA-bound proteins. As more data and features become available, one might expect predictive models to improve. Unfortunately, training these models is bottlenecked by the need to solve expensive linear algebra problems because variants in the genome are correlated with nearby variants, requiring inversion of large matrices. Previous methods have therefore been restricted to fitting small models, and fitting simplified summary statistics, rather than the full likelihood of the statistical model. In this paper, we leverage modern fast linear algebra techniques to develop DeepWAS (Deep genome Wide Association Studies), a method to train large and flexible neural network predictive models to optimize likelihood. Surprisingly, we find that larger models only improve performance when using our full likelihood approach; when trained by fitting traditional summary statistics, larger models perform no better than small ones. We find larger models trained on more features make better predictions, potentially improving disease predictions and therapeutic target identification.",
    "summary_cn": "本文提出DeepWAS方法，利用快速线性代数技术训练大型神经网络模型，优化全似然函数，以预测遗传变异对表型的影响，提升疾病预测和靶点识别能力。",
    "keywords": [
      "DeepWAS",
      "遗传变异",
      "神经网络",
      "全似然优化",
      "线性代数",
      "疾病预测"
    ],
    "triple": {
      "method": "利用快速线性代数训练大型神经网络",
      "result": "大模型在全似然方法下性能提升，预测更优",
      "contribution": "开发DeepWAS方法，改善疾病预测和靶点识别"
    }
  },
  {
    "id": "842fd606e78b",
    "venue": "ICML",
    "year": 2025,
    "title": "Distilling the Knowledge in Data Pruning",
    "url": "https://icml.cc/virtual/2025/poster/46023",
    "abstract": "With the increasing size of datasets used for training neural networks, data pruning has gained traction in recent years. However, most current data pruning algorithms are limited in their ability to preserve accuracy compared to models trained on the full data, especially in high pruning regimes. In this paper we explore the application of data pruning while incorporating knowledge distillation (KD) when training on a pruned subset. That is, rather than relying solely on ground-truth labels, we also use the soft predictions from a teacher network pre-trained on the complete data. By integrating KD into training, we demonstrate significant improvement across datasets, pruning methods, and on all pruning fractions. We first establish a theoretical motivation for employing self-distillation to improve training on pruned data. Then, we empirically make a compelling and highly practical observation: using KD, simple random pruning is comparable or superior to sophisticated pruning methods across all pruning regimes. On ImageNet for example, we achieve superior accuracy despite training on a random subset of only 50% of the data. Additionally, we demonstrate a crucial connection between the pruning factor and the optimal knowledge distillation weight. This helps mitigate the impact of samples with noisy labels and low-quality images retained by typical pruning algorithms. Finally, we make an intriguing observation: when using lower pruning fractions, larger teachers lead to accuracy degradation, while surprisingly, employing teachers with a smaller capacity than the student's may improve results. Our code will be made available.",
    "summary_cn": "本研究将知识蒸馏融入数据剪枝训练，显著提升模型精度，使简单随机剪枝媲美复杂方法，并探索了剪枝比例与蒸馏权重的关键联系。",
    "keywords": [
      "数据剪枝",
      "知识蒸馏",
      "随机剪枝",
      "模型训练",
      "ImageNet",
      "精度提升"
    ],
    "triple": {
      "method": "结合知识蒸馏训练剪枝数据",
      "result": "随机剪枝精度媲美复杂方法，ImageNet上50%数据达优",
      "contribution": "提出高效数据剪枝框架，揭示剪枝与蒸馏权重关系"
    }
  },
  {
    "id": "5d34a3ea9c44",
    "venue": "ICML",
    "year": 2025,
    "title": "Compositional Flows for 3D Molecule and Synthesis Pathway Co-design",
    "url": "https://icml.cc/virtual/2025/poster/46473",
    "abstract": "Many generative applications, such as synthesis-based 3D molecular design, involve constructing compositional objects with continuous features.Here, we introduce Compositional Generative Flows (CGFlow), a novel framework that extends flow matching to generate objects in compositional steps while modeling continuous states. Our key insight is that modeling compositional state transitions can be formulated as a straightforward extension of the flow matching interpolation process.We further build upon the theoretical foundations of generative flow networks (GFlowNets), enabling reward-guided sampling of compositional structures. We apply CGFlow to synthesizable drug design by jointly designing the molecule's synthetic pathway with its 3D binding pose.Our approach achieves state-of-the-art binding affinity and synthesizability on all 15 targets from the LIT-PCBA benchmark, and 4.2x improvement in sampling efficiency compared to 2D synthesis-based baseline.To our best knowledge, our method is also the first to achieve state of-art-performance in both Vina Dock (-9.42) and AiZynth success rate (36.1\\%) on the CrossDocked2020 benchmark.",
    "summary_cn": "提出CGFlow框架，结合流匹配与生成流网络，实现可合成3D分子与合成路径的协同设计，在多个基准测试中取得最优性能。",
    "keywords": [
      "CGFlow",
      "3D分子设计",
      "合成路径",
      "流匹配",
      "生成流网络",
      "药物设计"
    ],
    "triple": {
      "method": "流匹配扩展与生成流网络结合",
      "result": "在LIT-PCBA和CrossDocked2020基准上实现最优结合亲和力与合成成功率",
      "contribution": "首次实现可合成3D分子与路径协同设计的高效框架"
    }
  },
  {
    "id": "7ef4f178098b",
    "venue": "ICML",
    "year": 2025,
    "title": "Nearly Optimal Sample Complexity for Learning with Label Proportions",
    "url": "https://icml.cc/virtual/2025/poster/43817",
    "abstract": "We investigate Learning from Label Proportions (LLP), a partial information setting where examples in a training set are grouped into bags, and only aggregate label values in each bag are available. Despite the partial observability, the goal is still to achieve small regret at the level of individual examples. We give results on the sample complexity of LLP under square loss, showing that our sample complexity is essentially optimal. From an algorithmic viewpoint, we rely on carefully designed variants of Empirical Risk Minimization, and Stochastic Gradient Descent algorithms, combined with ad hoc variance reduction techniques. On one hand, our theoretical results improve in important ways on the existing literature on LLP, specifically in the way the sample complexity depends on the bag size. On the other hand, we validate our  algorithmic solutions on several datasets, demonstrating improved empirical performance (better accuracy for less samples)  against recent baselines.",
    "summary_cn": "研究标签比例学习(LLP)的样本复杂度，提出接近最优的样本复杂度，并设计基于经验风险最小化和随机梯度下降的算法，在多个数据集上验证了性能提升。",
    "keywords": [
      "标签比例学习",
      "样本复杂度",
      "经验风险最小化",
      "随机梯度下降",
      "方差减少",
      "最优性"
    ],
    "triple": {
      "method": "经验风险最小化与随机梯度下降结合方差减少",
      "result": "样本复杂度接近最优，实验性能优于基线",
      "contribution": "改进LLP样本复杂度理论并提升算法效果"
    }
  },
  {
    "id": "6ccda27d9929",
    "venue": "ICML",
    "year": 2025,
    "title": "\"Who experiences large model decay and why?\" A Hierarchical Framework for Diagnosing Heterogeneous Performance Drift",
    "url": "https://icml.cc/virtual/2025/poster/45315",
    "abstract": "Machine learning (ML) models frequently experience performance degradation when deployed in new contexts. Such degradation is rarely uniform: some subgroups may suffer large performance decay while others may not. Understanding where and how large differences in performance arise is critical for designing targeted corrective actions that mitigate decay for the most affected subgroups while minimizing any unintended effects. Current approaches do not provide such detailed insight, as they either (i) explain how average performance shifts arise or (ii) identify adversely affected subgroups without insight into how this occurred. To this end, we introduce a S ubgroup-scanning H ierarchical I nference F ramework for performance drif T (SHIFT). SHIFT first asks \"Is there any subgroup with unacceptably large performance decay due to covariate/outcome shifts?\" ( Where? ) and, if so, dives deeper to ask \"Can we explain this using more detailed variable(subset)-specific shifts?\" ( How? ). In real-world experiments, we find that SHIFT identifies interpretable subgroups affected by performance decay, and suggests targeted actions that effectively mitigate the decay.",
    "summary_cn": "提出SHIFT框架，通过分层推理诊断机器学习模型性能漂移，识别受协变量或结果偏移影响的子组，并提供针对性缓解措施。",
    "keywords": [
      "性能漂移",
      "子组分析",
      "SHIFT框架",
      "机器学习部署",
      "异质性诊断",
      "模型衰减"
    ],
    "triple": {
      "method": "分层推理与子组扫描",
      "result": "识别受性能衰减影响的子组并解释原因",
      "contribution": "提供针对性缓解措施框架"
    }
  },
  {
    "id": "de05e62fc5ba",
    "venue": "ICML",
    "year": 2025,
    "title": "Steer LLM Latents for Hallucination Detection",
    "url": "https://icml.cc/virtual/2025/poster/45122",
    "abstract": "Hallucinations in LLMs pose a significant concern to their safe deployment in real-world applications. Recent approaches have leveraged the latent space of LLMs for hallucination detection, but their embeddings, optimized for linguistic coherence rather than factual accuracy, often fail to clearly separate truthful and hallucinated content.To this end, we propose the T ruthfulness S eparator V ector ( TSV ), a lightweight and flexible steering vector that reshapes the LLM’s representation space during inference to enhance the separation between truthful and hallucinated outputs, without altering model parameters.Our two-stage framework first trains TSV on a small set of labeled exemplars to form compact and well-separated clusters.It then augments the exemplar set with unlabeled LLM generations, employing an optimal transport-based algorithm for pseudo-labeling combined with a confidence-based filtering process.Extensive experiments demonstrate that TSV achieves state-of-the-art performance with minimal labeled data, exhibiting strong generalization across datasets and providing a practical solution for real-world LLM applications.",
    "summary_cn": "提出TSV向量，通过重塑LLM表示空间增强真实与幻觉输出的分离，无需修改模型参数，实现高效幻觉检测。",
    "keywords": [
      "幻觉检测",
      "表示空间",
      "轻量级向量",
      "最优传输",
      "伪标签",
      "泛化能力"
    ],
    "triple": {
      "method": "训练TSV向量并优化伪标签",
      "result": "SOTA性能，最小标注数据，强泛化",
      "contribution": "提供实用LLM幻觉检测方案"
    }
  },
  {
    "id": "5bb4b9f41020",
    "venue": "ICML",
    "year": 2025,
    "title": "Speak Easy: Eliciting Harmful Jailbreaks from LLMs with Simple Interactions",
    "url": "https://icml.cc/virtual/2025/poster/44478",
    "abstract": "Large language models (LLMs) are designed to follow safety guidelines that prevent harmful use. However, researchers have found ways to bypass these safeguards and generate dangerous content, a tactic known as \"jailbreaking.\" While previous work has focused on technical methods for carrying out such attacks, we asked two new questions: First, are these harmful responses actually useful in helping someone carry out harmful actions? Second, can such responses be triggered through simple, everyday interactions?We found that the most harmful responses are both actionable (offering clear steps to follow) and informative (providing useful details). Surprisingly, these kinds of responses can be elicited using simple, non-technical methods. To better evaluate this risk, we develop HarmScore, a tool that measures how much a model response enables harmful actions. We also introduce Speak Easy, a simple jailbreak framework that uses natural, multi-step conversations across different languages to bypass safety measures. These findings highlight a critical vulnerability: even without advanced skills, users can exploit common interaction patterns to misuse LLMs. Recognizing this risk is an important step toward building safer and more responsible AI systems.",
    "summary_cn": "研究发现，通过简单多语言对话即可绕过LLMs安全防护，生成有害且可操作的内容。开发了HarmScore评估工具和Speak Easy越狱框架，揭示AI系统安全漏洞。",
    "keywords": [
      "大语言模型",
      "越狱攻击",
      "安全漏洞",
      "多语言对话",
      "风险评估",
      "有害内容"
    ],
    "triple": {
      "method": "自然多语言对话框架",
      "result": "简单交互即可触发有害响应",
      "contribution": "揭示LLMs安全风险并提出评估工具"
    }
  },
  {
    "id": "ff62587ad837",
    "venue": "ICML",
    "year": 2025,
    "title": "Learning Soft Sparse Shapes for Efficient Time-Series Classification",
    "url": "https://icml.cc/virtual/2025/poster/46130",
    "abstract": "Shapelets are discriminative subsequences (or shapes) with high interpretability in time series classification. Due to the time-intensive nature of shapelet discovery, existing shapelet-based methods mainly focus on selecting discriminative shapes while discarding others to achieve candidate subsequence sparsification. However, this approach may exclude beneficial shapes and overlook the varying contributions of shapelets to classification performance. To this end, we propose a Soft sparse Shapes (SoftShape) model for efficient time series classification. Our approach mainly introduces soft shape sparsification and soft shape learning blocks. The former transforms shapes into soft representations based on classification contribution scores, merging lower-scored ones into a single shape to retain and differentiate all subsequence information. The latter facilitates intra- and inter-shape temporal pattern learning, improving model efficiency by using sparsified soft shapes as inputs. Specifically, we employ a learnable router to activate a subset of class-specific expert networks for intra-shape pattern learning. Meanwhile, a shared expert network learns inter-shape patterns by converting sparsified shapes into sequences. Extensive experiments show that SoftShape outperforms state-of-the-art methods and produces interpretable results.",
    "summary_cn": "提出SoftShape模型，通过软形状稀疏化和学习模块，高效保留所有子序列信息并学习时序模式，提升时间序列分类性能与可解释性。",
    "keywords": [
      "时间序列分类",
      "形状稀疏化",
      "软表示",
      "模式学习",
      "可解释性",
      "高效分类"
    ],
    "triple": {
      "method": "软形状稀疏化与学习模块",
      "result": "优于现有方法，生成可解释结果",
      "contribution": "提升分类效率与性能"
    }
  },
  {
    "id": "c3df7335ad19",
    "venue": "ICML",
    "year": 2025,
    "title": "Position: Trustworthy AI Agents Require the Integration of Large Language Models and Formal Methods",
    "url": "https://icml.cc/virtual/2025/poster/40101",
    "abstract": "Large Language Models (LLMs) have emerged as a transformative AI paradigm, profoundly influencing broad aspects of daily life. Despite their remarkable performance, LLMs exhibit a fundamental limitation: hallucination—the tendency to produce misleading outputs that appear plausible. This inherent unreliability poses significant risks, particularly in high-stakes domains where trustworthiness is essential.On the other hand, Formal Methods (FMs), which share foundations with symbolic AI, provide mathematically rigorous techniques for modeling, specifying, reasoning, and verifying the correctness of systems. These methods have been widely employed in mission-critical domains such as aerospace, defense, and cybersecurity. However, the broader adoption of FMs remains constrained by significant challenges, including steep learning curves, limited scalability, and difficulties in adapting to the dynamic requirements of daily applications.To build trustworthy AI agents, we argue that the integration of LLMs and FMs is necessary to overcome the limitations of both paradigms. LLMs offer adaptability and human-like reasoning but lack formal guarantees of correctness and reliability. FMs provide rigor but need enhanced accessibility and automation to support broader adoption from LLMs.",
    "summary_cn": "本文主张结合大语言模型与形式化方法，以构建可信AI代理。前者灵活但易产生幻觉，后者严谨但应用受限，整合可互补优势。",
    "keywords": [
      "大语言模型",
      "形式化方法",
      "可信AI",
      "幻觉",
      "系统验证",
      "AI集成"
    ],
    "triple": {
      "method": "整合LLMs与FMs",
      "result": "互补灵活性与严谨性",
      "contribution": "提出可信AI代理构建路径"
    }
  },
  {
    "id": "419d40e39f22",
    "venue": "ICML",
    "year": 2025,
    "title": "DocVXQA: Context-Aware Visual Explanations for Document Question Answering",
    "url": "https://icml.cc/virtual/2025/poster/43613",
    "abstract": "We propose DocVXQA , a novel framework for visually self-explainable document question answering, where the goal is not only to produce accurate answers to questions but also to learn visual heatmaps that highlight critical regions, offering interpretable justifications for the model decision. To integrate explanations into the learning process, we quantitatively formulate explainability principles as explicit learning criteria.Unlike conventional relevance map methods that solely emphasize regions relevant to the answer, our context-aware DocVXQA delivers explanations that are contextually sufficient yet representation-efficient. This fosters user trust while achieving a balance between predictive performance and interpretability in document visual question answering applications. Extensive experiments, including human evaluation, provide strong evidence supporting the effectiveness of our method.",
    "summary_cn": "DocVXQA框架通过上下文感知视觉热图，在文档问答中实现自解释，平衡预测性能与可解释性。",
    "keywords": [
      "文档问答",
      "视觉解释",
      "上下文感知",
      "可解释性",
      "热图",
      "自解释模型"
    ],
    "triple": {
      "method": "上下文感知视觉热图学习",
      "result": "提升预测性能与解释质量",
      "contribution": "提出自解释文档问答框架"
    }
  },
  {
    "id": "780f47da2d10",
    "venue": "ICML",
    "year": 2025,
    "title": "Position: Medical Large Language Model Benchmarks Should Prioritize Construct Validity",
    "url": "https://icml.cc/virtual/2025/poster/40129",
    "abstract": "Medical large language models (LLMs) research often makes bold claims, from encoding clinical knowledge to reasoning like a physician. These claims are usually backed by evaluation on competitive benchmarks—a tradition inherited from mainstream machine learning. But how do we separate real progress from a leaderboard flex? Medical LLM benchmarks, much like those in other fields, are arbitrarily constructed using medical licensing exam questions. For these benchmarks to truly measure progress, they must accurately capture the real-world tasks they aim to represent. In this position paper, we argue that medical LLM benchmarks should—and indeed can—be empirically evaluated for their construct validity. In the psychological testing literature, “construct validity” refers to the ability of a test to measure an underlying “construct”, that is the actual conceptual target of evaluation. By drawing an analogy between LLM benchmarks and psychological tests, we explain how frameworks from this field can provide empirical foundations for validating benchmarks. To put these ideas into practice, we use real-world clinical data in proof-of-concept experiments to evaluate popular medical LLM benchmarks and report significant gaps in their construct validity. Finally, we outline a vision for a new ecosystem of medical LLM evaluation centered around the creation of valid benchmarks.",
    "summary_cn": "本文主张医学大语言模型基准应优先考虑结构效度，通过心理学测试框架实证评估，发现现有基准存在显著效度差距，并呼吁建立以有效基准为核心的新评估体系。",
    "keywords": [
      "医学大语言模型",
      "基准评估",
      "结构效度",
      "心理学测试",
      "临床数据",
      "实证验证"
    ],
    "triple": {
      "method": "类比心理学测试框架，使用真实临床数据进行概念验证实验",
      "result": "发现流行医学LLM基准存在显著结构效度差距",
      "contribution": "提出以结构效度为核心的医学LLM基准评估新生态愿景"
    }
  },
  {
    "id": "554c3f2e9148",
    "venue": "ICML",
    "year": 2025,
    "title": "Doubly Robust Conformalized Survival Analysis with Right-Censored Data",
    "url": "https://icml.cc/virtual/2025/poster/46585",
    "abstract": "We present a conformal inference method for constructing lower prediction bounds for survival times from right-censored data, extending recent approaches designed for more restrictive type-I censoring scenarios. The proposed method imputes unobserved censoring times using a machine learning model, and then analyzes the imputed data using a survival model calibrated via weighted conformal inference. This approach is theoretically supported by an asymptotic double robustness property. Empirical studies on simulated and real data demonstrate that our method leads to relatively informative predictive inferences and is especially robust in challenging settings where the survival model may be inaccurate.",
    "summary_cn": "提出一种双重鲁棒的保形推理方法，用于右删失生存数据的预测区间构建，通过机器学习模型插补未观测的删失时间，并利用加权保形推理校准生存模型，在生存模型不准确时仍保持稳健性。",
    "keywords": [
      "保形推理",
      "生存分析",
      "右删失数据",
      "双重鲁棒性",
      "机器学习",
      "预测区间"
    ],
    "triple": {
      "method": "机器学习插补与加权保形推理",
      "result": "构建信息丰富的预测区间，在模型不准确时保持稳健",
      "contribution": "扩展保形推理至右删失数据，提供理论双重鲁棒保证"
    }
  },
  {
    "id": "d24b1491eeb2",
    "venue": "ICML",
    "year": 2025,
    "title": "Parrot: Multilingual Visual Instruction Tuning",
    "url": "https://icml.cc/virtual/2025/poster/44886",
    "abstract": "The rapid development of Multimodal Large Language Models (MLLMs), such as GPT-4, marks a significant step toward artificial general intelligence. Existing methods typically align vision encoders with LLMs via supervised fine-tuning (SFT), but this often deteriorates their ability to handle multiple languages as training progresses. We empirically observe that imbalanced SFT datasets, largely English-centric, degrade performance on non-English languages due to the failure in multilingual token alignment. To address this, we propose Parrot, a novel approach that leverages textual guidance for visual token alignment at the language level. Parrot conditions visual tokens on diverse language inputs and uses Mixture-of-Experts (MoE) to align multilingual tokens. By computing cross-attention between initial visual features and textual embeddings, we select the most relevant experts, converting visual tokens into language-specific representations. Additionally, we introduce the Massive Multilingual Multimodal Benchmark (MMMB), a new benchmark comprising 6 languages, 15 categories, and 12,000 questions, to assess multilingual capabilities. Parrot achieves state-of-the-art performance on both the multilingual benchmarks and a wide range of multimodal tasks. Code and dataset are available at: \\url{https://github.com/AIDC-AI/Parrot}.",
    "summary_cn": "Parrot提出一种基于文本引导的多语言视觉指令调优方法，通过MoE对齐多语言标记，提升多语言多模态任务性能，并引入新基准MMMB进行评估。",
    "keywords": [
      "多语言视觉指令调优",
      "混合专家",
      "多模态大语言模型",
      "标记对齐",
      "多语言基准",
      "跨模态学习"
    ],
    "triple": {
      "method": "使用文本引导和MoE对齐多语言视觉标记",
      "result": "在多语言基准和多模态任务中达到最先进性能",
      "contribution": "提出Parrot方法和MMMB基准，解决多语言性能退化问题"
    }
  },
  {
    "id": "9aede3042355",
    "venue": "ICML",
    "year": 2025,
    "title": "scSSL-Bench: Benchmarking Self-Supervised Learning for Single-Cell Data",
    "url": "https://icml.cc/virtual/2025/poster/44286",
    "abstract": "Self-supervised learning (SSL) has proven to be a powerful approach for extracting biologically meaningful representations from single-cell data. To advance our understanding of SSL methods applied to single-cell data, we present scSSL-Bench, a comprehensive benchmark that evaluates nineteen SSL methods. Our evaluation spans nine datasets and focuses on three common downstream tasks: batch correction, cell type annotation, and missing modality prediction. Furthermore, we systematically assess various data augmentation strategies. Our analysis reveals task-specific trade-offs: the specialized single-cell frameworks, scVI, CLAIRE, and the finetuned scGPT excel at uni-modal batch correction, while generic SSL methods, such as VICReg and SimCLR, demonstrate superior performance in cell typing and multi-modal data integration. Random masking emerges as the most effective augmentation technique across all tasks, surpassing domain-specific augmentations. Notably, our results indicate the need for a specialized single-cell multi-modal data integration framework. scSSL-Bench provides a standardized evaluation platform and concrete recommendations for applying SSL to single-cell analysis, advancing the convergence of deep learning and single-cell genomics.",
    "summary_cn": "scSSL-Bench基准测试评估19种自监督学习方法在单细胞数据上的表现，涵盖批量校正、细胞类型注释和多模态预测任务，发现任务特异性权衡，并推荐随机掩码作为最佳数据增强策略。",
    "keywords": [
      "自监督学习",
      "单细胞数据",
      "基准测试",
      "数据增强",
      "多模态整合",
      "深度学习"
    ],
    "triple": {
      "method": "构建scSSL-Bench基准，评估19种SSL方法",
      "result": "揭示任务特异性权衡，随机掩码增强效果最佳",
      "contribution": "提供标准化平台，推动深度学习与单细胞基因组学融合"
    }
  },
  {
    "id": "fbca8a047d89",
    "venue": "ICML",
    "year": 2025,
    "title": "Test-Time Learning for Large Language Models",
    "url": "https://icml.cc/virtual/2025/poster/44367",
    "abstract": "While Large Language Models (LLMs) have exhibited remarkable emergent capabilities through extensive pre-training,  they still face critical limitations in generalizing to specialized domains and handling diverse linguistic variations, known as distribution shifts. In this paper, we propose a Test-Time Learning (TTL) paradigm for LLMs, namely TLM, which dynamically adapts LLMs to target domains using only unlabeled test data during testing. Specifically, we first provide empirical evidence and theoretical insights to reveal that more accurate predictions from LLMs can be achieved by minimizing the input perplexity of the unlabeled test data. Based on this insight, we formulate the Test-Time Learning process of LLMs as input perplexity minimization, enabling self-supervised enhancement of LLM performance. Furthermore, we observe that high-perplexity samples tend to be more informative for model optimization. Accordingly, we introduce a Sample Efficient Learning Strategy that actively selects and emphasizes these high-perplexity samples for test-time updates. Lastly, to mitigate catastrophic forgetting and ensure adaptation stability, we adopt Low-Rank Adaptation (LoRA) instead of full-parameter optimization, which allows lightweight model updates while preserving more original knowledge from the model. We introduce the AdaptEval benchmark for TTL and demonstrate through experiments that TLM improves performance by at least 20% compared to original LLMs on domain knowledge adaptation.",
    "summary_cn": "提出TLM方法，通过测试时学习动态适应大语言模型，利用未标注测试数据最小化困惑度，提升领域适应性能至少20%。",
    "keywords": [
      "测试时学习",
      "大语言模型",
      "领域适应",
      "困惑度最小化",
      "低秩适应",
      "样本选择"
    ],
    "triple": {
      "method": "测试时学习与低秩适应",
      "result": "性能提升至少20%",
      "contribution": "实现轻量级领域适应"
    }
  },
  {
    "id": "7a66ddebc5bc",
    "venue": "ICML",
    "year": 2025,
    "title": "Continuously Updating Digital Twins using Large Language Models",
    "url": "https://icml.cc/virtual/2025/poster/44291",
    "abstract": "Digital twins are models of real-world systems that can simulate their dynamics in response to potential actions. In complex settings, the state and action variables, and available data and knowledge relevant to a system can constantly change, requiring digital twins to continuously update with these changes to remain relevant. Current approaches struggle in this regard, as they require fixed, well-defined modelling environments, and they cannot adapt to novel variables without re-designs, or incorporate new information without re-training. To address this, we frame digital twinning as an in-context learning problem using large language models, enabling seamless updates to the twin at inference time. We develop CALM-DT, a Context-Adaptive Language Model-based Digital Twin that can accurately simulate across diverse state-action spaces using in-context learning alone by utilising fine-tuned encoders for sample retrieval. We empirically demonstrate CALM-DT's competitive performance with existing digital twin approaches, and its unique ability to adapt to changes in its modelling environment without parameter updates.",
    "summary_cn": "提出CALM-DT，一种基于大语言模型的数字孪生方法，通过上下文学习实现动态更新，无需参数调整即可适应环境变化。",
    "keywords": [
      "数字孪生",
      "大语言模型",
      "上下文学习",
      "自适应更新",
      "CALM-DT"
    ],
    "triple": {
      "method": "基于大语言模型的上下文学习与微调编码器检索",
      "result": "在多样化状态-动作空间中准确模拟，无需参数更新即可适应环境变化",
      "contribution": "实现数字孪生的动态自适应更新，克服传统方法固定建模环境的限制"
    }
  },
  {
    "id": "f14c213ae21e",
    "venue": "ICML",
    "year": 2024,
    "title": "Re-Dock: Towards Flexible and Realistic Molecular Docking with Diffusion Bridge",
    "url": "https://icml.cc/virtual/2024/poster/34081",
    "abstract": "Accurate prediction of protein-ligand binding structures, a task known as molecular docking is crucial for drug design but remains challenging. While deep learning has shown promise, existing methods often depend on holo-protein structures (docked, and not accessible in realistic tasks) or neglect pocket sidechain conformations, leading to limited practical utility and unrealistic conformation predictions. To fill these gaps, we introduce an under-explored task, named flexible docking to predict poses of ligand and pocket sidechains simultaneously and introduce Re-Dock, a novel diffusion bridge generative model extended to geometric manifolds. Specifically, we propose energy-to-geometry mapping inspired by the Newton-Euler equation to co-model the binding energy and conformations for reflecting the energy-constrained docking generative process. Comprehensive experiments on designed benchmark datasets including apo-dock and cross-dock demonstrate our model's superior effectiveness and efficiency over current methods.",
    "summary_cn": "提出Re-Dock扩散桥模型，通过能量-几何映射同时预测配体和口袋侧链构象，在柔性对接任务中实现更真实高效的结合结构预测。",
    "keywords": [
      "分子对接",
      "扩散模型",
      "柔性对接",
      "能量几何映射",
      "药物设计",
      "构象预测"
    ],
    "triple": {
      "method": "扩散桥生成模型与能量-几何映射",
      "result": "在apo-dock和cross-dock基准上超越现有方法",
      "contribution": "提出柔性对接任务并提升预测实用性与真实性"
    }
  },
  {
    "id": "f5228c211014",
    "venue": "NeurIPS",
    "year": 2024,
    "title": "Enhancing vision-language models for medical imaging: bridging the 3D gap with innovative slice selection",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2024/hash/b53513b83232116ae25f57a174a7c993-Abstract-Datasets_and_Benchmarks_Track.html",
    "abstract": "Recent approaches to vision-language tasks are built on the remarkable capabilities of large vision-language models (VLMs). These models excel in zero-shot and few-shot learning, enabling them to learn new tasks without parameter updates. However, their primary challenge lies in their design, which primarily accommodates 2D input, thus limiting their effectiveness for medical images, particularly radiological images like MRI and CT, which are typically 3D. To bridge the gap between state-of-the-art 2D VLMs and 3D medical image data, we developed an innovative, one-pass, unsupervised representative slice selection method called Vote-MI, which selects representative 2D slices from 3D medical imaging. To evaluate the effectiveness of vote-MI when implemented with VLMs, we introduce BrainMD, a robust, multimodal dataset comprising 2,453 annotated 3D MRI brain scans with corresponding textual radiology reports and electronic health records. Based on BrainMD, we further develop two benchmarks, BrainMD-select (including the most representative 2D slice of 3D image) and BrainBench (including various vision-language downstream tasks). Extensive experiments on the BrainMD dataset and its two corresponding benchmarks demonstrate that our representative selection method significantly improves performance in zero-shot and few-shot learning tasks. On average, Vote-MI achieves a 14.6\\% and 16.6\\% absolute gain for zero-shot and few-shot learning, respectively, compared to randomly selecting examples. Our studies represent a significant step toward integrating AI in medical imaging to enhance patient care and facilitate medical research. We hope this work will serve as a foundation for data selection as vision-language models are increasingly applied to new tasks.",
    "summary_cn": "提出Vote-MI方法，从3D医学影像中无监督选择代表性2D切片，结合BrainMD数据集，显著提升视觉语言模型在零样本和少样本学习中的性能。",
    "keywords": [
      "视觉语言模型",
      "3D医学影像",
      "切片选择",
      "零样本学习",
      "BrainMD数据集",
      "无监督学习"
    ],
    "triple": {
      "method": "Vote-MI无监督切片选择",
      "result": "零样本和少样本学习性能分别提升14.6%和16.6%",
      "contribution": "弥合2D视觉语言模型与3D医学影像间的差距"
    }
  },
  {
    "id": "1d490581452b",
    "venue": "NeurIPS",
    "year": 2024,
    "title": "MDAgents: An Adaptive Collaboration of LLMs for Medical Decision-Making",
    "url": "https://neurips.cc/virtual/2024/poster/96041",
    "abstract": "The NeurIPS Logo above may be used on presentations. Right-click and choose\n                            download. It is a vector graphic and may be used at any scale.",
    "summary_cn": "MDAgents提出一种基于LLM的自适应协作框架，用于提升医疗决策的准确性与效率。",
    "keywords": [
      "医疗决策",
      "大语言模型",
      "自适应协作",
      "框架设计",
      "决策支持"
    ],
    "triple": {
      "method": "LLM自适应协作框架",
      "result": "提升医疗决策准确性",
      "contribution": "提出高效医疗决策支持系统"
    }
  },
  {
    "id": "27593adfe4e0",
    "venue": "NeurIPS",
    "year": 2024,
    "title": "SegVol: Universal and Interactive Volumetric Medical Image Segmentation",
    "url": "https://neurips.cc/virtual/2024/poster/96893",
    "abstract": "Precise image segmentation provides clinical study with instructive information. Despite the remarkable progress achieved in medical image segmentation, there is still an absence of a 3D foundation segmentation model that can segment a wide range of anatomical categories with easy user interaction. In this paper, we propose a 3D foundation segmentation model, named SegVol, supporting universal and interactive volumetric medical image segmentation. By scaling up training data to 90K unlabeled Computed Tomography (CT) volumes and 6K labeled CT volumes, this foundation model supports the segmentation of over 200 anatomical categories using semantic and spatial prompts. To facilitate efficient and precise inference on volumetric images, we design a zoom-out-zoom-in mechanism. Extensive experiments on 22 anatomical segmentation tasks verify that SegVol outperforms the competitors in 19 tasks, with improvements up to 37.24\\% compared to the runner-up methods. We demonstrate the effectiveness and importance of specific designs by ablation study. We expect this foundation model can promote the development of volumetric medical image analysis. The model and code are publicly available at https://github.com/BAAI-DCAI/SegVol.",
    "summary_cn": "SegVol是一种通用交互式3D医学图像分割基础模型，通过大规模CT数据训练和缩放机制，支持200多个解剖类别分割，在22项任务中19项领先。",
    "keywords": [
      "3D医学图像分割",
      "基础模型",
      "交互式分割",
      "CT图像",
      "解剖类别",
      "缩放机制"
    ],
    "triple": {
      "method": "大规模CT数据训练与缩放机制",
      "result": "22项任务中19项领先，最高提升37.24%",
      "contribution": "提出通用交互式3D分割基础模型"
    }
  },
  {
    "id": "e0cef01c577e",
    "venue": "NeurIPS",
    "year": 2024,
    "title": "Knowledge-Empowered Dynamic Graph Network for Irregularly Sampled Medical Time Series",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2024/hash/7c04aea54c2a60a632a47bd451cd2849-Abstract-Conference.html",
    "abstract": "Irregularly Sampled Medical Time Series (ISMTS) are commonly found in the healthcare domain, where different variables exhibit unique temporal patterns while interrelated. However, many existing methods fail to efficiently consider the differences and correlations among medical variables together, leading to inadequate capture of fine-grained features at the variable level in ISMTS. We propose Knowledge-Empowered Dynamic Graph Network (KEDGN), a graph neural network empowered by variables' textual medical knowledge, aiming to model variable-specific temporal dependencies and inter-variable dependencies in ISMTS. Specifically, we leverage a pre-trained language model to extract semantic representations for each variable from their textual descriptions of medical properties, forming an overall semantic view among variables from a medical perspective. Based on this, we allocate variable-specific parameter spaces to capture variable-specific temporal patterns and generate a complete variable graph to measure medical correlations among variables. Additionally, we employ a density-aware mechanism to dynamically adjust the variable graph at different timestamps, adapting to the time-varying correlations among variables in ISMTS. The variable-specific parameter spaces and dynamic graphs are injected into the graph convolutional recurrent network to capture intra-variable and inter-variable dependencies in ISMTS together. Experiment results on four healthcare datasets demonstrate that KEDGN significantly outperforms existing methods.",
    "summary_cn": "提出KEDGN模型，利用医学知识构建动态图网络，有效捕捉不规则采样医疗时间序列中变量间及变量内的依赖关系，在四个数据集上表现优异。",
    "keywords": [
      "不规则采样医疗时间序列",
      "图神经网络",
      "医学知识",
      "动态图",
      "变量依赖",
      "语义表示"
    ],
    "triple": {
      "method": "基于预训练语言模型提取变量语义，构建动态图网络",
      "result": "在四个医疗数据集上显著优于现有方法",
      "contribution": "结合医学知识建模变量特定与变量间依赖，提升不规则序列分析性能"
    }
  },
  {
    "id": "c57f23f331d1",
    "venue": "NeurIPS",
    "year": 2025,
    "title": "Learning to Zoom with Anatomical Relations for Medical Structure Detection",
    "url": "https://neurips.cc/virtual/2025/poster/118432",
    "abstract": "Accurate anatomical structure detection is a critical preliminary step for diagnosing diseases characterized by structural abnormalities. In clinical practice, medical experts frequently adjust the zoom level of medical images to obtain comprehensive views for diagnosis. This common interaction results in significant variations in the apparent scale of anatomical structures across different images or fields of view. However, the information embedded in these zoom-induced scale changes is often overlooked by existing detection algorithms.  In addition, human organs possess a priori, fixed topological knowledge. To overcome this limitation, we propose ZR-DETR, a zoom-aware probabilistic framework tailored for medical object detection. ZR-DETR uniquely incorporates scale-sensitive zoom embeddings, anatomical relation constraints, and a Gaussian Process-based detection head. This architecture enables the framework to jointly model semantic context, enforce anatomical plausibility, and quantify detection uncertainty. Empirical validation across three diverse medical imaging benchmarks demonstrates that ZR-DETR consistently outperforms strong baselines in both single-domain and unsupervised domain adaptation scenarios.",
    "summary_cn": "提出ZR-DETR框架，结合缩放感知与解剖关系约束，提升医学结构检测精度，在多个基准测试中表现优异。",
    "keywords": [
      "医学目标检测",
      "缩放感知",
      "解剖关系",
      "ZR-DETR",
      "不确定性量化",
      "域适应"
    ],
    "triple": {
      "method": "引入缩放嵌入与解剖约束的ZR-DETR框架",
      "result": "在单域与无监督域适应场景中超越基线",
      "contribution": "提升医学结构检测的准确性与鲁棒性"
    }
  },
  {
    "id": "02bf839ea288",
    "venue": "NeurIPS",
    "year": 2024,
    "title": "MedSafetyBench: Evaluating and Improving the Medical Safety of Large Language Models",
    "url": "https://neurips.cc/virtual/2024/poster/97606",
    "abstract": "As large language models (LLMs) develop increasingly sophisticated capabilities and find applications in medical settings, it becomes important to assess their medical safety due to their far-reaching implications for personal and public health, patient safety, and human rights. However, there is little to no understanding of the notion of medical safety in the context of LLMs, let alone how to evaluate and improve it. To address this gap, we first define the notion of medical safety in LLMs based on the Principles of Medical Ethics set forth by the American Medical Association. We then leverage this understanding to introduce MedSafetyBench, the first benchmark dataset designed to measure the medical safety of LLMs. We demonstrate the utility of MedSafetyBench by using it to evaluate and improve the medical safety of LLMs. Our results show that publicly-available medical LLMs do not meet standards of medical safety and that fine-tuning them using MedSafetyBench improves their medical safety while preserving their medical performance. By introducing this new benchmark dataset, our work enables a systematic study of the state of medical safety in LLMs and motivates future work in this area, paving the way to mitigate the safety risks of LLMs in medicine. The benchmark dataset and code are available at https://github.com/AI4LIFE-GROUP/med-safety-bench.",
    "summary_cn": "本文提出首个评估大语言模型医疗安全性的基准数据集MedSafetyBench，基于医学伦理原则定义医疗安全，并通过微调提升模型安全性，同时保持医疗性能。",
    "keywords": [
      "大语言模型",
      "医疗安全",
      "基准数据集",
      "医学伦理",
      "微调",
      "评估"
    ],
    "triple": {
      "method": "基于医学伦理原则定义医疗安全，构建MedSafetyBench基准数据集",
      "result": "公开医疗大语言模型未达医疗安全标准，微调后安全性提升且医疗性能保持",
      "contribution": "引入首个医疗安全基准数据集，推动大语言模型在医疗领域的安全性研究"
    }
  },
  {
    "id": "79fe1c166acb",
    "venue": "NeurIPS",
    "year": 2025,
    "title": "From Indicators to Insights: Diversity-Optimized for Medical Series-Text Decoding via LLMs",
    "url": "https://neurips.cc/virtual/2025/poster/117266",
    "abstract": "Medical time-series analysis differs fundamentally from general ones by requiring specialized domain knowledge to interpret complex signals and clinical context. Large language models (LLMs) hold great promise for augmenting medical time-series analysis by complementing raw series with rich contextual knowledge drawn from biomedical literature and clinical guidelines. However, realizing this potential depends on precise and meaningful prompts that guide the LLM to key information. Yet, determining what constitutes effective prompt content remains non-trivial—especially in medical settings where signal interpretation often hinges on subtle, expert-defined decision-making indicators.  To this end, we propose InDiGO, a knowledge-aware evolutionary learning framework that integrates clinical signals and decision-making indicators through iterative optimization. Across four medical benchmarks, InDiGO consistently outperforms prior methods.  The code is available at: https://github.com/jinxyBJTU/InDiGO.",
    "summary_cn": "提出InDiGO框架，结合临床信号与决策指标，通过进化学习优化LLM提示，在四个医学基准测试中表现优于现有方法。",
    "keywords": [
      "医学时间序列",
      "大语言模型",
      "提示优化",
      "进化学习",
      "决策指标",
      "临床分析"
    ],
    "triple": {
      "method": "知识感知进化学习框架",
      "result": "在四个医学基准上超越先前方法",
      "contribution": "提升LLM在医学时间序列分析中的性能"
    }
  },
  {
    "id": "5d1b843c2b89",
    "venue": "NeurIPS",
    "year": 2025,
    "title": "Saliency Guided Longitudinal Medical Visual Question Answering",
    "url": "https://neurips.cc/virtual/2025/128766",
    "abstract": "Longitudinal medical visual question answering (Diff-VQA) requires comparing paired studies from different time points and answering questions about clinically meaningful changes. In this setting, the difference signal and the consistency of visual focus across time are more informative than absolute single-image findings. We propose a saliency-guided encoder–decoder for chest X-ray Diff-VQA that turns post-hoc saliency into actionable supervision. The model first performs a lightweight near-identity affine pre-alignment to reduce nuisance motion between visits. It then executes a within-epoch two-step loop: step 1 extracts a medically relevant keyword from the answer and generates keyword-conditioned Grad-CAM on both images to obtain disease-focused saliency; step 2 applies the shared saliency mask to both time points and generates the final answer. This closes the language–vision loop so that the terms that matter also guide where the model looks, enforcing spatially consistent attention on corresponding anatomy. On Medical-Diff-VQA, the approach attains competitive performance on BLEU, ROUGE-L, CIDEr, and METEOR while providing intrinsic interpretability. Notably, the backbone and decoder are general-domain pretrained without radiology-specific pretraining, highlighting practicality and transferability. These results support saliency-conditioned generation with mild pre-alignment as a principled framework for longitudinal reasoning in medical VQA.",
    "summary_cn": "提出显著性引导编码器-解码器模型，用于胸部X光纵向医学视觉问答。通过关键词生成Grad-CAM显著性图，引导模型关注疾病区域，实现时空一致注意力，提升性能与可解释性。",
    "keywords": [
      "纵向医学视觉问答",
      "显著性引导",
      "Grad-CAM",
      "胸部X光",
      "时空一致性",
      "可解释性"
    ],
    "triple": {
      "method": "显著性引导编码器-解码器，结合轻量预对齐与关键词条件Grad-CAM",
      "result": "在BLEU等指标上表现竞争性，提供内在可解释性",
      "contribution": "建立显著性条件生成框架，支持医学VQA中的纵向推理"
    }
  },
  {
    "id": "9f85d69ee73d",
    "venue": "NeurIPS",
    "year": 2024,
    "title": "CARES: A Comprehensive Benchmark of Trustworthiness in Medical Vision Language Models",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2024/hash/fde7f40f8ced5735006810534dc66b33-Abstract-Datasets_and_Benchmarks_Track.html",
    "abstract": "Artificial intelligence has significantly impacted medical applications, particularly with the advent of Medical Large Vision Language Models (Med-LVLMs), sparking optimism for the future of automated and personalized healthcare. However, the trustworthiness of Med-LVLMs remains unverified, posing significant risks for future model deployment. In this paper, we introduce CARES and aim to comprehensively evaluate the Trustworthiness of Med-LVLMs across the medical domain. We assess the trustworthiness of Med-LVLMs across five dimensions, including trustfulness, fairness, safety, privacy, and robustness. CARES comprises about 41K question-answer pairs in both closed and open-ended formats, covering 16 medical image modalities and 27 anatomical regions. Our analysis reveals that the models consistently exhibit concerns regarding trustworthiness, often displaying factual inaccuracies and failing to maintain fairness across different demographic groups. Furthermore, they are vulnerable to attacks and demonstrate a lack of privacy awareness. We publicly release our benchmark and code in https://github.com/richard-peng-xia/CARES.",
    "summary_cn": "CARES基准全面评估医疗视觉语言模型的可信度，涵盖五个维度，揭示模型存在事实错误、公平性不足、易受攻击及隐私意识缺乏等问题。",
    "keywords": [
      "医疗视觉语言模型",
      "可信度评估",
      "基准测试",
      "公平性",
      "安全性",
      "隐私保护"
    ],
    "triple": {
      "method": "构建多维度基准CARES",
      "result": "模型存在可信度问题",
      "contribution": "提供公开评估工具"
    }
  },
  {
    "id": "c947c0e11075",
    "venue": "NeurIPS",
    "year": 2025,
    "title": "MedSG-Bench: A Benchmark for Medical Image Sequences Grounding",
    "url": "https://neurips.cc/virtual/2025/poster/121815",
    "abstract": "Visual grounding is essential for precise perception and reasoning in multimodal large language models (MLLMs), especially in medical imaging domains. While existing medical visual grounding benchmarks primarily focus on single-image scenarios, real-world clinical applications often involve sequential images, where accurate lesion localization across different modalities and temporal tracking of disease progression (e.g., pre- vs. post-treatment comparison) require fine-grained cross-image semantic alignment and context-aware reasoning. To remedy the underrepresentation of image sequences in existing medical visual grounding benchmarks, we propose MedSG-Bench, the first benchmark tailored for Medical Image Sequences Grounding. It comprises eight VQA-style tasks, formulated into two paradigms of the grounding tasks, including 1) Image Difference Grounding, which focuses on detecting change regions across images, and 2) Image Consistency Grounding, which emphasizes detection of consistent or shared semantics across sequential images. MedSG-Bench covers 76 public datasets, 10 medical imaging modalities, and a wide spectrum of anatomical structures and diseases, totaling 9,630 question–answer pairs. We benchmark both general-purpose MLLMs (e.g., Qwen2.5-VL) and medical-domain specialized MLLMs (e.g., HuatuoGPT-vision), observing that even the advanced models exhibit substantial limitations in medical sequential grounding tasks. To advance this field, we construct MedSG-188K, a large-scale instruction-tuning dataset tailored for sequential visual grounding, and further develop MedSeq-Grounder, an MLLM designed to facilitate future research on fine-grained understanding across medical sequential images. We release all resources on https://github.com/Yuejingkun/MedSG-Bench",
    "summary_cn": "提出首个医学图像序列定位基准MedSG-Bench，涵盖8个任务与9,630个问答对，揭示现有多模态大模型在序列图像定位中的局限，并构建大规模指令调优数据集以促进研究。",
    "keywords": [
      "医学图像序列",
      "视觉定位",
      "多模态大语言模型",
      "基准测试",
      "疾病进展跟踪",
      "跨模态对齐"
    ],
    "triple": {
      "method": "构建包含图像差异与一致性定位任务的基准MedSG-Bench",
      "result": "现有先进模型在医学序列定位任务中表现显著不足",
      "contribution": "提供首个医学图像序列定位基准与大规模指令数据集，推动细粒度跨图像理解研究"
    }
  },
  {
    "id": "ae8e26853fd9",
    "venue": "NeurIPS",
    "year": 2024,
    "title": "Uni-Med: A Unified Medical Generalist Foundation Model For Multi-Task Learning Via Connector-MoE",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2024/hash/941938d6c3c57b4ef4a518965e238a6d-Abstract-Conference.html",
    "abstract": "Multi-modal large language models (MLLMs) have shown impressive capabilities as a general-purpose interface for various visual and linguistic tasks. However, building a unified MLLM for multi-task learning in the medical field remains a thorny challenge. To mitigate the tug-of-war problem of multi-modal multi-task optimization in MLLMs, recent advances primarily focus on improving the LLM components, while neglecting the connector that bridges the gap between modalities. In this paper, we introduce Uni-Med, a novel medical generalist foundation model which consists of a universal visual feature extraction module, a connector mixture-of-experts (CMoE) module, and an LLM. Benefiting from the proposed CMoE that leverages a well-designed router with a mixture of projection experts at the connector, Uni-Med achieves efficient solution to the tug-of-war problem and can perform six different medical tasks including question answering, visual question answering, report generation, referring expression comprehension, referring expression generation and image classification. To the best of our knowledge, Uni-Med is the first effort to tackle multi-task interference at the connector in MLLMs. Extensive ablation experiments validate the effectiveness of introducing CMoE under any configuration, with up to an average 8% performance gains. We further provide interpretation analysis of the tug-of-war problem from the perspective of gradient optimization and parameter statistics. Compared to previous state-of-the-art medical MLLMs, Uni-Med achieves competitive or superior evaluation metrics on diverse tasks. Code and resources are available at https://github.com/MSIIP/Uni-Med.",
    "summary_cn": "Uni-Med提出连接器混合专家模块，解决多模态多任务学习中的优化冲突，在六项医疗任务上表现优异。",
    "keywords": [
      "多模态大语言模型",
      "医疗任务",
      "混合专家",
      "连接器优化",
      "多任务学习",
      "视觉问答"
    ],
    "triple": {
      "method": "引入连接器混合专家模块",
      "result": "平均性能提升达8%，在六项医疗任务上表现竞争或更优",
      "contribution": "首次在连接器层面解决多任务干扰问题"
    }
  },
  {
    "id": "f8244bea6218",
    "venue": "NeurIPS",
    "year": 2024,
    "title": "MedCalc-Bench: Evaluating Large Language Models for Medical Calculations",
    "url": "https://neurips.cc/virtual/2024/poster/97666",
    "abstract": "Current benchmarks for evaluating large language models (LLMs) in medicine are primarily focused on question-answering involving domain knowledge and descriptive reasoning. While such qualitative capabilities are vital to medical diagnosis, in real-world scenarios, doctors frequently use clinical calculators that follow quantitative equations and rule-based reasoning paradigms for evidence-based decision support. To this end, we propose MedCalc-Bench, a first-of-its-kind dataset focused on evaluating the medical calculation capability of LLMs. MedCalc-Bench contains an evaluation set of over 1000 manually reviewed instances from 55 different medical calculation tasks. Each instance in MedCalc-Bench consists of a patient note, a question requesting to compute a specific medical value, a ground truth answer, and a step-by-step explanation showing how the answer is obtained. While our evaluation results show the potential of LLMs in this area, none of them are effective enough for clinical settings. Common issues include extracting the incorrect entities, not using the correct equation or rules for a calculation task, or incorrectly performing the arithmetic for the computation. We hope our study highlights the quantitative knowledge and reasoning gaps in LLMs within medical settings, encouraging future improvements of LLMs for various clinical calculation tasks. MedCalc-Bench is publicly available at: https://github.com/ncbi-nlp/MedCalc-Bench.",
    "summary_cn": "提出MedCalc-Bench数据集，评估大语言模型在医学计算任务中的表现，发现现有模型在临床计算中存在不足，需改进定量推理能力。",
    "keywords": [
      "医学计算",
      "大语言模型",
      "评估基准",
      "临床决策",
      "定量推理",
      "数据集"
    ],
    "triple": {
      "method": "构建包含1000多个实例的MedCalc-Bench数据集",
      "result": "现有模型在实体提取、方程应用和算术计算上存在错误",
      "contribution": "首次系统评估LLMs的医学计算能力，促进临床任务改进"
    }
  },
  {
    "id": "56184d54615c",
    "venue": "NeurIPS",
    "year": 2024,
    "title": "Med-Real2Sim: Non-Invasive Medical Digital Twins using Physics-Informed Self-Supervised Learning",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2024/hash/0b081a44ed0b8c0c4aa6bd886a60bea4-Abstract-Conference.html",
    "abstract": "A digital twin is a virtual replica of a real-world physical phenomena that uses mathematical modeling to characterize and simulate its defining features. By constructing digital twins for disease processes, we can perform in-silico simulations that mimic patients' health conditions and counterfactual outcomes under hypothetical interventions in a virtual setting. This eliminates the need for invasive procedures or uncertain treatment decisions. In this paper, we propose a method to identify digital twin model parameters using only noninvasive patient health data. We approach the digital twin modeling as a composite inverse problem, and observe that its structure resembles pretraining and finetuning in self-supervised learning (SSL). Leveraging this, we introduce a physics-informed SSL algorithm that initially pretrains a neural network on the pretext task of learning a differentiable simulator of a physiological process. Subsequently, the model is trained to reconstruct physiological measurements from noninvasive modalities while being constrained by the physical equations learned in pretraining. We apply our method to identify digital twins of cardiac hemodynamics using noninvasive echocardiogram videos, and demonstrate its utility in unsupervised disease detection and in-silico clinical trials.",
    "summary_cn": "提出Med-Real2Sim方法，利用物理信息自监督学习从无创医疗数据构建医学数字孪生，应用于心脏血流动力学模拟，支持无监督疾病检测和虚拟临床试验。",
    "keywords": [
      "医学数字孪生",
      "物理信息自监督学习",
      "无创数据",
      "心脏血流动力学",
      "虚拟临床试验",
      "疾病检测"
    ],
    "triple": {
      "method": "物理信息自监督学习",
      "result": "从超声视频构建心脏数字孪生",
      "contribution": "实现无创医学模拟与疾病检测"
    }
  },
  {
    "id": "47623c55c987",
    "venue": "NeurIPS",
    "year": 2024,
    "title": "FairMedFM: Fairness Benchmarking for Medical Imaging Foundation Models",
    "url": "https://neurips.cc/virtual/2024/poster/97809",
    "abstract": "The advent of foundation models (FMs) in healthcare offers unprecedented opportunities to enhance medical diagnostics through automated classification and segmentation tasks. However, these models also raise significant concerns about their fairness, especially when applied to diverse and underrepresented populations in healthcare applications. Currently, there is a lack of comprehensive benchmarks, standardized pipelines, and easily adaptable libraries to evaluate and understand the fairness performance of FMs in medical imaging, leading to considerable challenges in formulating and implementing solutions that ensure equitable outcomes across diverse patient populations. To fill this gap, we introduce FairMedFM, a fairness benchmark for FM research in medical imaging. FairMedFM integrates with 17 popular medical imaging datasets, encompassing different modalities, dimensionalities, and sensitive attributes. It explores 20 widely used FMs, with various usages such as zero-shot learning, linear probing, parameter-efficient fine-tuning, and prompting in various downstream tasks -- classification and segmentation. Our exhaustive analysis evaluates the fairness performance over different evaluation metrics from multiple perspectives, revealing the existence of bias, varied utility-fairness trade-offs on different FMs, consistent disparities on the same datasets regardless FMs, and limited effectiveness of existing unfairness mitigation methods. Furthermore, FairMedFM provides an open-sourced codebase at https://github.com/FairMedFM/FairMedFM, supporting extendible functionalities and applications and inclusive for studies on FMs in medical imaging over the long term.",
    "summary_cn": "FairMedFM是医学影像基础模型的公平性基准，整合17个数据集和20个模型，揭示模型偏见、效用-公平权衡及现有缓解方法效果有限。",
    "keywords": [
      "公平性基准",
      "医学影像",
      "基础模型",
      "偏见评估",
      "开源代码库",
      "多模态数据"
    ],
    "triple": {
      "method": "整合多数据集与模型进行公平性分析",
      "result": "发现偏见、权衡差异及缓解方法效果有限",
      "contribution": "提供首个医学影像基础模型公平性基准与开源工具"
    }
  },
  {
    "id": "8b03e5f89b44",
    "venue": "NeurIPS",
    "year": 2024,
    "title": "PediatricsGPT: Large Language Models as Chinese Medical Assistants for Pediatric Applications",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2024/hash/fa5b423e24b442180bcd4e13ae75a27f-Abstract-Conference.html",
    "abstract": "Developing intelligent pediatric consultation systems offers promising prospects for improving diagnostic efficiency, especially in China, where healthcare resources are scarce. Despite recent advances in Large Language Models (LLMs) for Chinese medicine, their performance is sub-optimal in pediatric applications due to inadequate instruction data and vulnerable training procedures.To address the above issues, this paper builds PedCorpus, a high-quality dataset of over 300,000 multi-task instructions from pediatric textbooks, guidelines, and knowledge graph resources to fulfil diverse diagnostic demands. Upon well-designed PedCorpus, we propose PediatricsGPT, the first Chinese pediatric LLM assistant built on a systematic and robust training pipeline.In the continuous pre-training phase, we introduce a hybrid instruction pre-training mechanism to mitigate the internal-injected knowledge inconsistency of LLMs for medical domain adaptation. Immediately, the full-parameter Supervised Fine-Tuning (SFT) is utilized to incorporate the general medical knowledge schema into the models. After that, we devise a direct following preference optimization to enhance the generation of pediatrician-like humanistic responses. In the parameter-efficient secondary SFT phase,a mixture of universal-specific experts strategy is presented to resolve the competency conflict between medical generalist and pediatric expertise mastery. Extensive results based on the metrics, GPT-4, and doctor evaluations on distinct downstream tasks show that PediatricsGPT consistently outperforms previous Chinese medical LLMs. The project and data will be released at https://github.com/ydk122024/PediatricsGPT.",
    "summary_cn": "本文构建了高质量儿科数据集PedCorpus，并提出首个中文儿科大语言模型助手PediatricsGPT，通过系统训练流程提升儿科诊断性能。",
    "keywords": [
      "儿科大语言模型",
      "PedCorpus数据集",
      "混合指令预训练",
      "监督微调",
      "偏好优化",
      "医学助手"
    ],
    "triple": {
      "method": "构建PedCorpus数据集，采用混合指令预训练、全参数SFT和偏好优化",
      "result": "在多项评估中优于现有中文医学大语言模型",
      "contribution": "开发首个中文儿科大语言模型助手，提升儿科诊断效率"
    }
  },
  {
    "id": "fd04f7c4ec17",
    "venue": "NeurIPS",
    "year": 2024,
    "title": "FairMedFM: Fairness Benchmarking for Medical Imaging Foundation Models",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2024/hash/c9826b9ea5e1b49b256329934a578d83-Abstract-Datasets_and_Benchmarks_Track.html",
    "abstract": "The advent of foundation models (FMs) in healthcare offers unprecedented opportunities to enhance medical diagnostics through automated classification and segmentation tasks. However, these models also raise significant concerns about their fairness, especially when applied to diverse and underrepresented populations in healthcare applications. Currently, there is a lack of comprehensive benchmarks, standardized pipelines, and easily adaptable libraries to evaluate and understand the fairness performance of FMs in medical imaging, leading to considerable challenges in formulating and implementing solutions that ensure equitable outcomes across diverse patient populations. To fill this gap, we introduce FairMedFM, a fairness benchmark for FM research in medical imaging. FairMedFM integrates with 17 popular medical imaging datasets, encompassing different modalities, dimensionalities, and sensitive attributes. It explores 20 widely used FMs, with various usages such as zero-shot learning, linear probing, parameter-efficient fine-tuning, and prompting in various downstream tasks -- classification and segmentation. Our exhaustive analysis evaluates the fairness performance over different evaluation metrics from multiple perspectives, revealing the existence of bias, varied utility-fairness trade-offs on different FMs, consistent disparities on the same datasets regardless FMs, and limited effectiveness of existing unfairness mitigation methods. Furthermore, FairMedFM provides an open-sourced codebase at https://github.com/FairMedFM/FairMedFM, supporting extendible functionalities and applications and inclusive for studies on FMs in medical imaging over the long term.",
    "summary_cn": "FairMedFM是医学影像基础模型的公平性基准，整合17个数据集和20个模型，揭示偏见存在、效用-公平权衡差异及现有缓解方法有限，提供开源代码库。",
    "keywords": [
      "公平性基准",
      "医学影像",
      "基础模型",
      "偏见评估",
      "开源代码库",
      "数据集整合"
    ],
    "triple": {
      "method": "整合多数据集与模型进行公平性分析",
      "result": "发现偏见、权衡差异及缓解方法有限",
      "contribution": "提供首个医学影像基础模型公平性基准与开源工具"
    }
  },
  {
    "id": "f4b67ef26e60",
    "venue": "NeurIPS",
    "year": 2024,
    "title": "Enhancing Medical VQA with Multimodal Determination Rationales",
    "url": "https://neurips.cc/virtual/2024/106856",
    "abstract": "Medical Visual Question Answering (MedVQA), which offers language responses to image-based medical inquiries, represents a challenging task and significant advancement in healthcare. It assists medical experts to swiftly interpret medical images, thereby enabling faster and more accurate diagnoses. However, the model interpretability and transparency of existing MedVQA solutions are often limited, posing challenges in understanding their decision-making processes. To address this issue, we devise a semi-automated annotation process to streamline data preparation and build new benchmark MedVQA datasets R-RAD, R-SLAKE and R-Path. These datasets provide intermediate medical decision-making rationales generated by multimodal large language models and human annotations for question-answering pairs in existing MedVQA datasets, i.e., VQA-RAD, SLAKE and PathVQA. Moreover, we design a novel framework, MedThink, which finetunes lightweight pretrained generative models by incorporating medical decision-making rationales. MedThink includes three distinct strategies to generate decision outcomes and corresponding rationales, thereby clearly showcasing the medical decision-making process during reasoning. Our comprehensive experiments show that our method achieves an accuracy of 83.5\\% on R-RAD, 86.3\\% on R-SLAKE and 87.2\\% on R-Path. These results significantly exceed those of existing state-of-the-art models with comparable parameters. Datasets and code will be released.",
    "summary_cn": "提出MedThink框架，通过半自动标注构建含决策依据的MedVQA数据集，结合多模态大模型生成医学决策依据，提升模型准确性与可解释性。",
    "keywords": [
      "医学视觉问答",
      "多模态大语言模型",
      "决策依据",
      "可解释性",
      "半自动标注",
      "MedThink"
    ],
    "triple": {
      "method": "半自动标注构建数据集，MedThink框架微调轻量生成模型",
      "result": "在R-RAD、R-SLAKE、R-Path数据集上准确率达83.5%、86.3%、87.2%",
      "contribution": "提升MedVQA准确率与决策过程可解释性"
    }
  },
  {
    "id": "be11bffd7681",
    "venue": "NeurIPS",
    "year": 2024,
    "title": "Towards Multi-dimensional Explanation Alignment for Medical Classification",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2024/hash/ea370419760b421ce12e3082eb2ae1a8-Abstract-Conference.html",
    "abstract": "The lack of interpretability in the field of medical image analysis has significant ethical and legal implications. Existing interpretable methods in this domain encounter several challenges, including dependency on specific models, difficulties in understanding and visualization, and issues related to efficiency. To address these limitations, we propose a novel framework called Med-MICN (Medical Multi-dimensional Interpretable Concept Network). Med-MICN provides interpretability alignment for various angles, including neural symbolic reasoning, concept semantics, and saliency maps, which are superior to current interpretable methods. Its advantages include high prediction accuracy, interpretability across multiple dimensions, and automation through an end-to-end concept labeling process that reduces the need for extensive human training effort when working with new datasets. To demonstrate the effectiveness and interpretability of Med-MICN, we apply it to four benchmark datasets and compare it with baselines. The results clearly demonstrate the superior performance and interpretability of our Med-MICN.",
    "summary_cn": "提出Med-MICN框架，通过多维度解释对齐提升医学图像分类的可解释性，在多个基准数据集上验证了其优越性能。",
    "keywords": [
      "医学图像分析",
      "可解释性",
      "多维度对齐",
      "Med-MICN",
      "概念网络",
      "神经符号推理"
    ],
    "triple": {
      "method": "Med-MICN框架",
      "result": "在四个基准数据集上表现优于基线",
      "contribution": "实现多维度解释对齐，提升可解释性与自动化"
    }
  },
  {
    "id": "e9c77f6239aa",
    "venue": "NeurIPS",
    "year": 2024,
    "title": "Medformer: A Multi-Granularity Patching Transformer for Medical Time-Series Classification",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2024/hash/3fe2a777282299ecb4f9e7ebb531f0ab-Abstract-Conference.html",
    "abstract": "Medical time series (MedTS) data, such as Electroencephalography (EEG) and Electrocardiography (ECG), play a crucial role in healthcare, such as diagnosing brain and heart diseases. Existing methods for MedTS classification primarily rely on handcrafted biomarkers extraction and CNN-based models, with limited exploration of transformer-based models. In this paper, we introduce Medformer, a multi-granularity patching transformer tailored specifically for MedTS classification. Our method incorporates three novel mechanisms to leverage the unique characteristics of MedTS: cross-channel patching to leverage inter-channel correlations, multi-granularity embedding for capturing features at different scales, and two-stage (intra- and inter-granularity) multi-granularity self-attention for learning features and correlations within and among granularities. We conduct extensive experiments on five public datasets under both subject-dependent and challenging subject-independent setups. Results demonstrate Medformer's superiority over 10 baselines, achieving top averaged ranking across five datasets on all six evaluation metrics. These findings underscore the significant impact of our method on healthcare applications, such as diagnosing Myocardial Infarction, Alzheimer's, and Parkinson's disease. We release the source code at https://github.com/DL4mHealth/Medformer.",
    "summary_cn": "Medformer是一种多粒度分块Transformer，专为医疗时间序列分类设计，通过跨通道分块和多粒度自注意力机制，在五个公开数据集上超越10个基线模型，提升疾病诊断性能。",
    "keywords": [
      "医疗时间序列",
      "Transformer",
      "多粒度分块",
      "跨通道相关",
      "疾病诊断",
      "自注意力"
    ],
    "triple": {
      "method": "多粒度分块Transformer",
      "result": "在五个数据集上超越10个基线模型",
      "contribution": "提升医疗时间序列分类性能"
    }
  },
  {
    "id": "b5415a6ff44f",
    "venue": "NeurIPS",
    "year": 2025,
    "title": "SMMILE: An expert-driven benchmark for multimodal medical in-context learning",
    "url": "https://neurips.cc/virtual/2025/poster/121577",
    "abstract": "Multimodal in-context learning (ICL) remains underexplored despite significant potential for domains such as medicine. Clinicians routinely encounter diverse, specialized tasks requiring adaptation from limited examples, such as drawing insights from a few relevant prior cases or considering a constrained set of differential diagnoses. While multimodal large language models (MLLMs) have shown advances in medical visual question answering (VQA), their ability to learn multimodal tasks from context is largely unknown. We introduce SMMILE, the first expert-driven multimodal ICL benchmark for medical tasks. Eleven medical experts curated problems, each including a multimodal query and multimodal in-context examples as task demonstrations. SMMILE encompasses 111 problems (517 question-image-answer triplets) covering 6 medical specialties and 13 imaging modalities. We further introduce SMMILE++, an augmented variant with 1038 permuted problems. A comprehensive evaluation of 15 MLLMs demonstrates that most models exhibit moderate to poor multimodal ICL ability in medical tasks. In open-ended evaluations, ICL contributes only an 8% average improvement over zero-shot on SMMILE and 9.4% on SMMILE++. We observe a susceptibility for irrelevant in-context examples: even a single noisy or irrelevant example can degrade performance by up to 9.5%. Moreover, we observe that MLLMs are affected by a recency bias, where placing the most relevant example last can lead to substantial performance improvements of up to 71%. Our findings highlight critical limitations and biases in current MLLMs when learning multimodal medical tasks from context. SMMILE is available at https://smmile-benchmark.github.io.",
    "summary_cn": "SMMILE是首个专家驱动的医学多模态上下文学习基准，涵盖111个问题。评估15个MLLM发现其医学多模态上下文学习能力有限，易受无关示例和近因偏差影响。",
    "keywords": [
      "多模态上下文学习",
      "医学基准",
      "专家驱动",
      "大语言模型",
      "视觉问答",
      "性能评估"
    ],
    "triple": {
      "method": "构建专家驱动的多模态医学基准SMMILE，评估15个MLLM",
      "result": "MLLM医学多模态上下文学习能力有限，易受无关示例和近因偏差影响",
      "contribution": "揭示当前MLLM在医学多模态上下文学习中的关键局限与偏差"
    }
  },
  {
    "id": "875892ba1c4b",
    "venue": "NeurIPS",
    "year": 2025,
    "title": "Offline Guarded Safe Reinforcement Learning for Medical Treatment Optimization Strategies",
    "url": "https://neurips.cc/virtual/2025/poster/119943",
    "abstract": "The NeurIPS Logo above may be used on presentations. Right-click and choose\n                            download. It is a vector graphic and may be used at any scale.",
    "summary_cn": "本研究提出一种离线安全强化学习方法，用于优化医疗治疗策略，确保在数据驱动决策中的安全性和有效性。",
    "keywords": [
      "离线强化学习",
      "医疗优化",
      "安全策略",
      "数据驱动",
      "治疗策略",
      "安全约束"
    ],
    "triple": {
      "method": "离线安全强化学习",
      "result": "优化医疗治疗策略",
      "contribution": "确保决策安全有效"
    }
  },
  {
    "id": "91aa0e8e7039",
    "venue": "NeurIPS",
    "year": 2024,
    "title": "GMAI-MMBench: A Comprehensive Multimodal Evaluation Benchmark Towards General Medical AI",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2024/hash/ab7e02fd60e47e2a379d567f6b54f04e-Abstract-Datasets_and_Benchmarks_Track.html",
    "abstract": "Large Vision-Language Models (LVLMs) are capable of handling diverse data types such as imaging, text, and physiological signals, and can be applied in various fields. In the medical field, LVLMs have a high potential to offer substantial assistance for diagnosis and treatment. Before that, it is crucial to develop benchmarks to evaluate LVLMs' effectiveness in various medical applications. Current benchmarks are often built upon specific academic literature, mainly focusing on a single domain, and lacking varying perceptual granularities. Thus, they face specific challenges, including limited clinical relevance, incomplete evaluations, and insufficient guidance for interactive LVLMs. To address these limitations, we developed the GMAI-MMBench, the most comprehensive general medical AI benchmark with well-categorized data structure and multi-perceptual granularity to date. It is constructed from 284 datasets across 38 medical image modalities, 18 clinical-related tasks, 18 departments, and 4 perceptual granularities in a Visual Question Answering (VQA) format. Additionally, we implemented a lexical tree structure that allows users to customize evaluation tasks, accommodating various assessment needs and substantially supporting medical AI research and applications. We evaluated 50 LVLMs, and the results show that even the advanced GPT-4o only achieves an accuracy of 53.96\\%, indicating significant room for improvement. Moreover, we identified five key insufficiencies in current cutting-edge LVLMs that need to be addressed to advance the development of better medical applications. We believe that GMAI-MMBench will stimulate the community to build the next generation of LVLMs toward GMAI.",
    "summary_cn": "本文提出GMAI-MMBench，一个全面的通用医疗AI多模态评估基准，涵盖多种医学图像模态、临床任务和感知粒度，以评估大型视觉语言模型在医疗领域的性能。",
    "keywords": [
      "医疗AI",
      "多模态评估",
      "大型视觉语言模型",
      "视觉问答",
      "基准测试",
      "医学图像"
    ],
    "triple": {
      "method": "构建多模态医疗基准GMAI-MMBench，涵盖284个数据集、38种图像模态、18项任务和4种感知粒度，采用VQA格式和词汇树结构",
      "result": "评估50个LVLMs，GPT-4o准确率仅53.96%，揭示模型在医疗领域存在显著不足",
      "contribution": "提供全面、可定制的医疗AI评估基准，促进下一代通用医疗AI发展"
    }
  },
  {
    "id": "3b0fca3d3eb6",
    "venue": "NeurIPS",
    "year": 2024,
    "title": "Curriculum Fine-tuning of Vision Foundation Model for Medical Image Classification Under Label Noise",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2024/hash/2093ed77c549eda95bd6f7212b735b43-Abstract-Conference.html",
    "abstract": "Deep neural networks have demonstrated remarkable performance in various vision tasks, but their success heavily depends on the quality of the training data. Noisy labels are a critical issue in medical datasets and can significantly degrade model performance. Previous clean sample selection methods have not utilized the well pre-trained features of vision foundation models (VFMs) and assumed that training begins from scratch. In this paper, we propose CUFIT, a curriculum fine-tuning paradigm of VFMs for medical image classification under label noise. Our method is motivated by the fact that linear probing of VFMs is relatively unaffected by noisy samples, as it does not update the feature extractor of the VFM, thus robustly classifying the training samples. Subsequently, curriculum fine-tuning of two adapters is conducted, starting with clean sample selection from the linear probing phase. Our experimental results demonstrate that CUFIT outperforms previous methods across various medical image benchmarks. Specifically, our method surpasses previous baselines by 5.0\\%, 2.1\\%, 4.6\\%, and 5.8\\% at a 40\\% noise rate on the HAM10000, APTOS-2019, BloodMnist, and OrgancMnist datasets, respectively. Furthermore, we provide extensive analyses to demonstrate the impact of our method on noisy label detection. For instance, our method shows higher label precision and recall compared to previous approaches. Our work highlights the potential of leveraging VFMs in medical image classification under challenging conditions of noisy labels.",
    "summary_cn": "提出CUFIT方法，通过线性探测和课程微调适配器，利用视觉基础模型处理医学图像分类中的标签噪声，在多个数据集上性能优于基线。",
    "keywords": [
      "医学图像分类",
      "标签噪声",
      "视觉基础模型",
      "课程微调",
      "线性探测",
      "CUFIT"
    ],
    "triple": {
      "method": "线性探测与课程微调适配器",
      "result": "在多个数据集上性能提升2.1%-5.8%",
      "contribution": "利用视觉基础模型提升噪声标签下的分类鲁棒性"
    }
  },
  {
    "id": "af3c223e1cb0",
    "venue": "NeurIPS",
    "year": 2024,
    "title": "Representation Learning of Structured Data for Medical Foundation Models",
    "url": "https://neurips.cc/virtual/2024/102673",
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance across various domains, including healthcare. However, their ability to effectively represent structured non-textual data, such as the alphanumeric medical codes used in records like ICD-10 or SNOMED-CT, is limited and has been particularly exposed in recent research. This paper examines the challenges LLMs face in processing medical codes due to the shortcomings of current tokenization methods. As a result, we introduce the UniStruct architecture to design a multimodal medical foundation model of unstructured text and structured data, which addresses these challenges by adapting subword tokenization techniques specifically for the structured medical codes. Our approach is validated through model pre-training on both an extensive internal medical database and a public repository of structured medical records. Trained on over 1 billion tokens on the internal medical database, the proposed model achieves up to a 23% improvement in evaluation metrics, with around 2% gain attributed to our proposed tokenization. Additionally, when evaluated on the EHRSHOT public benchmark with a 1/1000 fraction of the pre-training data, the UniStruct model improves performance on over 42% of the downstream tasks. Our approach not only enhances the representation and generalization capabilities of patient-centric models but also bridges a critical gap in representation learning models’ ability to handle complex structured medical data, alongside unstructured text.",
    "summary_cn": "本文提出UniStruct架构，通过改进子词分词技术处理结构化医疗代码，提升多模态医学基础模型性能，在评估指标上实现高达23%的改进。",
    "keywords": [
      "UniStruct架构",
      "结构化医疗数据",
      "子词分词",
      "多模态基础模型",
      "表示学习",
      "医疗代码"
    ],
    "triple": {
      "method": "改进子词分词技术处理医疗代码",
      "result": "评估指标提升23%，下游任务性能改善超42%",
      "contribution": "增强患者中心模型表示与泛化能力，弥合结构化数据处理差距"
    }
  },
  {
    "id": "9f877e81b204",
    "venue": "NeurIPS",
    "year": 2025,
    "title": "m1: Unleash the Potential of Test-Time Scaling for Medical Reasoning in Large Language Models",
    "url": "https://neurips.cc/virtual/2025/124931",
    "abstract": "Test-time scaling has emerged as a powerful technique for enhancing the reasoning capabilities of large language models (LLMs). However, its effectiveness in medical reasoning remains uncertain, as the medical domain fundamentally differs from mathematical tasks in terms of knowledge representation and decision-making processes. In this paper, we provide the first comprehensive investigation of test-time scaling for medical reasoning and present \\textbf{m1}, a simple yet effective approach that increases a model’s medical reasoning capability at inference. Through extensive experiments on open-source LLMs (Qwen2.5, 7B and 32B), we demonstrate that increasing the ``thinking'' token budget consistently improves accuracy without additional model training. Our evaluation across diverse medical tasks demonstrates that test-time scaling significantly enhances medical reasoning, enabling lightweight fine-tuned models to achieve performance comparable to computationally intensive counterparts (e.g., our 32B model matches previous 70B-scale medical LLMs). We identify an optimal reasoning token budget of approximately 4K, beyond which performance may degrade due to overthinking. Budget forcing, which controls test-time computation by extending reasoning through iterative prompts (e.g., appending \"Wait\"), helps models double-check answers but does not necessarily improve overall medical QA performance and, in some cases, introduces errors into previously correct responses. Critically, our analysis highlights insufficient medical knowledge as a primary failure mode, a limitation unresolvable through increased reasoning alone, underscoring the necessity of incorporating medical knowledge. Furthermore, increasing data scale, enhancing data quality, or expanding model capacity consistently improves medical knowledge grounding and thus boosts performance, particularly on challenging medical benchmarks where smaller models reach performance saturation. These findings reveal fundamental differences between medical and mathematical reasoning capabilities in LLMs. All data, code, and models will be publicly available to encourage future exploration in optimizing inference strategies in clinical AI applications.",
    "summary_cn": "研究提出m1方法，通过增加推理时token预算提升大语言模型的医学推理能力，无需额外训练，使轻量模型达到大型模型性能，并揭示医学与数学推理的根本差异。",
    "keywords": [
      "医学推理",
      "大语言模型",
      "测试时缩放",
      "推理优化",
      "知识表示",
      "临床AI"
    ],
    "triple": {
      "method": "增加推理时token预算",
      "result": "提升医学推理准确率，轻量模型匹配大型模型性能",
      "contribution": "揭示医学推理特性，优化临床AI推理策略"
    }
  },
  {
    "id": "864386b36fda",
    "venue": "NeurIPS",
    "year": 2025,
    "title": "MedicalNarratives: Connecting Medical Vision and Language with Localized Narratives",
    "url": "https://neurips.cc/virtual/2025/poster/121849",
    "abstract": "Multi-modal models are data hungry. While datasets with natural images are abundant, medical image datasets can not afford the same luxury. To enable representation learning for medical images at scale, we turn to YouTube, a platform with a large reservoir of open-source medical pedagogical videos. We curate MedicalNarratives, a dataset 4.7M medical image-text pairs, with 1M samples containing dense annotations in the form of traces spatial traces (and bounding boxes), and 118K videos centered on the trace event (with aligned text), enabling spatiotemporal grounding beyond single frames. Similar to think-aloud studies where instructors speak while hovering their mouse cursor movements over relevant image regions, 1M images in MedicalNarratives contains localized mouse traces in image pixels, creating a spatial association between the text and pixels. To evaluate the utility of MedicalNarratives, we train GenMedClip with a CLIP-like objective using our dataset spanning 12 medical domains. GenMedClip outperforms previous state-of-the-art models on all 12 domains on a newly constructed medical imaging benchmark. Data, demo, code, and models will be made available.",
    "summary_cn": "从YouTube医学教学视频中构建MedicalNarratives数据集，含470万图像-文本对，其中100万带空间轨迹标注。基于此训练的GenMedClip模型在12个医学领域超越现有最佳模型。",
    "keywords": [
      "医学多模态",
      "数据集构建",
      "空间轨迹标注",
      "CLIP模型",
      "医学图像理解",
      "视频分析"
    ],
    "triple": {
      "method": "利用YouTube医学视频构建带空间轨迹标注的数据集",
      "result": "GenMedClip在12个医学领域超越现有最佳模型",
      "contribution": "提供大规模医学多模态数据集并提升模型性能"
    }
  },
  {
    "id": "e0cb2e9456f8",
    "venue": "NeurIPS",
    "year": 2025,
    "title": "SCRIBE: A Fine-Tuned Transformer Embedding Model for Evaluating Medical School Personal Statements",
    "url": "https://neurips.cc/virtual/2025/133812",
    "abstract": "Personal statements are a crucial part of the medical school application process, offering applicants the opportunity to showcase their personality, experiences, and motivation for pursuing a career in medicine. However, many students struggle to draft and revise these essays while juggling pre-med commitments. To address this, we present the Semantic and Contextual Rubric-Based Intelligence for Biomedical Essays (SCRIBE), a novel offline tool for the automated evaluation of medical school personal statements, which fine-tunes the state-of-the-art e5-large-v2. SCRIBE provides automatic, structured feedback, lowering barriers for students who may lack access to mentoring or editing support. Our tool segments essays into semantically coherent sections, classifies each into rubric categories (Spark, Healthcare Experience, Showing Doctor Qualities, Spin), and assigns a score from 1 to 4. The SCRIBE tool was trained on manually annotated text by subject experts. The novel contributions of our research work include: (1) Development of an automated tool named SCRIBE for practical evaluation of medical personal statements by fine-tuning the state-of-the-art e5-large-v2 model, which is publicly available.",
    "summary_cn": "SCRIBE是基于e5-large-v2微调的Transformer模型，用于自动评估医学院个人陈述，提供结构化反馈和评分。",
    "keywords": [
      "SCRIBE",
      "医学院个人陈述",
      "自动评估",
      "Transformer微调",
      "结构化反馈",
      "e5-large-v2"
    ],
    "triple": {
      "method": "微调e5-large-v2模型",
      "result": "实现个人陈述自动分段、分类和评分",
      "contribution": "开发公开可用的自动化评估工具"
    }
  },
  {
    "id": "851feda26c89",
    "venue": "NeurIPS",
    "year": 2025,
    "title": "GauSAM: Contour‑Guided 2D Gaussian Fields for Multi‑Scale Medical Image Segmentation with Segment Anything",
    "url": "https://neurips.cc/virtual/2025/poster/119713",
    "abstract": "Effective multiscale medical image segmentation requires simultaneously preserving smooth spatial continuity and accurately delineating high-frequency boundaries, yet pixel-wise decoders often fail to maintain this balance consistently across varying resolutions. We introduce GauSAM, which seamlessly integrates contour‑guided 2D Gaussian probability fields into the Segment Anything Model to address these challenges. In our framework, segmentation masks are parameterized as continuous probability fields of learnable 2D Gaussian primitives, enforcing spatially smooth and structurally consistent. Contourlet transforms extract rich multidirectional frequency information, notably edges and fine textures, which dynamically guide the spatial distribution of Gaussian primitives to substantially improve boundary fidelity in complex structures. The incorporation of these high-frequency contour priors also enriches the expressive capacity of the SAM image encoder. Extensive experiments on diverse 2D medical segmentation tasks confirm that GauSAM consistently delivers robust generalization and state-of-the-art performance with only 1.2M trainable parameters. The official implementation of GauSAM is publicly available at https://github.com/Quinten-Wu504/GauSAM.",
    "summary_cn": "GauSAM将轮廓引导的2D高斯概率场集成到Segment Anything模型中，通过轮廓波变换提取高频边界信息指导高斯原语分布，提升了多尺度医学图像分割的边界精度和泛化性能。",
    "keywords": [
      "医学图像分割",
      "高斯概率场",
      "轮廓波变换",
      "多尺度分割",
      "边界精度",
      "Segment Anything"
    ],
    "triple": {
      "method": "集成轮廓引导2D高斯场与SAM",
      "result": "在多种2D医学分割任务中实现SOTA性能",
      "contribution": "提升边界精度与模型泛化能力"
    }
  },
  {
    "id": "fbe989d87bc5",
    "venue": "NeurIPS",
    "year": 2024,
    "title": "FEDMEKI: A Benchmark for Scaling Medical Foundation Models via Federated Knowledge Injection",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2024/hash/0235cbb8cd6425d0b55daefce388fc0b-Abstract-Datasets_and_Benchmarks_Track.html",
    "abstract": "This study introduces the Federated Medical Knowledge Injection (FedMEKI) platform, a new benchmark designed to address the unique challenges of integrating medical knowledge into foundation models under privacy constraints. By leveraging a cross-silo federated learning approach, FedMEKI circumvents the issues associated with centralized data collection, which is often prohibited under health regulations like the Health Insurance Portability and Accountability Act (HIPAA) in the USA. The platform is meticulously designed to handle multi-site, multi-modal, and multi-task medical data, which includes 7 medical modalities, including images, signals, texts, laboratory test results, vital signs, input variables, and output variables. The curated dataset to validate FedMEKI covers 8 medical tasks, including 6 classification tasks (lung opacity detection, COVID-19 detection, electrocardiogram (ECG) abnormal detection, mortality prediction, sepsis protection, and enlarged cardiomediastinum detection) and 2 generation tasks (medical visual question answering (MedVQA) and ECG noise clarification). This comprehensive dataset is partitioned across several clients to facilitate the decentralized training process under 16 benchmark approaches. FedMEKI not only preserves data privacy but also enhances the capability of medical foundation models by allowing them to learn from a broader spectrum of medical knowledge without direct data exposure, thereby setting a new benchmark in the application of foundation models within the healthcare sector.",
    "summary_cn": "FedMEKI平台通过联邦学习整合多模态医疗数据，在保护隐私下提升基础模型能力，涵盖8项医疗任务，为医疗AI设立新基准。",
    "keywords": [
      "联邦学习",
      "医疗基础模型",
      "隐私保护",
      "多模态数据",
      "基准测试",
      "知识注入"
    ],
    "triple": {
      "method": "跨孤岛联邦学习",
      "result": "提升模型性能并保护数据隐私",
      "contribution": "建立医疗基础模型联邦知识注入基准"
    }
  },
  {
    "id": "2a7e9813bdd1",
    "venue": "NeurIPS",
    "year": 2024,
    "title": "MedSafetyBench: Evaluating and Improving the Medical Safety of Large Language Models",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2024/hash/3ac952d0264ef7a505393868a70a46b6-Abstract-Datasets_and_Benchmarks_Track.html",
    "abstract": "As large language models (LLMs) develop increasingly sophisticated capabilities and find applications in medical settings, it becomes important to assess their medical safety due to their far-reaching implications for personal and public health, patient safety, and human rights. However, there is little to no understanding of the notion of medical safety in the context of LLMs, let alone how to evaluate and improve it. To address this gap, we first define the notion of medical safety in LLMs based on the Principles of Medical Ethics set forth by the American Medical Association. We then leverage this understanding to introduce MedSafetyBench, the first benchmark dataset designed to measure the medical safety of LLMs. We demonstrate the utility of MedSafetyBench by using it to evaluate and improve the medical safety of LLMs. Our results show that publicly-available medical LLMs do not meet standards of medical safety and that fine-tuning them using MedSafetyBench improves their medical safety while preserving their medical performance. By introducing this new benchmark dataset, our work enables a systematic study of the state of medical safety in LLMs and motivates future work in this area, paving the way to mitigate the safety risks of LLMs in medicine. The benchmark dataset and code are available at https://github.com/AI4LIFE-GROUP/med-safety-bench.",
    "summary_cn": "本文提出首个评估大语言模型医疗安全性的基准数据集MedSafetyBench，基于医学伦理原则定义医疗安全，并通过微调提升模型安全性，同时保持医疗性能。",
    "keywords": [
      "大语言模型",
      "医疗安全",
      "基准数据集",
      "医学伦理",
      "微调",
      "评估"
    ],
    "triple": {
      "method": "构建MedSafetyBench基准数据集",
      "result": "公开医疗大语言模型未达安全标准，微调后安全性提升且性能保持",
      "contribution": "定义医疗安全概念，提供首个评估基准，推动医疗大语言模型安全研究"
    }
  },
  {
    "id": "0716a9b34cf3",
    "venue": "NeurIPS",
    "year": 2024,
    "title": "Sm: enhanced localization in Multiple Instance Learning for medical imaging classification",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2024/hash/8db9279f593652ee9bb2223b4a2c43fa-Abstract-Conference.html",
    "abstract": "Multiple Instance Learning (MIL) is widely used in medical imaging classification to reduce the labeling effort. While only bag labels are available for training, one typically seeks predictions at both bag and instance levels (classification and localization tasks, respectively). Early MIL methods treated the instances in a bag independently. Recent methods account for global and local dependencies among instances. Although they have yielded excellent results in classification, their performance in terms of localization is comparatively limited. We argue that these models have been designed to target the classification task, while implications at the instance level have not been deeply investigated. Motivated by a simple observation -- that neighboring instances are likely to have the same label -- we propose a novel, principled, and flexible mechanism to model local dependencies. It can be used alone or combined with any mechanism to model global dependencies (e.g., transformers). A thorough empirical validation shows that our module leads to state-of-the-art performance in localization while being competitive or superior in classification. Our code is at https://github.com/Franblueee/SmMIL.",
    "summary_cn": "提出Sm模块增强多示例学习中的局部依赖建模，提升医学影像分类的定位性能，同时保持分类竞争力。",
    "keywords": [
      "多示例学习",
      "医学影像分类",
      "局部依赖",
      "定位性能",
      "深度学习"
    ],
    "triple": {
      "method": "引入局部依赖建模机制",
      "result": "定位性能达最优，分类表现优异",
      "contribution": "提升医学影像定位准确性"
    }
  },
  {
    "id": "83a1c98de4f0",
    "venue": "NeurIPS",
    "year": 2025,
    "title": "The Boundaries of Fair AI in Medical Image Prognosis: A Causal Perspective",
    "url": "https://neurips.cc/virtual/2025/poster/118323",
    "abstract": "As machine learning (ML) algorithms are increasingly used in medical image analysis, concerns have emerged about their potential biases against certain social groups. Although many approaches have been proposed to ensure the fairness of ML models, most existing works focus only on medical image diagnosis tasks, such as image classification and segmentation, and overlooked prognosis scenarios, which involve predicting the likely outcome or progression of a medical condition over time. To address this gap, we introduce FairTTE, the first comprehensive framework for assessing fairness in time-to-event (TTE) prediction in medical imaging. FairTTE encompasses a diverse range of imaging modalities and TTE outcomes, integrating cutting-edge TTE prediction and fairness algorithms to enable systematic and fine-grained analysis of fairness in medical image prognosis. Leveraging causal analysis techniques, FairTTE uncovers and quantifies distinct sources of bias embedded within medical imaging datasets. Our large-scale evaluation reveals that bias is pervasive across different imaging modalities and that current fairness methods offer limited mitigation. We further demonstrate a strong association between underlying bias sources and model disparities, emphasizing the need for holistic approaches that target all forms of bias. Notably, we find that fairness becomes increasingly difficult to maintain under distribution shifts, underscoring the limitations of existing solutions and the pressing need for more robust, equitable prognostic models.",
    "summary_cn": "本文提出FairTTE框架，首次从因果视角评估医学影像预后中的公平性，揭示普遍偏见并指出现有方法在分布偏移下效果有限。",
    "keywords": [
      "医学影像预后",
      "公平性评估",
      "因果分析",
      "时间到事件预测",
      "偏见来源",
      "分布偏移"
    ],
    "triple": {
      "method": "开发FairTTE框架，整合TTE预测与公平算法",
      "result": "发现偏见普遍存在，现有方法缓解有限，分布偏移下公平性更难维持",
      "contribution": "首次系统评估医学影像预后公平性，强调需针对所有偏见来源的整体方法"
    }
  },
  {
    "id": "66e450919ad0",
    "venue": "NeurIPS",
    "year": 2025,
    "title": "DAAC: Discrepancy-Aware Adaptive Contrastive Learning for Medical Time series",
    "url": "https://neurips.cc/virtual/2025/poster/115757",
    "abstract": "Medical time-series data play a vital role in disease diagnosis but suffer from limited labeled samples and single-center bias, which hinder model generalization and lead to overfitting. To address these challenges, we propose DAAC (Discrepancy-Aware Adaptive Contrastive learning), a learnable multi-view contrastive framework that integrates external normal samples and enhances feature learning through adaptive contrastive strategies. DAAC consists of two key modules: (1) a Discrepancy Estimator, built upon a GAN-enhanced encoder-decoder architecture, captures the distribution of normal data and computes reconstruction errors as indicators of abnormality. These discrepancy features augment the target dataset to mitigate overfitting. (2) an Adaptive Contrastive Learner uses multi-head attention to extract discriminative representations by contrasting embeddings across multiple views and data granularities (subject, trial, epoch, and temporal levels), eliminating the need for handcrafted positive-negative sample pairs. Extensive experiments on three clinical datasets—covering Alzheimer’s disease, Parkinson’s disease, and myocardial infarction—demonstrate that DAAC significantly outperforms existing methods, even when only 10\\% of labeled data is available, showing strong generalization and diagnostic performance. Our code is available at https://github.com/CUHKSZ-MED-BioE/DAAC.",
    "summary_cn": "提出DAAC框架，通过差异感知和自适应对比学习，利用外部正常样本增强医学时间序列特征，有效缓解过拟合，在阿尔茨海默病等数据集上显著提升诊断性能。",
    "keywords": [
      "医学时间序列",
      "对比学习",
      "过拟合缓解",
      "多视图学习",
      "异常检测",
      "自适应策略"
    ],
    "triple": {
      "method": "差异感知自适应对比学习框架",
      "result": "在三个临床数据集上显著优于现有方法",
      "contribution": "提升小样本下的泛化与诊断性能"
    }
  },
  {
    "id": "82a9e6c662a1",
    "venue": "NeurIPS",
    "year": 2024,
    "title": "A Textbook Remedy for Domain Shifts: Knowledge Priors for Medical Image Analysis",
    "url": "https://neurips.cc/virtual/2024/poster/95098",
    "abstract": "While deep networks have achieved broad success in analyzing natural images, when applied to medical scans, they often fail in unexcepted situations. We investigate this challenge and focus on model sensitivity to domain shifts, such as data sampled from different hospitals or data confounded by demographic variables such as sex, race, etc, in the context of chest X-rays and skin lesion images. A key finding we show empirically is that existing visual backbones lack an appropriate prior from the architecture for reliable generalization in these settings. Taking inspiration from medical training, we propose giving deep networks a prior grounded in explicit medical knowledge communicated in natural language. To this end, we introduce Knowledge-enhanced Bottlenecks (KnoBo), a class of concept bottleneck models that incorporates knowledge priors that constrain it to reason with clinically relevant factors found in medical textbooks or PubMed. KnoBo uses retrieval-augmented language models to design an appropriate concept space paired with an automatic training procedure for recognizing the concept. We evaluate different resources of knowledge and recognition architectures on a broad range of domain shifts across 20 datasets. In our comprehensive evaluation with two imaging modalities, KnoBo outperforms fine-tuned models on confounded datasets by 32.4% on average. Finally, evaluations reveal that PubMed is a promising resource for making medical models less sensitive to domain shift, outperforming other resources on both diversity of information and final prediction performance.",
    "summary_cn": "针对医学图像分析中的域偏移问题，提出知识增强瓶颈模型，通过整合医学教科书和PubMed知识，提升模型在跨域数据集上的泛化性能。",
    "keywords": [
      "域偏移",
      "医学图像分析",
      "知识先验",
      "概念瓶颈模型",
      "PubMed",
      "泛化性能"
    ],
    "triple": {
      "method": "知识增强瓶颈模型整合医学知识",
      "result": "在20个数据集上平均提升32.4%性能",
      "contribution": "提升模型对域偏移的鲁棒性"
    }
  },
  {
    "id": "c853ca563291",
    "venue": "NeurIPS",
    "year": 2024,
    "title": "Image-aware Evaluation of Generated Medical Reports",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2024/hash/0fbbc5129cafcee8530223b8565561ac-Abstract-Conference.html",
    "abstract": "The paper proposes a novel evaluation metric for automatic medical report generation from X-ray images, VLScore. It aims to overcome the limitations of existing evaluation methods, which either focus solely on textual similarities, ignoring clinical aspects, or concentrate only on a single clinical aspect, the pathology, neglecting all other factors. The key idea of our metric is to measure the similarity between radiology reports while considering the corresponding image. We demonstrate the benefit of our metric through evaluation on a dataset where radiologists marked errors in pairs of reports, showing notable alignment with radiologists' judgments. In addition, we provide a new dataset for evaluating metrics. This dataset includes well-designed perturbations that distinguish between significant modifications (e.g., removal of a diagnosis) and insignificant ones. It highlights the weaknesses in current evaluation metrics and provides a clear framework for analysis.",
    "summary_cn": "提出VLScore新指标，结合图像评估X光报告生成质量，优于现有方法，与放射科医生判断一致，并提供新数据集验证。",
    "keywords": [
      "医学报告生成",
      "评估指标",
      "图像感知",
      "放射学",
      "VLScore",
      "数据集"
    ],
    "triple": {
      "method": "提出VLScore指标，结合图像与文本相似度",
      "result": "与放射科医生判断一致，优于现有评估方法",
      "contribution": "提供新评估指标和数据集，提升医学报告生成评估质量"
    }
  },
  {
    "id": "11838c9667a4",
    "venue": "NeurIPS",
    "year": 2024,
    "title": "A Benchmark for Long-Form Medical Question Answering",
    "url": "https://neurips.cc/virtual/2024/103919",
    "abstract": "There is a lack of benchmarks for evaluating large language models (LLMs) in long-form medical question answering (QA). Most existing benchmarks for medical QA evaluation focus on automatic metrics and multiple-choice questions. While valuable, these benchmarks do not fully capture or assess the complexities of real-world clinical applications where LLMs are being deployed. Furthermore, the limited studies on long-form answer generation in medical QA are primarily closed-source, with no access to human medical expert annotations, making it difficult to reproduce results and improve baselines. In this work, we introduce a new publicly available benchmark featuring real-world consumer medical questions with long-form answer evaluations annotated by medical doctors. We conduct pairwise comparisons of responses from various open and closed medical and general-purpose LLMs based on criteria such as correctness, helpfulness, harmfulness, and bias. Additionally, we perform a comprehensive LLM-as-a-judge analysis to study the alignment between human judgments and LLMs. Our preliminary results highlight the strong potential of open LLMs in medical QA compared to leading closed models.",
    "summary_cn": "本文提出一个公开的长篇医学问答基准，包含医生标注的真实问题与答案，用于评估大语言模型在临床应用中的表现。",
    "keywords": [
      "医学问答",
      "基准测试",
      "大语言模型",
      "长篇答案",
      "医生标注",
      "模型评估"
    ],
    "triple": {
      "method": "构建公开基准并进行模型比较",
      "result": "开源模型在医学问答中表现潜力强",
      "contribution": "填补长篇医学问答评估空白"
    }
  },
  {
    "id": "d368b704236c",
    "venue": "NeurIPS",
    "year": 2025,
    "title": "The Adaptive Neuron Network Expansion for Medical  Image Segmentation",
    "url": "https://neurips.cc/virtual/2025/131937",
    "abstract": "Many deep learning architectures struggle to combine the benefits of local feature extraction and global contextual awareness, limiting their effectiveness in complex segmentation tasks (i.e., medical image segmentation). In this work, we introduce ANNE: The Adaptive Neuron Network Expansion for 2D Medical Image Segmentation. ANNE is an efficient yet computationally effective deep learning architecture, inspired by the Remez and Progressive Expansion Neurons algorithms, designed to approximate complex functions with fewer trainable parameters. Our model utilizes Mamba and Adaptable Progressive Expansion Neurons-based techniques to better leverage the combined benefits of local and global features. Experimental results demonstrate competitive performance in various medical image segmentation tasks, including retinal, polyp, and skin lesion segmentation, while achieving a significantly reduced number of trainable parameters compared to existing deep learning models.",
    "summary_cn": "提出ANNE模型，结合Mamba与自适应扩展神经元，以较少参数实现医学图像分割，在视网膜、息肉等任务中表现优异。",
    "keywords": [
      "医学图像分割",
      "自适应扩展神经元",
      "Mamba",
      "参数效率",
      "深度学习",
      "局部与全局特征"
    ],
    "triple": {
      "method": "结合Mamba与自适应扩展神经元",
      "result": "分割性能优异且参数显著减少",
      "contribution": "提出高效轻量分割模型ANNE"
    }
  },
  {
    "id": "72208a85110d",
    "venue": "NeurIPS",
    "year": 2025,
    "title": "Toward a Vision-Language Foundation Model for Medical Data: Multimodal Dataset and Benchmarks for Vietnamese PET/CT Report Generation",
    "url": "https://neurips.cc/virtual/2025/poster/121676",
    "abstract": "Vision-Language Foundation Models (VLMs), trained on large-scale multimodal datasets, have driven significant advances in Artificial Intelligence (AI) by enabling rich cross-modal reasoning. Despite their success in general domains, applying these models to medical imaging remains challenging due to the limited availability of diverse imaging modalities and multilingual clinical data. Most existing medical VLMs are trained on a subset of imaging modalities and focus primarily on high-resource languages, thus limiting their generalizability and clinical utility. To address these limitations, we introduce a novel Vietnamese-language multimodal medical dataset consisting of 2,757 whole-body PET/CT volumes from independent patients and their corresponding full-length clinical reports. This dataset is designed to fill two pressing gaps in medical AI development: (1) the lack of PET/CT imaging data in existing VLMs training corpora, which hinders the development of models capable of handling functional imaging tasks; and (2) the underrepresentation of low-resource languages, particularly the Vietnamese language, in medical vision-language research. To the best of our knowledge, this is the first dataset to provide comprehensive PET/CT-report pairs in Vietnamese. We further introduce a training framework to enhance VLMs' learning, including data augmentation and expert-validated test sets. We conduct comprehensive experiments benchmarking state-of-the-art VLMs on downstream tasks, including medical report generation and visual question answering. The experimental results show that incorporating our dataset significantly improves the performance of existing VLMs. However, despite these advancements, the models still underperform on clinically critical criteria, particularly the diagnosis of lung cancer, indicating substantial room for future improvement. We believe this dataset and benchmark will serve as a pivotal step in advancing the development of more robust VLMs for medical imaging, particularly in low-resource languages, and improving their clinical relevance in Vietnamese healthcare.",
    "summary_cn": "本文构建首个越南语PET/CT影像与报告数据集，提出增强训练框架，显著提升视觉语言模型在医学报告生成等任务上的性能，但临床关键指标仍有改进空间。",
    "keywords": [
      "视觉语言模型",
      "PET/CT影像",
      "越南语数据集",
      "医学报告生成",
      "低资源语言",
      "多模态学习"
    ],
    "triple": {
      "method": "构建越南语PET/CT数据集与增强训练框架",
      "result": "提升现有模型性能，但肺癌诊断等临床指标仍不足",
      "contribution": "填补PET/CT数据与低资源语言空白，推动医学AI发展"
    }
  },
  {
    "id": "c827b4c346ae",
    "venue": "NeurIPS",
    "year": 2025,
    "title": "LoMix: Learnable Weighted Multi-Scale Logits Mixing for Medical Image Segmentation",
    "url": "https://neurips.cc/virtual/2025/poster/119650",
    "abstract": "The NeurIPS Logo above may be used on presentations. Right-click and choose\n                            download. It is a vector graphic and may be used at any scale.",
    "summary_cn": "LoMix提出可学习的加权多尺度逻辑混合方法，提升医学图像分割性能，通过自适应融合多尺度特征优化分割结果。",
    "keywords": [
      "医学图像分割",
      "多尺度特征融合",
      "可学习加权",
      "逻辑混合",
      "深度学习"
    ],
    "triple": {
      "method": "可学习的加权多尺度逻辑混合",
      "result": "提升分割性能",
      "contribution": "优化医学图像分割结果"
    }
  },
  {
    "id": "de7b4761cd59",
    "venue": "NeurIPS",
    "year": 2025,
    "title": "Demo: Orchestrating Large Language Model Agents and Resources for Medical Deep Research",
    "url": "https://neurips.cc/virtual/2025/124871",
    "abstract": "Deep Research represents the convergence of large language models (LLMs), advanced reasoning, and information retrieval for expert-level inquiry. Existing Deep Research systems are constrained by reliability issues, limited integration with specialized resources, and inflexible output formats. In this paper, we introduce \\textbf{Medical Deep Research}, an open, agentic system designed for in-depth medical and clinical investigations. Our framework features a multi-agent research module and a resource module that integrates curated medical tools and can dynamically discover Model Context Protocols (MCPs). Through fine-grained design for planning, tool orchestration, query processing, report formatting, and MCP integration, the system supports comprehensive medical information retrieval and customizable output generation. We evaluate this system across four key aspects: resource coverage, tractability, correctness, and helpfulness. Our evaluation results demonstrate the potential of Medical Deep Research to serve as a reliable and powerful platform for medical research.",
    "summary_cn": "本文提出Medical Deep Research系统，结合多智能体与资源模块，支持深度医学检索与定制化报告生成，提升可靠性与灵活性。",
    "keywords": [
      "医学研究",
      "大语言模型",
      "多智能体系统",
      "信息检索",
      "资源集成",
      "报告生成"
    ],
    "triple": {
      "method": "多智能体与资源模块设计",
      "result": "系统在资源覆盖、可追踪性、正确性和实用性方面表现良好",
      "contribution": "提供可靠、灵活的医学深度研究平台"
    }
  },
  {
    "id": "1987227232fb",
    "venue": "NeurIPS",
    "year": 2024,
    "title": "MDAgents: An Adaptive Collaboration of LLMs for Medical Decision-Making",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2024/hash/90d1fc07f46e31387978b88e7e057a31-Abstract-Conference.html",
    "abstract": "Foundation models are becoming valuable tools in medicine. Yet despite their promise, the best way to leverage Large Language Models (LLMs) in complex medical tasks remains an open question. We introduce a novel multi-agent framework, named **M**edical **D**ecision-making **Agents** (**MDAgents**) that helps to address this gap by automatically assigning a collaboration structure to a team of LLMs. The assigned solo or group collaboration structure is tailored to the medical task at hand, a simple emulation inspired by the way real-world medical decision-making processes are adapted to tasks of different complexities. We evaluate our framework and baseline methods using state-of-the-art LLMs across a suite of real-world medical knowledge and clinical diagnosis benchmarks, including a comparison ofLLMs’ medical complexity classification against human physicians. MDAgents achieved the **best performance in seven out of ten** benchmarks on tasks requiring an understanding of medical knowledge and multi-modal reasoning, showing a significant **improvement of up to 4.2\\%** ($p$ < 0.05) compared to previous methods' best performances. Ablation studies reveal that MDAgents effectively determines medical complexity to optimize for efficiency and accuracy across diverse medical tasks. Notably, the combination of moderator review and external medical knowledge in group collaboration resulted in an average accuracy **improvement of 11.8\\%**. Our code can be found at https://github.com/mitmedialab/MDAgents.",
    "summary_cn": "MDAgents提出多智能体框架，根据医疗任务复杂度自动分配LLM协作结构，在多项医学基准测试中表现最佳，准确率提升最高达4.2%。",
    "keywords": [
      "多智能体框架",
      "医疗决策",
      "大语言模型",
      "自适应协作",
      "医学基准测试",
      "准确率提升"
    ],
    "triple": {
      "method": "自适应多智能体协作框架",
      "result": "十项基准中七项最佳，准确率提升最高4.2%",
      "contribution": "优化医疗任务中LLM协作效率与准确性"
    }
  },
  {
    "id": "8e9bfc3118bc",
    "venue": "NeurIPS",
    "year": 2025,
    "title": "Beyond Conventional Transformers: A Medical X-ray Attention Block for Improved Multi-Label Diagnosis",
    "url": "https://neurips.cc/virtual/2025/128744",
    "abstract": "Transformers have reshaped visual recognition through generic self-attention, yet their application to specialized domains like medical imaging remains underexplored. In this work, we introduce the Medical X-ray Attention (MXA) block, a domain-specific attention mechanism designed specifically for multi-label chest X-ray diagnosis. Unlike conventional attention modules, MXA augments transformer backbones with inductive priors tailored to radiology, including a lightweight region-of-interest pooling and CBAM-style channel–spatial attention, both integrated in parallel with multi-head self-attention. To reduce the computational burden of traditional transformers and support deployment in clinical settings, we embed MXA within an Efficient Vision Transformer (EfficientViT) and apply knowledge distillation from a calibrated DenseNet-121 teacher. This combined approach produces a model that is both accurate and resource-efficient. Our framework achieves 0.85 mean AUC on the CheXpert benchmark, representing a +0.19 absolute improvement and approximately 233\\% relative improvement over chance-level performance (AUC = 0.5) compared to a vanilla EfficientViT baseline. These results demonstrate that attention modules can be overfit in a beneficial, task-aware sense to the unique structure and demands of clinical imaging. More broadly, we show that transformers do not need to remain generic, and that domain-specific attention can bridge the gap between expressive global modeling and real-world deployment.",
    "summary_cn": "提出医学X射线注意力块（MXA），结合区域池化与通道-空间注意力，集成于高效视觉Transformer，提升多标签胸片诊断性能，在CheXpert基准上达到0.85平均AUC。",
    "keywords": [
      "医学影像",
      "注意力机制",
      "多标签诊断",
      "高效视觉Transformer",
      "知识蒸馏",
      "CheXpert"
    ],
    "triple": {
      "method": "设计MXA块，集成区域池化与通道-空间注意力，结合高效视觉Transformer与知识蒸馏",
      "result": "在CheXpert基准上平均AUC达0.85，相对基线提升显著",
      "contribution": "提出领域专用注意力机制，提升医学影像诊断性能与部署效率"
    }
  },
  {
    "id": "ede817c3a688",
    "venue": "NeurIPS",
    "year": 2024,
    "title": "DiversityMedQA: A Benchmark for Assessing Demographic Biases in Medical Diagnosis using Large Language Models",
    "url": "https://neurips.cc/virtual/2024/103874",
    "abstract": "As large language models (LLMs) gain traction in healthcare, concerns about their susceptibility to demographic biases are growing. We introduce DiversityMedQA, a novel benchmark designed to assess LLM responses to medical queries across diverse patient demographics, such as gender and ethnicity. By perturbing questions from the MedQA dataset, which comprises medical board exam questions, we created a benchmark that captures the nuanced differences in medical diagnosis across varying patient profiles. Our findings reveal notable discrepancies in model performance when tested against these demographic variations. Furthermore, to ensure the perturbations were accurate, we also propose a filtering strategy that validates each perturbation. By releasing DiversityMedQA, we provide a resource for evaluating and mitigating demographic bias in LLM medical diagnoses.",
    "summary_cn": "提出DiversityMedQA基准，通过扰动MedQA问题评估大语言模型在不同人口统计特征（如性别、种族）下的医疗诊断偏见，发现模型性能存在显著差异。",
    "keywords": [
      "DiversityMedQA",
      "人口统计偏见",
      "大语言模型",
      "医疗诊断",
      "基准评估",
      "扰动验证"
    ],
    "triple": {
      "method": "扰动MedQA问题并过滤验证",
      "result": "模型性能随人口特征变化出现差异",
      "contribution": "提供评估医疗诊断偏见的基准资源"
    }
  },
  {
    "id": "a6c82e90cbd5",
    "venue": "NeurIPS",
    "year": 2024,
    "title": "Can Large Language Models Provide Emergency Medical Help Where There Is No Ambulance? A Comparative Study on Large Language Model Understanding of Emergency Medical Scenarios in Resource-Constrained Settings",
    "url": "https://neurips.cc/virtual/2024/108252",
    "abstract": "There are a few medicine-oriented evaluation datasets and benchmarks for assessing the performance of various LLMs in clinical scenarios; however, there is a paucity of information on the real-world utility of LLMs in context-specific scenarios in resource-constrained settings. In this study, 16 iterations of a decision support tool for medical emergencies using 4 distinct generalized LLMs were constructed, alongside a combination of 4 Prompt Engineering techniques. In total 428 model responses were quantitatively and qualitatively evaluated by 22 clinicians familiar with the medical scenarios and background contexts. The best model-technique pair had a mean rating of 8.05/10. Our study highlights the benefits of In-Context Learning with few-shot prompting, and the utility of the relatively novel self-questioning prompting technique. We demonstrate the benefits of combining various prompting techniques to elicit the best performance of LLMs. We also highlight the need for continuous human expert verification in the development and deployment of LLM-based health applications, especially in use cases where context is paramount.",
    "summary_cn": "研究评估四种通用大语言模型在资源有限医疗场景中的应急决策支持能力，结合提示工程技术，经临床专家评估，最佳模型-技术对平均评分8.05/10。",
    "keywords": [
      "大语言模型",
      "应急医疗",
      "资源有限",
      "提示工程",
      "临床评估",
      "决策支持"
    ],
    "triple": {
      "method": "结合四种提示工程技术构建决策支持工具",
      "result": "最佳模型-技术对平均评分8.05/10",
      "contribution": "验证提示工程组合与专家验证在医疗应用中的重要性"
    }
  },
  {
    "id": "182f7d24cf5d",
    "venue": "NeurIPS",
    "year": 2024,
    "title": "Towards Privacy-Preserving Medical Imaging: Federated Learning with Differential Privacy and Secure Aggregation Using a Modified ResNet Architecture",
    "url": "https://neurips.cc/virtual/2024/109149",
    "abstract": "With increasing concerns over privacy in healthcare, especially for sensitive medical data, this research introduces a federated learning framework that combines local differential privacy and secure aggregation using Secure Multi-Party Computation for medical image classification. Further, we propose DPResNet, a modified ResNet architecture optimized for differential privacy. Leveraging the BloodMNIST benchmark dataset, we simulate a realistic data-sharing environment across different hospitals, addressing the distinct privacy challenges posed by federated healthcare data. Experimental results indicate that our privacy-preserving federated model achieves accuracy levels close to non-private models, surpassing traditional approaches while maintaining strict data confidentiality. By enhancing the privacy, efficiency, and reliability of healthcare data management, our approach offers substantial benefits to patients, healthcare providers, and the broader healthcare ecosystem.",
    "summary_cn": "本研究提出结合差分隐私和安全聚合的联邦学习框架DPResNet，用于医疗图像分类，在保护隐私的同时实现接近非私有模型的准确率。",
    "keywords": [
      "联邦学习",
      "差分隐私",
      "医疗图像分类",
      "安全聚合",
      "DPResNet",
      "隐私保护"
    ],
    "triple": {
      "method": "联邦学习结合差分隐私与安全聚合，使用改进的ResNet架构",
      "result": "在BloodMNIST数据集上实现接近非私有模型的准确率",
      "contribution": "提升医疗数据隐私保护与分类性能"
    }
  },
  {
    "id": "1425f3cd8e14",
    "venue": "NeurIPS",
    "year": 2024,
    "title": "SM3-Text-to-Query: Synthetic Multi-Model Medical Text-to-Query Benchmark",
    "url": "https://neurips.cc/virtual/2024/poster/97708",
    "abstract": "Electronic health records (EHRs) are stored in various database systems with different database models on heterogeneous storage architectures, such as relational databases, document stores, or graph databases. These different database models have a big impact on query complexity and performance. While this has been a known fact in database research, its implications for the growing number of Text-to-Query systems have surprisingly not been investigated so far.In this paper, we present SM3-Text-to-Query, the first multi-model medical Text-to-Query benchmark based on synthetic patient data from Synthea, following the SNOMED-CT taxonomy---a widely used knowledge graph ontology covering medical terminology. SM3-Text-to-Query provides data representations for relational databases (PostgreSQL), document stores (MongoDB), and graph databases (Neo4j and GraphDB (RDF)), allowing the evaluation across four popular query languages, namely SQL, MQL, Cypher, and SPARQL.We systematically and manually develop 408 template questions, which we augment to construct a benchmark of 10K diverse natural language question/query pairs for these four query languages (40K pairs overall). On our dataset, we evaluate several common in-context-learning (ICL) approaches for a set of representative closed and open-source LLMs.Our evaluation sheds light on the trade-offs between database models and query languages for different ICL strategies and LLMs. Last,SM3-Text-to-Query is easily extendable to additional query languages or real, standard-based patient databases.",
    "summary_cn": "提出首个多模型医疗文本转查询基准SM3-Text-to-Query，基于合成患者数据，支持四种查询语言评估，揭示不同数据库模型与LLM性能权衡。",
    "keywords": [
      "文本转查询",
      "多模型数据库",
      "医疗基准",
      "合成数据",
      "查询语言评估",
      "大语言模型"
    ],
    "triple": {
      "method": "基于Synthea合成数据构建多模型基准，手动开发模板问题并扩展为10K问题/查询对",
      "result": "评估多种ICL方法与LLM，揭示数据库模型与查询语言对性能的权衡关系",
      "contribution": "提供首个可扩展的多模型医疗文本转查询基准，支持跨数据库系统比较"
    }
  },
  {
    "id": "b490ba9f0e80",
    "venue": "NeurIPS",
    "year": 2025,
    "title": "Geometric Shape Matching for Explainable and Accurate Medical Image Segmentation: A Post-Processing Refinement Framework",
    "url": "https://neurips.cc/virtual/2025/128789",
    "abstract": "Deep learning models for medical image segmentation, while achieving remarkable performance, often produce anatomically implausible outputs that compromise clinical trust and adoption. We propose a novel inference-time refinement framework that leverages geometric shape matching against a curated library of high-quality organ segmentations to enhance TotalSegmentator predictions without requiring retraining or ground truth data. Our approach provides interpretable corrections by comparing predicted segmentations with anatomically plausible reference templates through a geometry-based matching framework. The framework operates as a modular post-processing layer, addressing TotalSegmentator's occasional anatomical hallucinations while maintaining compatibility with existing clinical workflows. Proof-of-concept experiments on liver segmentation using the CT-ORG dataset demonstrate an average 15\\% improvement in Dice scores for poor-performing segmentations. This work presents a promising direction for improving segmentation reliability in clinical deployment while preserving the interpretability required for medical applications.",
    "summary_cn": "提出一种基于几何形状匹配的后处理框架，通过高质量器官分割库优化TotalSegmentator预测，无需重新训练或真实数据，提高分割准确性和可解释性。",
    "keywords": [
      "几何形状匹配",
      "医学图像分割",
      "后处理框架",
      "可解释性",
      "TotalSegmentator",
      "临床部署"
    ],
    "triple": {
      "method": "几何形状匹配后处理",
      "result": "Dice分数平均提升15%",
      "contribution": "提升分割可靠性与可解释性"
    }
  },
  {
    "id": "0fbd37e9dd9c",
    "venue": "NeurIPS",
    "year": 2025,
    "title": "MedREK: Retrieval-Based Editing for Medical LLMs with Key-Aware Prompts",
    "url": "https://neurips.cc/virtual/2025/133175",
    "abstract": "LLMs hold great promise for healthcare applications, but the rapid evolution of medical knowledge and errors in training data often cause them to generate outdated or inaccurate information, limiting their applicability in high-stakes clinical practice. Model editing has emerged as a potential remedy without full retraining. While parameter-based editing often compromises locality and is thus ill-suited for the medical domain, retrieval-based editing offers a more viable alternative. However, it still faces two critical challenges: (1) representation overlap within the medical knowledge space often causes inaccurate retrieval and reduces editing accuracy; (2) existing methods are restricted to single-sample edits, while batch-editing remains largely unexplored despite its importance for real-world medical applications. To address these challenges, we first construct MedVersa, an enhanced benchmark with broader coverage of medical subjects, designed to evaluate both single and batch edits under strict locality constraints. We then propose MedREK, a retrieval-based editing framework that integrates a shared query-key module for precise matching with an attention-based prompt encoder for informative guidance. Experimental results on various medical benchmarks demonstrate that our MedREK achieves superior performance across different core metrics and provides the first validated solution for batch-editing in medical LLMs. Our code and dataset are available at https://github.com/mylittleriver/MedREK.",
    "summary_cn": "针对医学大模型知识过时问题，提出MedREK检索编辑框架，通过共享查询键模块和注意力提示编码器提升编辑精度，支持批量编辑，在医学基准测试中表现优异。",
    "keywords": [
      "医学大模型",
      "检索编辑",
      "批量编辑",
      "知识更新",
      "MedREK",
      "MedVersa"
    ],
    "triple": {
      "method": "共享查询键模块与注意力提示编码器",
      "result": "在医学基准测试中实现优异性能",
      "contribution": "首个验证的医学大模型批量编辑解决方案"
    }
  },
  {
    "id": "e95978cef606",
    "venue": "NeurIPS",
    "year": 2024,
    "title": "Medical question-generation for pre-consultation with LLM in-context learning",
    "url": "https://neurips.cc/virtual/2024/106861",
    "abstract": "Pre-consultation gives healthcare providersa history of present illness (HPI) prior to a patient's visit,streamlining the visit and promoting shared decision making.Compared to a digital questionnaire,LLM-powered AI agents have proven successful inproviding a more natural interface for pre-consultation.But LLM-based approaches struggle to ask productive follow-up questions andrequire complex prompts to guide the consultation.While effective automated prompting strategies exist for medicalquestion-answering LLMs, the task of question generation for pre-consultationis lacking effective strategies.In this study, we develop a methodology for evaluating existing approaches to medical pre-consultation,using prior datasets of HPIs and patient-doctor dialogue.We propose a novel approach of converting abundant clinical note datainto question generation demonstrations and then retrieving relevantdemonstrations for in-context learning.We find this approach to question generation for pre-consultationachieves a higher recall of facts in ground truth consultationscompared against competitive baselines in prior literature across a range of simultated patient personalities.",
    "summary_cn": "本研究提出一种新方法，将临床笔记转化为问题生成示例，用于LLM上下文学习，以提高预咨询中事实召回率。",
    "keywords": [
      "预咨询",
      "问题生成",
      "上下文学习",
      "临床笔记",
      "大语言模型",
      "事实召回"
    ],
    "triple": {
      "method": "临床笔记转示例并检索用于上下文学习",
      "result": "事实召回率高于基线",
      "contribution": "提升预咨询问题生成效果"
    }
  },
  {
    "id": "36e793c33258",
    "venue": "NeurIPS",
    "year": 2024,
    "title": "Med-FastSAM: Improving Transfer Efficiency of SAM to Domain-Generalised Medical Image Segmentation",
    "url": "https://neurips.cc/virtual/2024/103888",
    "abstract": "Medical image segmentation is a crucial computer vision task in medical image analysis. Recently, the Segment Anything Model (SAM) has made significant advancements in natural image segmentation. Despite current studies indicating the potential of SAM to revolutionise medical image segmentation using parameter-efficient fine-tuning techniques, it still faces three primary challenges. Firstly, these methods still rely on the large vision transformer of SAM, which is computationally expensive. Secondly, the point and box prompt modes of SAM demand manual annotations, which are time-consuming and expensive in medical scenarios and reduce their clinical applicability. Thirdly, SAM leverages large-size patches to predict masks, resulting in the loss of fine-grained details. To address these limitations, in this paper, we propose a fast-transferring architecture for adapting SAM to domain-generalised medical image segmentation, named Med-FastSAM. Specifically, we introduce a lightweight knowledge aggregation encoder that combines the distilled natural image knowledge with learned medical-specific information for producing feature representation. Moreover, we devise a coarse prompt module to automatically generate coarse masks for guiding segmentation decoding. Furthermore, we design a multi-scale feature decoder to produce precise segmentation masks. Eventually, extensive experiments on four benchmark datasets have been conducted to evaluate the proposed model. The result demonstrates that Med-FastSAM outperforms state-of-the-art methods without any manual prompts. Especially, our model shows excellent zero-shot domain generalisation performance by using only 15.45\\% parameters compared to the standard SAM. The code for our work and more technical details can be found at https://github.com/GalacticHogrider/Med-FastSAM.",
    "summary_cn": "提出Med-FastSAM，通过轻量知识聚合编码器、粗提示模块和多尺度解码器，实现无需手动提示的高效医学图像分割，参数仅需SAM的15.45%。",
    "keywords": [
      "医学图像分割",
      "域泛化",
      "轻量模型",
      "自动提示",
      "多尺度解码",
      "零样本学习"
    ],
    "triple": {
      "method": "轻量知识聚合编码器、粗提示模块、多尺度解码器",
      "result": "超越现有方法，零样本域泛化性能优秀，参数减少至15.45%",
      "contribution": "实现高效无手动提示的医学图像分割模型"
    }
  },
  {
    "id": "76a4623c44d0",
    "venue": "NeurIPS",
    "year": 2025,
    "title": "MedVLThinker: Simple Baselines for Multimodal Medical Reasoning",
    "url": "https://neurips.cc/virtual/2025/124929",
    "abstract": "Large Reasoning Models (LRMs) have introduced a new paradigm in AI by enabling models to ``think before responding\" via chain-of-thought reasoning. However, the absence of open and reproducible recipes for building reasoning-centric medical LMMs hinders community-wide research, analysis, and comparison. In this paper, we present MedVLThinker, a suite of simple yet strong baselines. Our fully open recipe consists of: (1) systematic data curation for both text-only and image-text medical data, filtered according to varying levels of reasoning difficulty, and (2) two training paradigms: Supervised Fine-Tuning (SFT) on distilled reasoning traces and Reinforcement Learning with Verifiable Rewards (RLVR) based on final answer correctness. Across extensive experiments on the Qwen2.5-VL model family (3B, 7B) and six medical QA benchmarks, we find that RLVR consistently and significantly outperforms SFT. Additionally, under the RLVR framework, a key, counter-intuitive finding is that training on our curated text-only reasoning data provides a more substantial performance boost than training on multimodal image-text data. Our best open 7B model, trained using the RLVR recipe on text-only data, establishes a new state-of-the-art on existing public VQA benchmarks, surpassing all previous open-source medical LMMs. Furthermore, scaling our model to 32B achieves performance on par with the proprietary GPT-4o. We will release all curated data, models, and code to provide the community with a strong, open foundation for future research in multimodal medical reasoning.",
    "summary_cn": "提出MedVLThinker，一种用于多模态医学推理的简单基线方法。通过强化学习与可验证奖励训练，仅使用文本推理数据即超越现有开源模型，性能媲美GPT-4o。",
    "keywords": [
      "多模态医学推理",
      "强化学习",
      "可验证奖励",
      "数据筛选",
      "开源基准",
      "链式思维"
    ],
    "triple": {
      "method": "使用强化学习与可验证奖励训练",
      "result": "仅文本数据训练效果优于多模态，性能达GPT-4o水平",
      "contribution": "提供开源基准与数据，推动医学推理研究"
    }
  },
  {
    "id": "53762c62dba0",
    "venue": "NeurIPS",
    "year": 2025,
    "title": "MedVCTP: Improving Accuracy and Explainability in Medical Visual Reasoning",
    "url": "https://neurips.cc/virtual/2025/129997",
    "abstract": "Reasoning transparency and accuracy are critical to the implementation of AI algorithms in medical applications. However, modern medical visual-language models (VLMs) often generate conclusions without explicit reasoning, limiting clinician trust and potentially compromising the quality of diagnosis. Reasoning-focused VLMs remain confined to basic VQA datasets (e.g., A-OKVQA), while medical VLMs lack reasoning transparency, modularity, and output refinement capabilities. We introduce Medical Visual Chain-of-Thought Processing (MedVCTP), a training-free framework implementing a structured See–Think–Confirm pipeline. The See stage extracts global and regional visual concepts via advanced visual encoders. The Think stage generates reasoning-grounded answers through LLM-based chain-of-thought processing. The Confirm stage iteratively refines rationales via multi-shot prompting and cross-modal CLIP-based consistency checks, aligning reasoning with visual context to mitigate hallucination and enable visual grounding. Our modular design supports rapid deployment with interchangeable components for scalable performance. On SLAKE, MedVCTP achieves 85.8% accuracy-a 19.4% improvement over ablations without CLIP refinement—demonstrating that iterative cross-modal validation directly enhances both accuracy and reasoning coherence. These results establish MedVCTP as a step toward reliable, explainable medical visual reasoning systems deployable without task-specific training. Code and artifacts will be made available upon acceptance.",
    "summary_cn": "提出MedVCTP框架，通过See-Think-Confirm三阶段实现免训练医学视觉推理，提升准确性与可解释性。",
    "keywords": [
      "医学视觉推理",
      "可解释性",
      "链式思维",
      "免训练框架",
      "跨模态验证"
    ],
    "triple": {
      "method": "See-Think-Confirm三阶段管道",
      "result": "SLAKE数据集准确率达85.8%，提升19.4%",
      "contribution": "增强推理透明度与准确性，支持免训练部署"
    }
  },
  {
    "id": "4a2f2a1f4183",
    "venue": "NeurIPS",
    "year": 2024,
    "title": "MEDS-torch: An ML Pipleine for Inductive Experiments for EHR Medical Foundation Models",
    "url": "https://neurips.cc/virtual/2024/103000",
    "abstract": "We introduce meds-torch, a scalable and extensible pipeline designed to process any medical dataset adhering to the MEDS format—a universal schema for medical time series. We systematically compare three tokenization methods (Everything In Code, Triplet, and Text Code) and evaluate five transfer learning techniques, including variations of autoregressive generative modeling and contrastive learning, across multiple predictive tasks on the MIMIC-IV EHR dataset. Our empirical analysis provides actionable insights into the effectiveness of each method, demonstrating that certain tokenization and pretraining combinations significantly outperform others. By benchmarking these approaches against fully supervised learning models, we offer practical recommendations for selecting appropriate modeling strategies in diverse healthcare settings. The meds-torch pipeline not only streamlines the application of these methods but also promotes reproducibility and standardization in EHR research, facilitating more effective machine learning applications in healthcare.",
    "summary_cn": "MEDS-torch是一个可扩展的医疗数据处理管道，用于比较不同标记化和迁移学习方法在电子健康记录预测任务中的效果，提供实用建模建议。",
    "keywords": [
      "MEDS-torch",
      "电子健康记录",
      "标记化方法",
      "迁移学习",
      "机器学习管道",
      "MIMIC-IV"
    ],
    "triple": {
      "method": "比较三种标记化方法和五种迁移学习技术",
      "result": "某些组合显著优于其他方法",
      "contribution": "提供实用建模建议并促进研究标准化"
    }
  },
  {
    "id": "5cc6382097d0",
    "venue": "NeurIPS",
    "year": 2025,
    "title": "Shallow Robustness, Deep Vulnerabilities: Multi-Turn Evaluation of Medical LLMs",
    "url": "https://neurips.cc/virtual/2025/124951",
    "abstract": "Large language models (LLMs) are rapidly transitioning into medical clinical use, yet their reliability under realistic, multi-turn interactions remains poorly understood. Existing evaluation frameworks typically assess single-turn question answering under idealized conditions, overlooking the complexities of medical consultations where conflicting input, misleading context, and authority influence are common. We introduce MedQA-Followup, a framework for systematically evaluating multi-turn robustness in medical question answering. Our approach distinguishes between shallow robustness (resisting misleading initial context) and deep robustness (maintaining accuracy when answers are challenged across turns), while also introducing an indirect–direct axis that separates contextual framing (indirect) from explicit suggestion (direct). Using controlled interventions on the MedQA dataset, we evaluate five state-of-the-art LLMs and find that while models perform reasonably well under shallow perturbations, they exhibit severe vulnerabilities in multi-turn settings, with accuracy dropping from 91.2\\% to as low as 13.5\\% for Claude Sonnet 4. Counterintuitively, indirect, context-based interventions are often more harmful than direct suggestions, yielding larger accuracy drops across models and exposing a significant vulnerability for clinical deployment. Further compounding analyses reveal model differences, with some showing further performance drops under repeated interventions while others partially recovering or even improving. These findings highlight multi-turn robustness as a critical but underexplored dimension for safe and reliable deployment of medical LLMs.",
    "summary_cn": "研究评估医疗大语言模型在多轮对话中的鲁棒性，发现模型在浅层干扰下表现尚可，但在深层多轮挑战中准确性大幅下降，间接干预尤其有害。",
    "keywords": [
      "医疗大语言模型",
      "多轮对话评估",
      "鲁棒性",
      "MedQA-Followup",
      "临床部署",
      "准确性下降"
    ],
    "triple": {
      "method": "引入MedQA-Followup框架，区分浅层与深层鲁棒性，并在MedQA数据集上进行控制干预",
      "result": "模型在浅层干扰下表现较好，但多轮设置中准确性从91.2%降至13.5%，间接干预危害更大",
      "contribution": "揭示多轮鲁棒性是医疗LLM安全部署的关键未探索维度，强调临床部署中的脆弱性"
    }
  },
  {
    "id": "3807e4edcccd",
    "venue": "NeurIPS",
    "year": 2025,
    "title": "Personalized English Amharic Medical Image Caption and Speech Generation for Visually Impaired Patients Using Vision Transformer Fused with LLM",
    "url": "https://neurips.cc/virtual/2025/125497",
    "abstract": "Access to medical information is critical for healthcare equity, particularly for visually impaired citizens and lowresource language speakers. Our goal is to create a model that enables visually impaired individuals to access their medical image results, by turn the text into an audio message, and translate generated captions into local languages to understand their medical results in their mother tongue. developing algorithms that can generate captions, translating into Amharic and generate speech from images is a major goal of our study by fussing computer vision and Generative AI. Accessible images are essential for those who are blind or visually impaired.  In this study, following the design science approach, the data were gathered from the Tikur Anbessa specialized hospital, Addis Ababa University, and the data annotation was carried out by a domain expert. We preprocessed the data to make suitable for models. The work presents a novel approach model fusion such as Vision Transformer (ViT)GPT2, VIT-Llama2, and VGG16-LSTM architectures for medical image captioning. The model is designed to generate detailed captions for radiologists, translate the generated caption into Amharic, and speech for visually impaired patients. Among the model's ViT-Llama2 model generate high-quality caption and robust feature extraction, ensures precise, context-aware captions. Experiments demonstrate the effectiveness of this method, VIT-Llama2 achieving a high BLEU score of 0.633\\% in image captioning and enhanced usability and accessibility.  The system is deployed as a user-friendly application that accepts medical images as input, processes them through the models, outputs textual captions, translates generated caption into Amharic, and speech. This model bridges the gap in medical accessibility for low-resource language speakers, empowering visually impaired individuals and understand their medical image results.",
    "summary_cn": "本研究融合视觉Transformer与大型语言模型，开发了为视障患者生成医学图像描述、翻译成阿姆哈拉语并转换为语音的系统，提升医疗信息可及性。",
    "keywords": [
      "医学图像描述",
      "视觉Transformer",
      "大型语言模型",
      "阿姆哈拉语翻译",
      "语音生成",
      "视障辅助"
    ],
    "triple": {
      "method": "融合ViT与LLM（如ViT-Llama2）",
      "result": "BLEU得分0.633%，生成高质量描述与语音",
      "contribution": "提升低资源语言视障患者的医疗信息可及性"
    }
  },
  {
    "id": "f909505ab67c",
    "venue": "NeurIPS",
    "year": 2024,
    "title": "2:30 - 3:30 PM: Applying SAM to Few-shot Medical Image Segmentation using Mask Propagation and Auto-prompting",
    "url": "https://neurips.cc/virtual/2024/109352",
    "abstract": "Applying SAM to Few-shot Medical Image Segmentation using Mask Propagation and Auto-prompting",
    "summary_cn": "本研究提出一种结合掩码传播与自动提示的方法，将SAM应用于少样本医学图像分割，提升了分割精度与效率。",
    "keywords": [
      "SAM",
      "少样本学习",
      "医学图像分割",
      "掩码传播",
      "自动提示",
      "深度学习"
    ],
    "triple": {
      "method": "掩码传播与自动提示",
      "result": "提升分割精度与效率",
      "contribution": "扩展SAM在少样本医学分割的应用"
    }
  },
  {
    "id": "d20526b47e59",
    "venue": "NeurIPS",
    "year": 2024,
    "title": "Gaze-Assisted Medical Image Segmentation",
    "url": "https://neurips.cc/virtual/2024/103920",
    "abstract": "The annotation of patient organs is a crucial part of various diagnostic and treatment procedures, such as radiotherapy planning. Manual annotation is extremely time-consuming, while its automation using modern image analysis techniques has not yet reached levels sufficient for clinical adoption. This paper investigates the idea of semi-supervised medical image segmentation using human gaze as interactive input for segmentation correction. In particular, we fine-tuned the Segment Anything Model in Medical Images (MedSAM), a public solution that uses various prompt types as additional input for semi-automated segmentation correction. We used human gaze data from reading abdominal images as a prompt for fine-tuning MedSAM. The model was validated on a public WORD database, which consists of 120 CT scans of 16 abdominal organs. The results of the gaze-assisted MedSAM were shown to be superior to the results of the state-of-the-art segmentation models. In particular, the average Dice coefficient for 16 abdominal organs was 85.8\\%, 86.7\\%, 81.7\\%, and 90.5\\% for nnUNetV2, ResUNet, original MedSAM, and our gaze-assisted MedSAM model, respectively.",
    "summary_cn": "本研究利用人眼注视数据微调MedSAM模型，实现半监督医学图像分割。在腹部CT数据集上，该模型分割性能优于现有先进模型，平均Dice系数达90.5%。",
    "keywords": [
      "医学图像分割",
      "注视数据",
      "MedSAM",
      "半监督学习",
      "腹部器官",
      "CT扫描"
    ],
    "triple": {
      "method": "利用注视数据微调MedSAM",
      "result": "平均Dice系数90.5%，优于基准模型",
      "contribution": "提出注视辅助的半监督分割方法"
    }
  },
  {
    "id": "efda7fa4a7cb",
    "venue": "NeurIPS",
    "year": 2024,
    "title": "BenchX: A Unified Benchmark Framework for Medical Vision-Language Pretraining on Chest X-Rays",
    "url": "https://neurips.cc/virtual/2024/poster/97555",
    "abstract": "Medical Vision-Language Pretraining (MedVLP) shows promise in learning generalizable and transferable visual representations from paired and unpaired medical images and reports. MedVLP can provide useful features to downstream tasks and facilitate adapting task-specific models to new setups using fewer examples. However, existing MedVLP methods often differ in terms of datasets, preprocessing, and finetuning implementations. This pose great challenges in evaluating how well a MedVLP method generalizes to various clinically-relevant tasks due to the lack of unified, standardized, and comprehensive benchmark. To fill this gap, we propose BenchX, a unified benchmark framework that enables head-to-head comparison and systematical analysis between MedVLP methods using public chest X-ray datasets. Specifically, BenchX is composed of three components: 1) Comprehensive datasets covering nine datasets and four medical tasks; 2) Benchmark suites to standardize data preprocessing, train-test splits, and parameter selection; 3) Unified finetuning protocols that accommodate heterogeneous MedVLP methods for consistent task adaptation in classification, segmentation, and report generation, respectively. Utilizing BenchX, we establish baselines for nine state-of-the-art MedVLP methods and found that the performance of some early MedVLP methods can be enhanced to surpass more recent ones, prompting a revisiting of the developments and conclusions from prior works in MedVLP. Our code are available at https://github.com/yangzhou12/BenchX.",
    "summary_cn": "提出BenchX统一基准框架，用于标准化评估胸部X光医学视觉-语言预训练方法，涵盖多数据集与任务，促进公平比较与性能提升。",
    "keywords": [
      "医学视觉-语言预训练",
      "胸部X光",
      "基准框架",
      "标准化评估",
      "多任务学习",
      "性能比较"
    ],
    "triple": {
      "method": "构建统一基准框架BenchX",
      "result": "提升早期方法性能并超越新方法",
      "contribution": "促进MedVLP方法公平比较与系统分析"
    }
  },
  {
    "id": "e387dd9043fd",
    "venue": "NeurIPS",
    "year": 2024,
    "title": "Demographic Bias of Expert-Level Vision-Language Foundation Models in Medical Imaging",
    "url": "https://neurips.cc/virtual/2024/106883",
    "abstract": "Advances in artificial intelligence (AI) have achieved expert-level performance in medical imaging applications. Notably, self-supervised vision-language foundation models can detect a broad spectrum of pathologies without relying on explicit training annotations. However, it is crucial to ensure that these AI models do not mirror or amplify human biases, disadvantaging historically marginalized groups such as females or Black patients. In this study, we investigate the algorithmic fairness of state-of-the-art vision-language foundation models in chest X-ray diagnosis across five globally-sourced datasets. Our findings reveal that compared to board-certified radiologists, these foundation models consistently underdiagnose marginalized groups, with even higher rates seen in intersectional subgroups such as Black female patients. Such biases present over a wide range of pathologies and demographic attributes. Further analysis of the model embedding uncovers its significant encoding of demographic information beyond human levels. Deploying medical AI systems with biases can intensify pre-existing care disparities, posing potential challenges to equitable healthcare access and raising ethical questions about their clinical applications. Code is available at: https://github.com/YyzHarry/vlm-fairness.",
    "summary_cn": "研究发现，医学影像中的视觉语言基础模型在胸片诊断中存在人口统计学偏见，对边缘群体（如黑人女性）诊断不足，可能加剧医疗不平等。",
    "keywords": [
      "医学影像",
      "视觉语言模型",
      "人口统计学偏见",
      "公平性",
      "胸片诊断",
      "边缘群体"
    ],
    "triple": {
      "method": "分析五个全球数据集中的模型表现",
      "result": "模型对边缘群体诊断不足，编码人口信息超人类水平",
      "contribution": "揭示AI模型偏见风险，呼吁临床应用中关注公平性"
    }
  },
  {
    "id": "eef0e3793579",
    "venue": "NeurIPS",
    "year": 2025,
    "title": "Towards Doctor-Like Reasoning: Medical RAG Fusing Knowledge with Patient Analogy through Textual Gradients",
    "url": "https://neurips.cc/virtual/2025/poster/118649",
    "abstract": "Existing medical RAG systems mainly leverage knowledge from medical knowledge bases, neglecting the crucial role of experiential knowledge derived from similar patient cases - a key component of human clinical reasoning. To bridge this gap, we propose DoctorRAG, a RAG framework that emulates doctor-like reasoning by integrating both explicit clinical knowledge and implicit case-based experience. DoctorRAG enhances retrieval precision by first allocating conceptual tags for queries and knowledge sources, together with a hybrid retrieval mechanism from both relevant knowledge and patient. In addition, a Med-TextGrad module using multi-agent textual gradients is integrated to ensure that the final output adheres to the retrieved knowledge and patient query. Comprehensive experiments on multilingual, multitask datasets demonstrate that DoctorRAG significantly outperforms strong baseline RAG models and gains improvements from iterative refinements. Our approach generates more accurate, relevant, and comprehensive responses, taking a step towards more doctor-like medical reasoning systems.",
    "summary_cn": "提出DoctorRAG框架，融合医学知识与类似病例经验，通过标签分配、混合检索及文本梯度模块提升推理准确性，在多语言多任务数据集上表现优异。",
    "keywords": [
      "医学RAG",
      "病例类比",
      "混合检索",
      "文本梯度",
      "临床推理",
      "多语言任务"
    ],
    "triple": {
      "method": "标签分配与混合检索结合Med-TextGrad模块",
      "result": "在多语言多任务数据集上显著超越基线模型",
      "contribution": "模拟医生推理，提升回答准确性、相关性和全面性"
    }
  },
  {
    "id": "8b4c197e4330",
    "venue": "NeurIPS",
    "year": 2025,
    "title": "CARES: Comprehensive Evaluation of Safety and Adversarial Robustness in Medical LLMs",
    "url": "https://neurips.cc/virtual/2025/poster/121833",
    "abstract": "Large language models (LLMs) are increasingly deployed in medical contexts, raising critical concerns about safety, alignment, and susceptibility to adversarial manipulation. While prior benchmarks assess model refusal capabilities for harmful prompts, they often lack clinical specificity, graded harmfulness levels, and coverage of jailbreak-style attacks. We introduce CARES (Clinical Adversarial Robustness and Evaluation of Safety), a benchmark for evaluating LLM safety in healthcare. CARES includes over 18,000 prompts spanning eight medical safety principles, four harm levels, and four prompting styles—direct, indirect, obfuscated, and role-play—to simulate both malicious and benign use cases. We propose a three-way response evaluation protocol (Accept, Caution, Refuse) and a fine-grained Safety Score metric to assess model behavior. Our analysis reveals that many state-of-the-art LLMs remain vulnerable to jailbreaks that subtly rephrase harmful prompts, while also over-refusing safe but atypically phrased queries. Finally, we propose a mitigation strategy using a lightweight classifier to detect jailbreak attempts and steer models toward safer behavior via reminder-based conditioning. CARES provides a rigorous framework for testing and improving medical LLM safety under adversarial and ambiguous conditions.",
    "summary_cn": "CARES基准评估医疗大语言模型安全性与对抗鲁棒性，包含多级危害提示，揭示模型易受攻击且过度拒绝安全查询，提出轻量级分类器缓解策略。",
    "keywords": [
      "医疗大语言模型",
      "安全性评估",
      "对抗鲁棒性",
      "基准测试",
      "缓解策略",
      "临床安全"
    ],
    "triple": {
      "method": "构建CARES基准，含多级危害提示与三向评估协议",
      "result": "模型易受攻击且过度拒绝安全查询",
      "contribution": "提供测试框架与轻量级分类器缓解策略"
    }
  },
  {
    "id": "495db139a0f1",
    "venue": "NeurIPS",
    "year": 2025,
    "title": "ExGra-Med: Extended Context Graph Alignment for Medical Vision-Language Models",
    "url": "https://neurips.cc/virtual/2025/poster/117826",
    "abstract": "State-of-the-art medical multi-modal LLMs (med-MLLMs), such as LLaVA-Med and BioMedGPT, primarily depend on scaling model size and data volume, with training driven largely by autoregressive objectives. However, we reveal that this approach can lead to weak vision-language alignment, making these models overly dependent on costly instruction-following data. To address this, we introduce ExGra-Med, a novel multi-graph alignment framework that jointly aligns images, instruction responses, and extended captions in the latent space, advancing semantic grounding and cross-modal coherence. To scale to large LLMs (e.g., LLaMa-7B), we develop an efficient end-to-end training scheme using black-box gradient estimation, enabling fast and scalable optimization. Empirically, ExGra-Med matches LLaVA-Med’s performance using just 10\\% of pre-training data, achieving a 20.13\\% gain on VQA-RAD and approaching full-data performance. It also outperforms strong baselines like BioMedGPT and RadFM on visual chatbot and zero-shot classification tasks, demonstrating its promise for efficient, high-quality vision-language integration in medical AI.",
    "summary_cn": "ExGra-Med提出多图对齐框架，通过联合对齐图像、指令响应和扩展标题，提升医学视觉语言模型的语义基础和跨模态一致性，仅用10%数据即匹配LLaVA-Med性能。",
    "keywords": [
      "医学视觉语言模型",
      "多图对齐",
      "跨模态学习",
      "高效训练",
      "语义基础",
      "零样本分类"
    ],
    "triple": {
      "method": "多图对齐框架与黑盒梯度估计训练",
      "result": "仅用10%数据匹配LLaVA-Med，VQA-RAD提升20.13%",
      "contribution": "实现高效高质量的医学视觉语言集成"
    }
  },
  {
    "id": "513e066860e4",
    "venue": "NeurIPS",
    "year": 2025,
    "title": "Faithful or Just Plausible? Evaluating Faithfulness for Medical Reasoning in Closed-Source LLMs",
    "url": "https://neurips.cc/virtual/2025/124886",
    "abstract": "Closed-source large language models (LLMs), such as ChatGPT and Gemini, are increasingly consulted for medical advice, yet their explanations may appear plausible while failing to reflect the model’s underlying reasoning process. This gap poses serious risks as patients and clinicians may trust coherent but misleading explanations. We conduct a systematic black-box evaluation of faithfulness in medical reasoning among three widely used closed-source LLMs. Our study consists of three perturbation-based probes: (1) causal ablation, testing whether stated chain-of-thought (CoT) reasoning causally influences predictions; (2) positional bias, examining whether models create post-hoc justifications for answers driven by input positioning; and (3) hint injection, testing susceptibility to external suggestions. We complement these quantitative probes with a small-scale human evaluation of model responses to patient-style medical queries to examine concordance between physician assessments of explanation faithfulness and layperson perceptions of trustworthiness. We find that CoT reasoning steps often do not causally drive predictions, and models readily incorporate external hints without acknowledgment. In contrast, positional biases showed minimal impact in this setting. These results underscore that faithfulness, not just accuracy, must be central in evaluating LLMs for medicine, to ensure both public protection and safe clinical deployment.",
    "summary_cn": "评估闭源大语言模型在医学推理中的忠实性，发现其解释常与推理过程不符，存在因果脱钩和外部提示影响，强调需重视忠实性以确保安全应用。",
    "keywords": [
      "闭源大语言模型",
      "医学推理",
      "忠实性评估",
      "因果消融",
      "提示注入",
      "位置偏差"
    ],
    "triple": {
      "method": "基于扰动的黑盒评估与人类评估",
      "result": "推理步骤常不驱动预测，易受外部提示影响",
      "contribution": "强调医学评估需重视忠实性"
    }
  },
  {
    "id": "94cb99b71577",
    "venue": "NeurIPS",
    "year": 2025,
    "title": "Trustworthiness of LLMs in Grading and Demographic Fairness in Medical RAG",
    "url": "https://neurips.cc/virtual/2025/133826",
    "abstract": "Large Language Models (LLMs) have quickly become essential in healthcare, but even top models trained on vast biomedical datasets can generate factually incorrect or biased responses, with notable implications for medicine. Prior work shows that retrieval-augmented generation (RAG) can improve accuracy, raising the question of whether retrieval mitigates, exacerbates, or leaves bias unaffected, or if it introduces new forms of bias. To study this, we investigate patient-level bias through gender and ethnicity perturbations and physician-level bias through persona prompting, where models grade answers of a “female doctor” or “male doctor”. For the patient-level bias experiment, we compare a baseline LLM (GPT-4o Mini) to a MedRAG-enhanced version with two retrieval strategies(MedCPT and BM25) and two external corpora(Textbooks and Statpearls), while we extend evaluation to multiple models (GPT-4o Mini, Meta LLaMA 3.1 8B, Gemini, and Claude 3.5 Haiku) for the physician-level bias experiment, measuring endorsement and rejection accuracy to detect conservativeness or sycophancy.   We evaluate on the DiversityMedQA dataset, which perturbed MedQA questions along gender and ethnicity, totaling to 1,040 gender items and 1,068 ethnicity items where the clinically correct answer remains invariant under demographic edits. To reduce confounding, we modified only references to the primary patient, creating parallel male, female, and genderless versions, and used the released ethnicity subset as it was released by the dataset authors. We selected 300 gender pairs and 300 ethnicity pairs for analysis and utilized evaluation metrics including first-index accuracy, Majority@5, and total proportion, and applied two-proportion Z-tests to measure statistically significant disparities across demographic perturbations.  Altering the patient’s gender or ethnicity in the query had a negligible effect on base model performance: for both perturbations, first-answer accuracy, within the range of 69–71% for the original questions showed similar pattern for the perturbed pairs, and statistical tests confirmed no significant differences, indicating that diagnostic accuracy was not noticeably biased by patient demographic description. Retrieval augmentation (RAG) consistently boosted accuracy into the low-to-mid 70% range, improving performance without introducing new disparities. In the physician-persona experiments, all models showed a consistent conservative bias—more accurate at rejecting incorrect answers than endorsing correct ones—with highly significant gaps ranging from ~9 points (Claude) to over 40 points (GPT-4o Mini and LLaMA 3.1 8B). Persona settings (female vs. male doctor) produced very similar patterns of conservativeness with no significant gender differences, and we did not observe evidence of sycophancy. Instead, models systematically leaned conservative, rejecting incorrect answers more reliably than confirming correct ones.",
    "summary_cn": "研究评估LLMs在医学问答中的偏见与公平性，发现患者性别/种族扰动对诊断准确性无显著影响，RAG提升准确性且未引入新偏见；医生角色实验中模型普遍呈现保守倾向，但无性别差异或谄媚行为。",
    "keywords": [
      "大型语言模型",
      "检索增强生成",
      "医疗公平性",
      "偏见评估",
      "医学问答",
      "保守倾向"
    ],
    "triple": {
      "method": "使用DiversityMedQA数据集，通过性别/种族扰动和医生角色提示评估模型",
      "result": "患者扰动无显著偏见，RAG提升准确性；模型普遍呈现保守倾向，无性别差异",
      "contribution": "揭示了RAG在医疗中的公平性影响及模型的保守偏差特征"
    }
  },
  {
    "id": "9dcd6c22d1c7",
    "venue": "NeurIPS",
    "year": 2024,
    "title": "E2ENet: Dynamic Sparse Feature Fusion for Accurate and Efficient 3D Medical Image Segmentation",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2024/hash/d6be2b51b213f4f5994243ccb494d97e-Abstract-Conference.html",
    "abstract": "Deep neural networks have evolved as the leading approach in 3D medical image segmentation due to their outstanding performance. However, the ever-increasing model size and computational cost of deep neural networks have become the primary barriers to deploying them on real-world, resource-limited hardware. To achieve both segmentation accuracy and efficiency, we propose a 3D medical image segmentation model called Efficient to Efficient Network (E2ENet), which incorporates two parametrically and computationally efficient designs. i. Dynamic sparse feature fusion (DSFF) mechanism: it adaptively learns to fuse informative multi-scale features while reducing redundancy. ii. Restricted depth-shift in 3D convolution: it leverages the 3D spatial information while keeping the model and computational complexity as 2D-based methods. We conduct extensive experiments on AMOS, Brain Tumor Segmentation and BTCV Challenge, demonstrating that E2ENet consistently achieves a superior trade-off between accuracy and efficiency than prior arts across various resource constraints. %In particular, with a single model and single scale, E2ENet achieves comparable accuracy on the large-scale challenge AMOS-CT, while saving over 69% parameter count and 27% FLOPs in the inference phase, compared with the previousbest-performing method. Our code has been made available at: https://github.com/boqian333/E2ENet-Medical.",
    "summary_cn": "提出E2ENet模型，通过动态稀疏特征融合和受限深度移位3D卷积，在3D医学图像分割中实现高精度与高效率的平衡。",
    "keywords": [
      "3D医学图像分割",
      "动态稀疏特征融合",
      "计算效率",
      "E2ENet",
      "深度神经网络",
      "多尺度特征"
    ],
    "triple": {
      "method": "动态稀疏特征融合与受限深度移位3D卷积",
      "result": "在多个数据集上实现精度与效率的优越权衡，参数量减少69%，计算量降低27%",
      "contribution": "提出高效3D分割模型E2ENet，提升部署可行性"
    }
  },
  {
    "id": "06e80f3849db",
    "venue": "NeurIPS",
    "year": 2024,
    "title": "A Textbook Remedy for Domain Shifts: Knowledge Priors for Medical Image Analysis",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2024/hash/a4e683f0ce6b91e7fbdae9d32642d88f-Abstract-Conference.html",
    "abstract": "While deep networks have achieved broad success in analyzing natural images, when applied to medical scans, they often fail in unexcepted situations. We investigate this challenge and focus on model sensitivity to domain shifts, such as data sampled from different hospitals or data confounded by demographic variables such as sex, race, etc, in the context of chest X-rays and skin lesion images. A key finding we show empirically is that existing visual backbones lack an appropriate prior from the architecture for reliable generalization in these settings. Taking inspiration from medical training, we propose giving deep networks a prior grounded in explicit medical knowledge communicated in natural language. To this end, we introduce Knowledge-enhanced Bottlenecks (KnoBo), a class of concept bottleneck models that incorporates knowledge priors that constrain it to reason with clinically relevant factors found in medical textbooks or PubMed. KnoBo uses retrieval-augmented language models to design an appropriate concept space paired with an automatic training procedure for recognizing the concept. We evaluate different resources of knowledge and recognition architectures on a broad range of domain shifts across 20 datasets. In our comprehensive evaluation with two imaging modalities, KnoBo outperforms fine-tuned models on confounded datasets by 32.4% on average. Finally, evaluations reveal that PubMed is a promising resource for making medical models less sensitive to domain shift, outperforming other resources on both diversity of information and final prediction performance.",
    "summary_cn": "针对医学图像分析中的领域偏移问题，提出知识增强瓶颈模型，通过整合医学教科书和PubMed知识先验，提升模型在跨域数据上的泛化性能。",
    "keywords": [
      "领域偏移",
      "知识先验",
      "医学图像分析",
      "概念瓶颈模型",
      "PubMed",
      "泛化性能"
    ],
    "triple": {
      "method": "知识增强瓶颈模型整合医学知识先验",
      "result": "在20个数据集上平均提升32.4%性能",
      "contribution": "降低模型对领域偏移的敏感性"
    }
  },
  {
    "id": "87888c6cea75",
    "venue": "NeurIPS",
    "year": 2025,
    "title": "Stabilizing Reasoning in Medical LLMs with Continued Pretraining and Reasoning Preference Optimization",
    "url": "https://neurips.cc/virtual/2025/124895",
    "abstract": "Large Language Models (LLMs) show potential in medicine, yet clinical adoption is hindered by concerns over factual accuracy, language-specific limitations (e.g., Japanese), and critically, their reliability when required to generate reasoning explanations---a prerequisite for trust. This paper introduces Preferred-MedLLM-Qwen-72B, a 72B-parameter model optimized for the Japanese medical domain to achieve both high accuracy and stable reasoning. We employ a two-stage fine-tuning process on the Qwen2.5-72B base model: first, Continued Pretraining (CPT) on a comprehensive Japanese medical corpus instills deep domain knowledge. Second, Reasoning Preference Optimization (RPO), a preference-based method, enhances the generation of reliable reasoning pathways while preserving high answer accuracy. Evaluations on the Japanese Medical Licensing Exam benchmark (IgakuQA) show Preferred-MedLLM-Qwen-72B achieves state-of-the-art performance (0.868 accuracy), surpassing strong proprietary models like GPT-4o (0.866). Crucially, unlike baseline or CPT-only models which exhibit significant accuracy degradation (up to 11.5\\% and 3.8\\% respectively on IgakuQA) when prompted for explanations, our model maintains its high accuracy (0.868) under such conditions. This highlights RPO's effectiveness in stabilizing reasoning generation. This work underscores the importance of optimizing for reliable explanations alongside accuracy. We release the Preferred-MedLLM-Qwen-72B model weights to foster research into trustworthy LLMs for specialized, high-stakes applications.",
    "summary_cn": "提出Preferred-MedLLM-Qwen-72B模型，通过持续预训练和推理偏好优化，在日语医学领域实现高准确率与稳定推理，在执照考试基准上超越GPT-4o。",
    "keywords": [
      "医学大语言模型",
      "推理稳定性",
      "持续预训练",
      "偏好优化",
      "日语医学",
      "模型性能"
    ],
    "triple": {
      "method": "两阶段微调（持续预训练与推理偏好优化）",
      "result": "在日语医学考试基准上达到0.868准确率，超越GPT-4o",
      "contribution": "提升模型推理稳定性，并公开模型权重"
    }
  },
  {
    "id": "8b8c0fd2cb85",
    "venue": "NeurIPS",
    "year": 2025,
    "title": "SMI: Semantic Medical ID for Hierarchy-Aware Concept Representation",
    "url": "https://neurips.cc/virtual/2025/133204",
    "abstract": "Recent advances in generative AI have accelerated the use of language models (LMs) for clinical prediction tasks. However, existing biomedical LMs often struggle to capture clinically meaningful relationships among medical concepts, as they rely solely on data-driven text learning and overlook domain knowledge. In this study, we propose Semantic Medical ID (SMI) , a novel representation framework that integrates an expert-defined medical ontology into LM-based embeddings. By leveraging the hierarchical structure of medical ontologies, SMIs generate embeddings that preserve clinical relationships across major disease categories, subcategories, and specific conditions, enhancing interpretability for clinical end users. Experimental results demonstrate that SMI improves predictive accuracy in mortality and readmission tasks. SMI also exhibits greater robustness under cross-hospital distribution shifts, highlighting its effectiveness in producing clinically generalizable representations.",
    "summary_cn": "提出语义医学ID框架，整合医学本体到语言模型嵌入中，提升临床预测准确性和可解释性。",
    "keywords": [
      "语义医学ID",
      "医学本体",
      "语言模型",
      "临床预测",
      "可解释性",
      "分布偏移"
    ],
    "triple": {
      "method": "整合医学本体到语言模型嵌入",
      "result": "提高预测准确性及跨医院稳健性",
      "contribution": "增强临床关系表示与可解释性"
    }
  },
  {
    "id": "741b5a1c90ff",
    "venue": "NeurIPS",
    "year": 2024,
    "title": "Enhancing Medical NLP Systems: Integrating Upstash Vector and BGE-M3 for Accurate and Ethical Healthcare Data Management with Reduced Bias",
    "url": "https://neurips.cc/virtual/2024/107775",
    "abstract": "This paper proposes a novel NLP model in healthcare by including Utash Vector for in-time and contextual information retrieval and BGE-M3 for advanced understanding. The model overcomes the challenges posed by the existing systems, such as incomplete data retrieval, a semantically inconsistent database, and algorithm bias. Incorporating bias mitigation measures and fairness audits, it guarantees no unfair treatment of patients belonging to different groups. Aligned with the AMA Code of Medical Ethics, provides proper management of Electronic Health Records in better ways in terms of transparency, confidentiality, and accuracy. Although these problems are relieved, the accuracy of information is still a major issue, the abuse of artificial intelligence remains a risk, and the use of the AMA Code to guide the integration of artificial intelligence has its limitations. Each of these must operate with defensible use of AI and auditing as well as explanation of AI usage in clinical decision-making.",
    "summary_cn": "本文提出一种医疗NLP模型，结合Upstash Vector和BGE-M3提升数据检索与理解，减少算法偏见，确保患者公平，并遵循医学伦理管理电子健康记录。",
    "keywords": [
      "医疗NLP",
      "算法偏见",
      "电子健康记录",
      "伦理合规",
      "信息检索",
      "公平性审计"
    ],
    "triple": {
      "method": "集成Upstash Vector和BGE-M3",
      "result": "提升检索准确性并减少偏见",
      "contribution": "推动医疗AI的伦理与公平应用"
    }
  },
  {
    "id": "7c5fdd0a4314",
    "venue": "NeurIPS",
    "year": 2025,
    "title": "Cryptographic Fingerprinting for Medical AI: A Proof-of-Concept Approach to Protecting Healthcare ML Models from API Extraction",
    "url": "https://neurips.cc/virtual/2025/128627",
    "abstract": "Medical AI models represent valuable intellectual property that increasingly face threats from API-based model extraction attacks. We introduce a novel cryptographic fingerprinting approach designed specifically for healthcare machine learning models that embeds detectable watermarks while preserving clinical accuracy. Our method modifies uncertainty quantification patterns in neural network outputs to create cryptographically secure fingerprints without affecting medical predictions. Through rigorous experiments on ECG pattern classification using a dataset of 1,200 synthetic cardiac signals across 4 conditions, we demonstrate perfect accuracy preservation (99.33\\% maintained) while enabling statistical detection of model theft. Our proof-of-concept shows that sophisticated attackers achieving 98.33\\% extraction accuracy with 1,500 API queries produce 99\\% victim-surrogate agreement—providing statistical evidence of intellectual property theft. The approach introduces modest computational overhead (+12.92\\%), which remains practical for high-value medical AI deployment. This work establishes the first framework for protecting medical AI intellectual property through post-training fingerprinting and provides a foundation for future research in healthcare AI security.",
    "summary_cn": "提出一种医疗AI模型加密指纹方法，通过修改神经网络不确定性量化模式嵌入水印，保持临床准确性，有效检测模型窃取。",
    "keywords": [
      "医疗AI安全",
      "加密指纹",
      "模型提取攻击",
      "不确定性量化",
      "知识产权保护",
      "心电图分类"
    ],
    "triple": {
      "method": "修改神经网络不确定性量化模式嵌入水印",
      "result": "保持99.33%准确性，实现窃取检测，计算开销增加12.92%",
      "contribution": "建立首个医疗AI后训练指纹保护框架"
    }
  },
  {
    "id": "21afdc13c1cf",
    "venue": "NeurIPS",
    "year": 2024,
    "title": "Deep Learning in Medical Image Registration: Magic or Mirage?",
    "url": "https://neurips.cc/virtual/2024/poster/93821",
    "abstract": "Classical optimization and learning-based methods are the two reigning paradigms in deformable image registration. While optimization-based methods boast generalizability across modalities and robust performance, learning-based methods promise peak performance, incorporating weak supervision and amortized optimization. However, the exact conditions for either paradigm to perform well over the other are shrouded and not explicitly outlined in the existing literature. In this paper, we make an explicit correspondence between the mutual information of the distribution of per-pixel intensity and labels, and the performance of classical registration methods. This strong correlation hints to the fact that architectural designs in learning-based methods is unlikely to affect this correlation, and therefore, the performance of learning-based methods. This hypothesis is thoroughly validated with state-of-the-art classical and learning-based methods. However, learning-based methods with weak supervision can perform high-fidelity intensity and label registration, which is not possible with classical methods. Next, we show that this high-fidelity feature learning does not translate to invariance to domain shift, and learning-based methods are sensitive to such changes in the data distribution. We reassess and recalibrate performance expectations from classical and DLIR methods under access to label supervision, training time, and its generalization capabilities under minor domain shifts.",
    "summary_cn": "本文探讨了医学图像配准中经典优化与基于学习方法的性能对比，揭示了基于学习的方法在弱监督下能实现高保真配准，但对领域偏移敏感。",
    "keywords": [
      "医学图像配准",
      "深度学习",
      "弱监督",
      "领域偏移",
      "性能评估",
      "互信息"
    ],
    "triple": {
      "method": "分析互信息与性能关联，验证经典与学习方法",
      "result": "学习方法在弱监督下实现高保真配准但对领域偏移敏感",
      "contribution": "重新校准了经典与学习方法在监督、训练时间和泛化能力方面的性能期望"
    }
  },
  {
    "id": "4b43c3facf68",
    "venue": "NeurIPS",
    "year": 2025,
    "title": "Towards Memory-Efficient Foundation Models in Medical Imaging: A Federated Learning and Knowledge Distillation Approach",
    "url": "https://neurips.cc/virtual/2025/124939",
    "abstract": "The rapid development of medical foundation models has shown great promise for various healthcare applications. However, fine-tuning these models for downstream tasks remains challenging due to privacy concerns that limit centralized data collection from diverse sources. Federated learning (FL) offers a privacy-preserving solution by enabling multiple clients to collaboratively train a global model without sharing their local data. Despite its advantages, FL must balance model performance with communication and computation costs. Existing approaches often use parameter-efficient fine-tuning (PEFT) techniques to reduce communication overhead by transmitting fewer parameters. However, these methods require clients to host large foundation models, which is impractical for clients with limited memory. Meanwhile, conventional knowledge distillation (KD) methods fall short in FL due to misalignment between pre-trained foundation models and specific downstream tasks. To overcome these limitations, we propose Federated Reprogramming Knowledge Distillation (FedRD), a method that uses lightweight student models in clients and a medical foundation model on the server. A reprogramming module aligns the foundation model's feature space with the downstream task, enabling student models to mimic this representation collaboratively. FedRD significantly reduces memory and computation requirements while maintaining high accuracy. Experiments on three medical imaging datasets under non-IID data distributions demonstrate that FedRD outperforms federated KD and PEFT methods, offering an effective trade-off between accuracy, communication, and computational efficiency.",
    "summary_cn": "提出FedRD方法，结合联邦学习与知识蒸馏，使用轻量学生模型和服务器基础模型，在保护隐私下降低内存与计算需求，提升医疗影像任务性能。",
    "keywords": [
      "联邦学习",
      "知识蒸馏",
      "医疗影像",
      "内存高效",
      "隐私保护",
      "非独立同分布数据"
    ],
    "triple": {
      "method": "FedRD结合联邦学习与知识蒸馏",
      "result": "降低内存计算需求，保持高准确率",
      "contribution": "实现隐私、效率与性能的平衡"
    }
  },
  {
    "id": "3ed8965c00e2",
    "venue": "NeurIPS",
    "year": 2025,
    "title": "SynLLM: A Comparative Analysis of Large Language Models for Medical Tabular Synthetic Data Generation via Prompt Engineering",
    "url": "https://neurips.cc/virtual/2025/124918",
    "abstract": "Access to real-world medical data is often restricted due to privacy regulations, posing a significant barrier to the advancement of healthcare research. Synthetic data offers a promising alternative; however, generating realistic, clinically valid, and privacy-conscious records remains a major challenge. Recent advancements in Large Language Models (LLMs) offer new opportunities for structured data generation; however, existing approaches frequently lack systematic prompting strategies and comprehensive, multi-dimensional evaluation frameworks. In this paper, we present SynLLM, a modular framework for generating high-quality synthetic medical tabular data using 20 state-of-the-art open-source LLMs, including LLaMA, Mistral, and GPT variants, guided by structured prompts. We propose four distinct prompt types, ranging from example-driven to rule-based constraints, that encode schema, metadata, and domain knowledge to control generation without model fine-tuning. Our framework features a comprehensive evaluation pipeline that rigorously assesses generated data across statistical fidelity, clinical consistency, and privacy preservation. We evaluate SynLLM across three public medical datasets, including Diabetes, Cirrhosis, and Stroke, using 20 open-source LLMs. Our results show that prompt engineering significantly impacts data quality and privacy risk, with rule-based prompts achieving the best privacy-quality balance. SynLLM establishes that, when guided by well-designed prompts and evaluated with robust, multi-metric criteria, LLMs can generate synthetic medical data that is both clinically plausible and privacy-aware, paving the way for safer and more effective data sharing in healthcare research.",
    "summary_cn": "SynLLM框架利用20种开源大语言模型，通过提示工程生成高质量医疗表格合成数据，在统计保真度、临床一致性和隐私保护方面表现优异。",
    "keywords": [
      "合成数据",
      "大语言模型",
      "提示工程",
      "医疗数据",
      "隐私保护",
      "评估框架"
    ],
    "triple": {
      "method": "提示工程与多模型框架",
      "result": "基于规则的提示实现最佳隐私-质量平衡",
      "contribution": "提供高质量、隐私安全的医疗合成数据生成方案"
    }
  },
  {
    "id": "b768494affc6",
    "venue": "NeurIPS",
    "year": 2025,
    "title": "MedPAIR: Measuring Physicians and AI Relevance Alignment in Medical Question Answering",
    "url": "https://neurips.cc/virtual/2025/133199",
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance on various medical question-answering (QA) benchmarks, including standardized medical exams. However, correct answers alone do not ensure correct logic, and models may reach accurate conclusions through flawed processes. In this study, we introduce the MedPAIR (Medical Dataset Comparing Physicians and AI Relevance Estimation and Question Answering) dataset to evaluate how physician trainees and LLMs prioritize relevant information when answering QA questions. We obtain annotations on 1,300 QA pairs from 36 physician trainees, labeling each sentence within the question components for relevance. We compare these relevance estimates to those for LLMs, and further evaluate the impact of these \"relevant\" subsets on downstream task performance for both physician trainees and LLMs. We find that LLMs are frequently not aligned with the content relevance estimates of physician trainees. After filtering out physician trainee-labeled irrelevant sentences, accuracy improves for both the trainees and the LLMs.",
    "summary_cn": "研究引入MedPAIR数据集，评估医生与LLMs在医学问答中的信息相关性判断差异。发现LLMs与医生相关性估计常不一致，过滤无关句子可提升双方准确率。",
    "keywords": [
      "医学问答",
      "大型语言模型",
      "相关性对齐",
      "数据集",
      "医生评估",
      "性能提升"
    ],
    "triple": {
      "method": "构建MedPAIR数据集并标注句子相关性",
      "result": "LLMs与医生相关性判断常不一致，过滤无关句子提高准确率",
      "contribution": "提出评估框架揭示LLMs逻辑缺陷，促进医疗AI可靠性"
    }
  },
  {
    "id": "ff229149087f",
    "venue": "NeurIPS",
    "year": 2025,
    "title": "Demo: Sanitizing Medical Documents with Differential Privacy using Large Language Models",
    "url": "https://neurips.cc/virtual/2025/124873",
    "abstract": "Medical documents often contain sensitive information such as disease history and symptoms. Regulations like GDPR strictly prohibit leakage of such content.  A natural solution is to sanitize documents with large language models (LLMs) before sending them to untrusted providers. However, LLM-based paraphrasing remains vulnerable to membership inference attacks (MIA), which can reveal what private tokens were present in the input. Differentially Private Inference (DPI) offers formal guarantees against such leakage, but standard approaches severely degrade utility. Recent methods improve trade-offs by applying DP only to private tokens, yet this requires accurate tagging of private spans. In practice, privacy in medical text is highly context dependent and varies across organizations/jurisdictions, leading existing taggers to perform poorly. LLM-based taggers achieve higher accuracy but require costly fine-tuning and risk leaking private data through memorization.  We address this by introducing constitutional classifiers for private information tagging. Here, we learn a constitution i.e a set of natural language rules, directly from a small annotated subset, achieving stronger performance than existing taggers while requiring no fine-tuning. Importantly, the learned rules remain interpretable and auditable, allowing human experts to verify or edit them for compliance. We integrate our constitutional tagger with DPI through DP-Fusion, yielding an end-to-end pipeline for utility-preserving medical document sanitization using LLMs. The system is deployed and publicly available at www.documentprivacy.com .",
    "summary_cn": "提出基于宪法分类器的隐私信息标注方法，结合差分隐私推理，实现医疗文档去敏感化，在保护隐私的同时保持实用性。",
    "keywords": [
      "差分隐私",
      "大语言模型",
      "医疗文档",
      "隐私保护",
      "宪法分类器",
      "去敏感化"
    ],
    "triple": {
      "method": "宪法分类器标注隐私信息并集成差分隐私推理",
      "result": "提升隐私保护效果且保持文档实用性",
      "contribution": "提供可审计的端到端医疗文档去敏感化方案"
    }
  },
  {
    "id": "068e61011c49",
    "venue": "NeurIPS",
    "year": 2025,
    "title": "Generative AI Enables Medical Image Segmentation in Ultra Low-Data Regimes",
    "url": "https://neurips.cc/virtual/2025/126033",
    "abstract": "Semantic segmentation of medical images is pivotal in applications like disease diagnosis and treatment planning. While deep learning automates this task effectively, it struggles in ultra low-data regimes for the scarcity of annotated segmentation masks. To address this, we propose a generative deep learning framework that produces high-quality image-mask pairs as auxiliary training data. Unlike traditional generative models that separate data generation from model training, ours uses multi-level optimization for end-to-end data generation. This allows segmentation performance to guide the generation process, producing data tailored to improve segmentation outcomes. Our method demonstrates strong generalization across 11 medical image segmentation tasks and 19 datasets, covering various diseases, organs, and modalities. It improves performance by 10–20% (absolute) in both same- and out-of-domain settings and requires 8–20 times less training data than existing approaches. This greatly enhances the feasibility and cost-effectiveness of deep learning in data-limited medical imaging scenarios.",
    "summary_cn": "提出生成式深度学习框架，通过端到端优化生成高质量图像-掩码对，显著提升超低数据场景下的医学图像分割性能，减少数据需求。",
    "keywords": [
      "生成式AI",
      "医学图像分割",
      "超低数据",
      "端到端优化",
      "数据增强",
      "泛化能力"
    ],
    "triple": {
      "method": "生成式深度学习框架",
      "result": "性能提升10-20%，数据需求减少8-20倍",
      "contribution": "增强数据受限场景下的分割可行性与成本效益"
    }
  },
  {
    "id": "c6cf0f7fdc81",
    "venue": "NeurIPS",
    "year": 2025,
    "title": "VQ-Seg: Vector-Quantized Token Perturbation for Semi-Supervised Medical Image Segmentation",
    "url": "https://neurips.cc/virtual/2025/poster/118930",
    "abstract": "Consistency learning with feature perturbation is a widely used strategy in semi-supervised medical image segmentation. However, many existing perturbation methods rely on dropout, and thus require a careful manual tuning of the dropout rate, which is a sensitive hyperparameter and often difficult to optimize and may lead to suboptimal regularization. To overcome this limitation, we propose VQ-Seg, the first approach to employ vector quantization (VQ) to discretize the feature space and introduce a novel and controllable Quantized Perturbation Module (QPM) that replaces dropout. Our QPM perturbs discrete representations by shuffling the spatial locations of codebook indices, enabling effective and controllable regularization. To mitigate potential information loss caused by quantization, we design a dual-branch architecture where the post-quantization feature space is shared by both image reconstruction and segmentation tasks. Moreover, we introduce a Post-VQ Feature Adapter (PFA) to incorporate guidance from a foundation model (FM), supplementing the high-level semantic information lost during quantization. Furthermore, we collect a large-scale Lung Cancer (LC) dataset comprising 828 CT scans annotated for central-type lung carcinoma. Extensive experiments on the LC dataset and other public benchmarks demonstrate the effectiveness of our method, which outperforms state-of-the-art approaches. Codes will be released.",
    "summary_cn": "VQ-Seg提出基于向量量化的特征扰动方法，用于半监督医学图像分割，通过量化扰动模块和双分支架构提升性能，在肺癌CT数据集上表现优异。",
    "keywords": [
      "半监督分割",
      "向量量化",
      "特征扰动",
      "医学图像",
      "肺癌CT",
      "一致性学习"
    ],
    "triple": {
      "method": "向量量化特征扰动与双分支架构",
      "result": "在肺癌等数据集上超越现有方法",
      "contribution": "提出可控扰动模块，减少超参数调优需求"
    }
  },
  {
    "id": "2a6b9c07fcca",
    "venue": "NeurIPS",
    "year": 2025,
    "title": "Examining the Vulnerability of Multi-Agent Medical Systems to Human Interventions for Clinical Reasoning",
    "url": "https://neurips.cc/virtual/2025/124860",
    "abstract": "Human interventions at fault points can alter the diagnostic accuracy of multi-agent medical systems. We defined fault points as moments in doctor-patient conversations, where the Doctor Agent's reasoning became most vulnerable to external influence and change. Using a MedQA dataset, this study analyzed simulated doctor-patient conversations to measure how fault point interventions shifted reasoning and accuracy. Correct intervention methods showed an improvement in baseline diagnostic accuracy by as much as 40\\%, while incorrect or bias-related interventions degraded performance by up to 6\\%, and increased diagnostic drift and uncertainty. Beyond accuracy, the analysis revealed behavioral patterns between cognitive biases in simulated Medical AI and real-world clinical practice. Examples included premature closure and susceptibility to misleading cues. Overall, these findings demonstrate that strategically priming large-language models (LLMs) at their fault points can substantially enhance LLM-driven diagnostic systems, improve reliability, and reveal where interventions may introduce drift or reinforce bias.",
    "summary_cn": "研究多智能体医疗系统在医患对话故障点受人为干预的影响，正确干预可提升诊断准确率40%，错误干预则降低6%并增加偏差。",
    "keywords": [
      "多智能体医疗系统",
      "故障点干预",
      "诊断准确率",
      "认知偏差",
      "大语言模型",
      "临床推理"
    ],
    "triple": {
      "method": "基于MedQA数据集模拟医患对话分析",
      "result": "正确干预提升准确率40%，错误干预降低6%并增加偏差",
      "contribution": "揭示干预对LLM诊断系统的影响，提升可靠性并识别偏差风险"
    }
  },
  {
    "id": "3f78e79f298b",
    "venue": "NeurIPS",
    "year": 2024,
    "title": "Deep Learning in Medical Image Registration: Magic or Mirage?",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2024/hash/c3fe2a07ec47b89c50e89706d2e23358-Abstract-Conference.html",
    "abstract": "Classical optimization and learning-based methods are the two reigning paradigms in deformable image registration. While optimization-based methods boast generalizability across modalities and robust performance, learning-based methods promise peak performance, incorporating weak supervision and amortized optimization. However, the exact conditions for either paradigm to perform well over the other are shrouded and not explicitly outlined in the existing literature. In this paper, we make an explicit correspondence between the mutual information of the distribution of per-pixel intensity and labels, and the performance of classical registration methods. This strong correlation hints to the fact that architectural designs in learning-based methods is unlikely to affect this correlation, and therefore, the performance of learning-based methods. This hypothesis is thoroughly validated with state-of-the-art classical and learning-based methods. However, learning-based methods with weak supervision can perform high-fidelity intensity and label registration, which is not possible with classical methods. Next, we show that this high-fidelity feature learning does not translate to invariance to domain shift, and learning-based methods are sensitive to such changes in the data distribution. We reassess and recalibrate performance expectations from classical and DLIR methods under access to label supervision, training time, and its generalization capabilities under minor domain shifts.",
    "summary_cn": "本文探讨深度学习在医学图像配准中的表现，发现其性能与像素强度-标签互信息相关，弱监督下可实现高保真配准，但对域偏移敏感。",
    "keywords": [
      "医学图像配准",
      "深度学习",
      "弱监督",
      "域偏移",
      "互信息",
      "性能评估"
    ],
    "triple": {
      "method": "分析互信息与性能关联，验证经典与学习方法",
      "result": "学习方法弱监督下高保真配准，但对域偏移敏感",
      "contribution": "重新校准经典与深度学习方法在监督、训练和泛化方面的性能预期"
    }
  },
  {
    "id": "39485a9be3c6",
    "venue": "NeurIPS",
    "year": 2025,
    "title": "Chiron-o1: Igniting Multimodal Large Language Models towards Generalizable Medical Reasoning via Mentor-Intern Collaborative Search",
    "url": "https://neurips.cc/virtual/2025/poster/116739",
    "abstract": "Multimodal large language models (MLLMs) have begun to demonstrate robust reasoning capabilities on general tasks, yet their application in the medical domain remains in its early stages. Constructing chain-of-thought (CoT) training data is essential for bolstering the reasoning abilities of medical MLLMs. However, existing approaches exhibit a deficiency in offering a comprehensive framework for searching and evaluating effective reasoning paths towards critical diagnosis. To address this challenge, we propose Mentor-Intern Collaborative Search (MICS), a novel reasoning-path searching scheme to generate rigorous and effective medical CoT data. MICS first leverages mentor models to initialize the reasoning, one step at a time, then prompts each intern model to continue the thinking along those initiated paths, and finally selects the optimal reasoning path according to the overall reasoning performance of multiple intern models. The reasoning performance is determined by an MICS-Score, which assesses the quality of generated reasoning paths. Eventually, we construct MMRP, a multi-task medical reasoning dataset with ranked difficulty, and Chiron-o1,  a new medical MLLM devised via a curriculum learning strategy, with robust visual question-answering and generalizable reasoning capabilities. Extensive experiments demonstrate that Chiron-o1, trained on our CoT dataset constructed using MICS, achieves state-of-the-art performance across a list of medical visual question answering and reasoning benchmarks. Codes are available at https://github.com/Yankai96/Chiron-o1",
    "summary_cn": "提出MICS方法生成医学推理链数据，构建MMRP数据集和Chiron-o1模型，在医学视觉问答基准上实现最优性能。",
    "keywords": [
      "多模态大语言模型",
      "医学推理",
      "思维链",
      "导师-实习生协作搜索",
      "课程学习",
      "视觉问答"
    ],
    "triple": {
      "method": "MICS方法生成推理链数据",
      "result": "Chiron-o1在医学基准上达到最优",
      "contribution": "提升医学MLLM的泛化推理能力"
    }
  },
  {
    "id": "007640578756",
    "venue": "NeurIPS",
    "year": 2025,
    "title": "Beyond Distillation: Pushing the Limits of Medical LLM Reasoning with Minimalist Rule-Based RL",
    "url": "https://neurips.cc/virtual/2025/124866",
    "abstract": "Improving performance on complex tasks and enabling interpretable decision making in large language models (LLMs), especially for clinical applications, requires effective reasoning. Yet this remains challenging without supervised fine-tuning (SFT) on costly chain-of-thought (CoT) data distilled from closed-source models (e.g., GPT-4o). In this work, we present AlphaMed, the first medical LLM to show that reasoning capability can emerge purely through reinforcement learning (RL), using minimalist rule-based rewards on public multiple-choice QA datasets, without relying on SFT or distilled CoT data. AlphaMed achieves state-of-the-art results on six medical QA benchmarks, outperforming models trained with conventional SFT+RL pipelines. On challenging benchmarks (e.g., MedXpert), AlphaMed even surpasses larger or closed-source models such as DeepSeek-V3-671B and Claude-3.5-Sonnet. To understand the factors behind this success, we conduct a comprehensive data-centric analysis guided by three questions:(i) Can minimalist rule-based RL incentivize reasoning without distilled CoT supervision? (ii) How do dataset quantity and diversity impact reasoning? (iii) How does question difficulty shape the emergence and generalization of reasoning? Our findings show that dataset informativeness is a key driver of reasoning performance, and that minimalist RL on informative, multiple-choice QA data is effective at inducing reasoning without CoT supervision. We also observe divergent trends across benchmarks, underscoring limitations in current evaluation and the need for more challenging, reasoning-oriented medical QA benchmarks. The code and pretrained model weights will be publicly released upon acceptance.",
    "summary_cn": "AlphaMed 模型仅通过基于规则的强化学习，无需监督微调或思维链数据，在多项医疗问答基准测试中取得最优性能。",
    "keywords": [
      "医疗大语言模型",
      "强化学习",
      "推理能力",
      "规则奖励",
      "问答基准",
      "数据驱动分析"
    ],
    "triple": {
      "method": "基于规则的强化学习",
      "result": "在六个医疗QA基准上达到最优",
      "contribution": "首次证明无需监督微调即可激发医疗LLM推理能力"
    }
  },
  {
    "id": "a8c3c30afce1",
    "venue": "NeurIPS",
    "year": 2025,
    "title": "MedBrowseComp: Benchmarking Medical Deep Research and Computer Use",
    "url": "https://neurips.cc/virtual/2025/122474",
    "abstract": "Large language models (LLMs) are increasingly envisioned as decision-support tools in clinical practice, yet safe clinical reasoning demands the integration of heterogeneous knowledge bases—trials, primary studies, regulatory documents, and cost data—under strict accuracy constraints. Existing evaluations typically rely on synthetic prompts, reduce the task to single-hop factoid queries, or conflate reasoning with open-ended text generation, leaving their real-world utility unclear. To close this gap, we present \\textbf{MedBrowseComp}, the first benchmark that systematically tests an agent’s ability to reliably retrieve and synthesize multi-hop medical facts from up-to-date, domain-specific knowledge bases. MedBrowseComp holds 1,000+ human-curated questions that mirror clinical scenarios in which practitioners must reconcile information fragmented over many sources that are potentially conflicting. Applying MedBrowseComp to frontier agentic systems reveals \\textbf{marked performance shortfalls as low as 10\\%}. MedBrowseComp reveals critical gaps between current LLM performance and clinical usage, providing a testbed to guide future model and toolchain improvements for reliable medical information seeking.",
    "summary_cn": "提出MedBrowseComp基准，评估LLM在临床多源信息检索与合成中的能力，发现现有系统性能显著不足，仅达10%。",
    "keywords": [
      "医学基准",
      "大语言模型",
      "临床决策支持",
      "多跳推理",
      "信息检索",
      "性能评估"
    ],
    "triple": {
      "method": "构建人类标注的临床问题基准",
      "result": "前沿系统性能低至10%",
      "contribution": "揭示LLM与临床应用的差距，指导模型改进"
    }
  },
  {
    "id": "7ea16da2d2f5",
    "venue": "NeurIPS",
    "year": 2025,
    "title": "Collaborative Feature and Persona Enhancement for Trustworthy Medical Foundation Models",
    "url": "https://neurips.cc/virtual/2025/133194",
    "abstract": "Foundation models promise to democratize access to high-quality medical image segmentation, but they can still exhibit patient-dependent performance differences. We explore whether conditioning a compact U-shaped backbone on simple demographic metadata can improve worst-group segmentation without hurting overall accuracy or incurring significant computational overhead. We propose CEIGM-UNet, a compact U-shaped backbone that interleaves collaborative feature enhancement layers (CFEL) and Group Mamba blocks and integrates a Conditional Feature Recalibration (CFR) module that maps a low-dimensional metadata vector to FiLM-like channel-wise scale and shift parameters. Because public benchmarks rarely provide reliable age or sex labels, we simulate demographic variability on Synapse by grouping cases into small, medium, and large organ-volume subgroups and evaluate fairness using equal opportunity differences and generalized Dice disparities across these volume-defined cohorts. On Synapse and ACDC, our metadata-conditioned CEIGM-UNet achieves competitive Dice and HD95 compared with recent CNN-, Transformer-, and Mamba-based U-Nets, while modestly narrowing performance gaps between volume-based subgroups with negligible overhead. We discuss limitations of this proxy setup and outline how such metadata conditioning could be integrated into larger medical foundation models in a principled, fairness-aware way.",
    "summary_cn": "提出CEIGM-UNet模型，通过整合人口统计元数据，在保持整体精度的同时，缩小了医学图像分割中基于器官体积分组的性能差距，提升了公平性。",
    "keywords": [
      "医学图像分割",
      "公平性",
      "元数据条件化",
      "CEIGM-UNet",
      "性能差距",
      "U型网络"
    ],
    "triple": {
      "method": "使用元数据条件化与CFR模块增强紧凑U型网络",
      "result": "在Synapse和ACDC上竞争性精度，缩小体积分组性能差距",
      "contribution": "为医学基础模型提供公平感知的元数据集成方法"
    }
  },
  {
    "id": "8ff6c913f193",
    "venue": "NeurIPS",
    "year": 2025,
    "title": "RPRO: Ranked Preference Reinforcement Optimization for Enhancing  Medical QA and Diagnostic Reasoning",
    "url": "https://neurips.cc/virtual/2025/124920",
    "abstract": "Medical question answering requires advanced reasoning that integrates domain knowledge with logical inference. However, existing large language models (LLMs) often generate reasoning chains that lack factual accuracy and clinical reliability. We propose Ranked Preference Reinforcement Optimization (RPRO), a novel framework that uniquely combines reinforcement learning with preference-driven reasoning refinement to enhance clinical chain-of-thought (CoT) performance. RPRO differentiates itself from prior approaches by employing task-adaptive reasoning templates and a probabilistic evaluation mechanism that aligns outputs with established clinical workflows, while automatically identifying and correcting low-quality reasoning chains. Unlike traditional pairwise preference methods, RPRO introduces a groupwise ranking optimization based on the Bradley–Terry model and incorporates KL-divergence regularization for stable training.  Experiments on PubMedQA and MedQA-USMLE show consistent improvements over strong baselines. Remarkably, our 1.1B parameter model outperforms much larger 7B–13B models, including medical-specialized variants. These findings demonstrate that combining preference optimization with quality-driven refinement offers a scalable and effective approach to building more reliable, clinically grounded medical LLMs.",
    "summary_cn": "提出RPRO框架，结合强化学习与偏好优化，提升医学问答的推理准确性和临床可靠性，在PubMedQA和MedQA-USMLE上优于更大模型。",
    "keywords": [
      "医学问答",
      "强化学习",
      "偏好优化",
      "推理链",
      "临床可靠性",
      "Bradley-Terry模型"
    ],
    "triple": {
      "method": "RPRO框架结合强化学习与偏好优化",
      "result": "在PubMedQA和MedQA-USMLE上表现优于更大模型",
      "contribution": "提升医学问答的临床可靠性和推理准确性"
    }
  },
  {
    "id": "ea5f83947195",
    "venue": "NeurIPS",
    "year": 2025,
    "title": "MedBrowseComp: Benchmarking Medical Deep Research and Computer Use",
    "url": "https://neurips.cc/virtual/2025/124867",
    "abstract": "Large language models (LLMs) are increasingly envisioned as decision-support tools in clinical practice, yet safe clinical reasoning demands the integration of heterogeneous knowledge bases—trials, primary studies, regulatory documents, and cost data—under strict accuracy constraints. Existing evaluations typically rely on synthetic prompts, reduce the task to single-hop factoid queries, or conflate reasoning with open-ended text generation, leaving their real-world utility unclear. To close this gap, we present \\textbf{MedBrowseComp}, the first benchmark that systematically tests an agent’s ability to reliably retrieve and synthesize multi-hop medical facts from up-to-date, domain-specific knowledge bases. MedBrowseComp holds 1,000+ human-curated questions that mirror clinical scenarios in which practitioners must reconcile information fragmented over many sources that are potentially conflicting. Applying MedBrowseComp to frontier agentic systems reveals \\textbf{marked performance shortfalls as low as 10%}. MedBrowseComp reveals critical gaps between current LLM performance and clinical usage, providing a testbed to guide future model and toolchain improvements for reliable medical information seeking.",
    "summary_cn": "MedBrowseComp是首个评估智能体从多源医学知识库中可靠检索与综合多跳事实能力的基准，包含1000+人工问题，揭示前沿系统性能低至10%，突显LLM与临床应用的差距。",
    "keywords": [
      "医学基准",
      "多跳推理",
      "知识检索",
      "临床决策支持",
      "大语言模型评估",
      "智能体系统"
    ],
    "triple": {
      "method": "构建MedBrowseComp基准测试",
      "result": "前沿系统性能低至10%",
      "contribution": "揭示LLM与临床应用的差距，指导模型改进"
    }
  },
  {
    "id": "32de0146d6f4",
    "venue": "NeurIPS",
    "year": 2024,
    "title": "SM3-Text-to-Query: Synthetic Multi-Model Medical Text-to-Query Benchmark",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2024/hash/a182a8e6ebc91728b6e6b6382c9f7b1e-Abstract-Datasets_and_Benchmarks_Track.html",
    "abstract": "Electronic health records (EHRs) are stored in various database systems with different database models on heterogeneous storage architectures, such as relational databases, document stores, or graph databases. These different database models have a big impact on query complexity and performance. While this has been a known fact in database research, its implications for the growing number of Text-to-Query systems have surprisingly not been investigated so far.In this paper, we present SM3-Text-to-Query, the first multi-model medical Text-to-Query benchmark based on synthetic patient data from Synthea, following the SNOMED-CT taxonomy---a widely used knowledge graph ontology covering medical terminology. SM3-Text-to-Query provides data representations for relational databases (PostgreSQL), document stores (MongoDB), and graph databases (Neo4j and GraphDB (RDF)), allowing the evaluation across four popular query languages, namely SQL, MQL, Cypher, and SPARQL.We systematically and manually develop 408 template questions, which we augment to construct a benchmark of 10K diverse natural language question/query pairs for these four query languages (40K pairs overall). On our dataset, we evaluate several common in-context-learning (ICL) approaches for a set of representative closed and open-source LLMs.Our evaluation sheds light on the trade-offs between database models and query languages for different ICL strategies and LLMs. Last,SM3-Text-to-Query is easily extendable to additional query languages or real, standard-based patient databases.",
    "summary_cn": "SM3-Text-to-Query是首个基于合成患者数据的多模型医疗文本转查询基准，支持四种查询语言，评估了不同LLM在ICL策略下的表现。",
    "keywords": [
      "文本转查询",
      "多模型数据库",
      "医疗基准",
      "合成数据",
      "查询语言",
      "上下文学习"
    ],
    "triple": {
      "method": "基于Synthea合成数据和SNOMED-CT构建多模型基准，手动开发模板问题并扩展至10K对",
      "result": "评估了多种LLM的ICL方法，揭示了数据库模型与查询语言在不同策略下的权衡",
      "contribution": "提供首个可扩展的多模型医疗文本转查询基准，促进跨数据库系统的性能研究"
    }
  },
  {
    "id": "001d80ca1991",
    "venue": "NeurIPS",
    "year": 2024,
    "title": "Copycats: the many lives of a publicly available medical imaging dataset",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2024/hash/cdbeaeb8a0313940a5752c4ec8838ca6-Abstract-Datasets_and_Benchmarks_Track.html",
    "abstract": "Medical Imaging (MI) datasets are fundamental to artificial intelligence in healthcare. The accuracy, robustness, and fairness of diagnostic algorithms depend on the data (and its quality) used to train and evaluate the models. MI datasets used to be proprietary, but have become increasingly available to the public, including on community-contributed platforms (CCPs) like Kaggle or HuggingFace. While open data is important to enhance the redistribution of data's public value, we find that the current CCP governance model fails to uphold the quality needed and recommended practices for sharing, documenting, and evaluating datasets. In this paper, we conduct an analysis of publicly available machine learning datasets on CCPs, discussing datasets' context, and identifying limitations and gaps in the current CCP landscape. We highlight differences between MI and computer vision datasets, particularly in the potentially harmful downstream effects from poor adoption of recommended dataset management practices. We compare the analyzed datasets across several dimensions, including data sharing, data documentation, and maintenance. We find vague licenses, lack of persistent identifiers and storage, duplicates, and missing metadata, with differences between the platforms. Our research contributes to efforts in responsible data curation and AI algorithms for healthcare.",
    "summary_cn": "分析社区贡献平台上的公开医学影像数据集，发现其治理模式存在数据质量、文档和维护问题，影响AI算法的准确性和公平性。",
    "keywords": [
      "医学影像数据集",
      "社区贡献平台",
      "数据治理",
      "数据质量",
      "人工智能",
      "医疗保健"
    ],
    "triple": {
      "method": "分析公开数据集",
      "result": "发现许可证模糊、元数据缺失等问题",
      "contribution": "促进医疗数据负责任管理"
    }
  },
  {
    "id": "401dc457997e",
    "venue": "NeurIPS",
    "year": 2025,
    "title": "RAxSS: Retrieval-Augmented Sparse Sampling for Explainable Variable-Length Medical Time Series Classification",
    "url": "https://neurips.cc/virtual/2025/132323",
    "abstract": "Medical time series analysis is challenging due to data sparsity, noise, and highly variable recording lengths. Prior work has shown that stochastic sparse sampling effectively handles variable-length signals, while retrieval-augmented approaches improve explainability and robustness to noise and weak temporal correlations. In this study, we generalize the stochastic sparse sampling framework for retrieval-informed classification. Specifically, we weight window predictions by within-channel similarity and aggregate them in probability space, yielding convex series-level scores and an explicit evidence trail for explainability. Our method achieves competitive iEEG classification performance and provides practitioners with greater transparency and explainability. We evaluate our method in iEEG recordings collected in four medical centers, demonstrating its potential for reliable and explainable clinical variable-length time series classification.",
    "summary_cn": "本研究提出RAxSS方法，结合检索增强与稀疏采样，用于可变长度医疗时间序列分类，提升可解释性与鲁棒性，在iEEG数据上验证有效。",
    "keywords": [
      "检索增强",
      "稀疏采样",
      "医疗时间序列",
      "可解释性",
      "可变长度",
      "iEEG分类"
    ],
    "triple": {
      "method": "检索增强稀疏采样框架",
      "result": "在iEEG分类中表现竞争性，提供透明证据追踪",
      "contribution": "实现可靠且可解释的临床时间序列分类"
    }
  },
  {
    "id": "b44763d0cd66",
    "venue": "NeurIPS",
    "year": 2025,
    "title": "The Biased Oracle: Assessing LLMs’ Understandability and Empathy in Medical Diagnoses",
    "url": "https://neurips.cc/virtual/2025/124876",
    "abstract": "Large language models (LLMs) show promise for supporting diagnostic communication by generating explanations and guidance for patients. Yet their ability to produce outputs that are both understandable and empathetic remains uncertain. We assess two leading LLMs on medical diagnostic scenarios, measuring understandability with readability metrics and empathy through LLM-as-a-Judge compared to human ratings. Our results indicate that LLMs adapt explanations to sociodemographic variables and patient conditions. However, they also generate overly complex content and display biased affective empathy, leading to uneven accessibility and support. These patterns underscore the need for systematic calibration to ensure equitable patient communication.",
    "summary_cn": "评估大语言模型在医疗诊断中的可理解性与共情能力，发现其能适应患者背景但存在内容复杂和情感偏见，需系统校准以确保公平沟通。",
    "keywords": [
      "大语言模型",
      "医疗诊断",
      "可理解性",
      "共情能力",
      "偏见",
      "患者沟通"
    ],
    "triple": {
      "method": "使用可读性指标和LLM-as-a-Judge评估",
      "result": "模型适应患者背景但内容复杂且情感偏见",
      "contribution": "揭示模型偏见，强调系统校准的必要性"
    }
  },
  {
    "id": "d38e80c784c2",
    "venue": "NeurIPS",
    "year": 2025,
    "title": "Large Language Models as Medical Codes Selectors: a benchmark using the International Classification of Primary Care",
    "url": "https://neurips.cc/virtual/2025/124915",
    "abstract": "Background: Medical coding structures healthcare data for research, quality monitoring, and policy. This study assesses the potential of large language models (LLMs) to assign ICPC-2 codes using the output of a domain-specific search engine.Methods: A dataset of 437 Brazilian Portuguese clinical expressions, each annotated with ICPC-2 codes, was used. A semantic search engine (OpenAI's text-embedding-3-large) retrieved candidates from 73,563 labeled concepts. Thirty-three LLMs were prompted with each query and retrieved results to select the best-matching ICPC-2 code. Performance was evaluated using F1-score, along with token usage, cost, response time, and format adherence. Results: Twenty-eight models achieved F1-score > 0.8; ten exceeded 0.85. Top performers included gpt-4.5-preview, o3, and gemini-2.5-pro. Retriever optimization can improve performance by up to 4 points. Most models returned valid codes in the expected format, with reduced hallucinations. Smaller models (<3B) struggled with formatting and input length.Conclusions: LLMs show strong potential for automating ICPC-2 coding, even without fine-tuning. This work offers a benchmark and highlights challenges, but findings are limited by dataset scope and setup. Broader, multilingual, end-to-end evaluations are needed for clinical validation.",
    "summary_cn": "研究评估33个大语言模型自动分配ICPC-2医疗编码的能力，多数模型F1分数>0.8，显示无需微调即可有效辅助编码，但小模型存在格式和输入长度限制。",
    "keywords": [
      "大语言模型",
      "医疗编码",
      "ICPC-2",
      "语义搜索",
      "基准测试",
      "自动化"
    ],
    "triple": {
      "method": "使用语义搜索检索候选编码，提示LLMs选择最佳匹配",
      "result": "28个模型F1>0.8，优化检索可提升4%性能",
      "contribution": "建立LLMs医疗编码选择基准，证明其自动化潜力"
    }
  },
  {
    "id": "61c8f747a040",
    "venue": "NeurIPS",
    "year": 2025,
    "title": "Temporal Gaze Dynamics as Zero-Shot Prompts for Volumetric Medical Segmentation",
    "url": "https://neurips.cc/virtual/2025/132346",
    "abstract": "Guiding foundation models like SAM-2 for volumetric medical segmentation typically relies on inefficient manual prompts. We introduce a more efficient, multimodal approach using eye gaze—a continuous physiological time series—to steer the model's focus in a zero-shot manner. By fusing a user's temporal gaze stream with spatial image data, we enable dynamic, interactive 3D segmentation. Evaluating with SAM-2 and its medical variant, MedSAM-2, our gaze-based method proves significantly more time-efficient (e.g., 62 vs. 88 seconds per volume) than manual bounding boxes, with a modest accuracy trade-off. This work establishes a practical framework for incorporating human physiological signals into sequential, human-in-the-loop clinical tasks, paving the way for more intuitive AI interfaces.",
    "summary_cn": "提出利用眼动时序数据作为零样本提示，引导SAM-2等基础模型进行三维医学图像分割，实现高效动态交互，在时间效率上优于手动标注。",
    "keywords": [
      "眼动追踪",
      "零样本提示",
      "三维医学分割",
      "SAM-2",
      "人机交互",
      "多模态融合"
    ],
    "triple": {
      "method": "融合眼动时序与图像空间数据",
      "result": "分割时间显著缩短，精度略有下降",
      "contribution": "建立生理信号引导的交互式临床任务框架"
    }
  },
  {
    "id": "d54d831cbad3",
    "venue": "NeurIPS",
    "year": 2024,
    "title": "Hetero-UNet: Heterogeneous Transformer with Mamba for Medical Image Segmentation",
    "url": "https://neurips.cc/virtual/2024/103876",
    "abstract": "Convolutional Neural Networks (CNNs) have significantly advanced medical image segmentation, offering unparalleled local feature extraction capabilities. However, CNNs face limitations in capturing long-range dependencies due to the local nature of convolutional operations. Recently, State-Space Models (SSMs), such as Mamba, have presented an efficient solution by incorporating gating, convolutions, and data-dependent filtering mechanisms for long-range interaction modeling. However, as an attention-free mechanism, SSMs are less efficient at handling variable distance token-to-token interactions compared to attention. In this paper, we introduce Hetero-UNet, a novel hybrid U-Net architecture that incorporates SSMs and attention mechanisms to map long-range dependencies. Featuring a hybrid Transformer-Mamba encoder within original U-Net architecture, it excels at extracting both local and global features. Our extensive experiments across diverse tasks—abdominal organ segmentation in CT and MR, instrument segmentation in endoscopy, and cell segmentation in microscopy—demonstrates Hetero-UNet's superior performance over previous state-of-the-art segmentation models, paving the way for hybrid long-range dependency modeling in medical imaging. The code is available at https://github.com/ZhilingYan/Hetero-UNet.",
    "summary_cn": "提出Hetero-UNet，结合Transformer与Mamba，在U-Net中融合注意力与状态空间模型，有效建模长程依赖，提升医学图像分割性能。",
    "keywords": [
      "医学图像分割",
      "Transformer",
      "Mamba",
      "长程依赖",
      "混合架构",
      "U-Net"
    ],
    "triple": {
      "method": "融合Transformer与Mamba的混合U-Net编码器",
      "result": "在多种医学图像分割任务中超越现有最佳模型",
      "contribution": "为医学成像中的混合长程依赖建模开辟新途径"
    }
  },
  {
    "id": "e6d7a0a4def1",
    "venue": "NeurIPS",
    "year": 2025,
    "title": "SynthFair: A Semi-Synthetic Medical Imaging Dataset to Propel Research on Bias Detection & Mitigation",
    "url": "https://neurips.cc/virtual/2025/125803",
    "abstract": "The scarcity of large-scale datasets capable of capturing the rich diversity of the global population is currently a major limitation for the development of equitable AI tools in the medical domain. Underrepresentation of certain subpopulations renders the evaluation of bias audits and subsequent mitigation difficult in general, and practically unfeasible when it comes to intersectional studies. Moreover, spurious correlations in these datasets, which are challenging to identify, have a tendency to result in shortcut learning whereby models base their decisions on features unrelated to the task, which may lead to catastrophic failure at test time. Current fairness benchmarks are not representative of real-world data, making it difficult to draw conclusions that are relevant for clinical practice. SynthFair aims to bridge this gap by leveraging cutting-edge technology in generative AI. The use of GenAI will allow us to create a massive semi-synthetic dataset of chest x-ray images, augmenting a rich international collection of databases by means of counterfactual image generation. SynthFair is the result of an international collaboration with a proven track record in synthetic image creation, database curation, as well as in bias detection and mitigation in the context of medical imaging.",
    "summary_cn": "SynthFair利用生成AI创建大规模半合成胸部X光数据集，以解决医学AI中数据多样性和偏见检测的挑战。",
    "keywords": [
      "半合成数据集",
      "偏见检测",
      "生成AI",
      "医学影像",
      "公平性",
      "胸部X光"
    ],
    "triple": {
      "method": "生成AI与反事实图像生成",
      "result": "创建大规模半合成胸部X光数据集",
      "contribution": "推动医学AI偏见检测与缓解研究"
    }
  },
  {
    "id": "225053c005b7",
    "venue": "NeurIPS",
    "year": 2025,
    "title": "Examining the Vulnerability of Multi-Agent Medical Systems to Human Interventions for Clinical Reasoning",
    "url": "https://neurips.cc/virtual/2025/130246",
    "abstract": "Human interventions at fault points can alter the diagnostic accuracy of multi-agent medical systems. We defined fault points as moments in doctor-patient conversations, where the Doctor Agent's reasoning became most vulnerable to external influence and change. Using a MedQA dataset, this study analyzed simulated doctor-patient conversations to measure how fault point interventions shifted reasoning and accuracy. Correct intervention methods showed an improvement in baseline diagnostic accuracy up to 40\\%, while incorrect or bias-related interventions degraded performance by up to 6\\%, and increased diagnostic drift and uncertainty. Beyond accuracy, the analysis revealed behavioral patterns between cognitive biases in simulated Medical AI and real-world clinical practice. Examples included premature closure and susceptibility to misleading cues, which are concerning in healthcare, where reliability and fairness are critical. This makes fault points natural audit checkpoints for oversight or human verification. Overall, the findings reveal that priming large language models (LLMs) at fault points can improve reliability, expose drift and bias, and support stress-testing for certification.",
    "summary_cn": "研究多智能体医疗系统在医患对话故障点受人为干预的影响，正确干预可提升诊断准确率40%，错误干预则降低6%，并揭示认知偏差模式。",
    "keywords": [
      "多智能体医疗系统",
      "故障点干预",
      "诊断准确率",
      "认知偏差",
      "大语言模型",
      "临床推理"
    ],
    "triple": {
      "method": "基于MedQA数据集模拟医患对话分析",
      "result": "正确干预提升准确率40%，错误干预降低6%并增加偏差",
      "contribution": "揭示故障点作为审计检查点，支持系统可靠性测试与认证"
    }
  },
  {
    "id": "246eef41b314",
    "venue": "NeurIPS",
    "year": 2025,
    "title": "MIRA: Medical Time Series Foundation Model for Real-World Health Data",
    "url": "https://neurips.cc/virtual/2025/poster/119424",
    "abstract": "A unified foundation model for medical time series—pretrained on open access and ethically reviewed medical corpora—offers the potential to reduce annotation burdens, minimize model customization, and enable robust transfer across clinical institutions, modalities, and tasks, particularly in data-scarce or privacy-constrained environments.  However, existing time series foundation models struggle to handle medical time series data due to its inherent challenges, including irregular intervals, heterogeneous sampling rates, and frequent missingness. To address these challenges, we introduce MIRA, a unified foundation model specifically designed for medical time series forecasting. MIRA incorporates a Continuous-Time Rotary Positional Encoding that enables fine-grained modeling of variable time intervals, a frequency-specific mixture-of-experts layer that routes computation across latent frequency regimes to further promote temporal specialization, and a Continuous Dynamics Extrapolation Block based on Neural ODE that models the continuous trajectory of latent states, enabling accurate forecasting at arbitrary target timestamps. Pretrained on a large-scale and diverse medical corpus comprising over 454 billion time points collect from publicly available datasets, MIRA achieving reductions in forecasting errors by an average of 8% and 6% in out-of-distribution and in-distribution scenarios, respectively. We also introduce a comprehensive benchmark spanning multiple downstream clinical tasks, establishing a foundation for future research in medical time series modeling.",
    "summary_cn": "MIRA是专为医疗时间序列设计的统一基础模型，通过连续时间编码和专家层处理不规则数据，在预测任务中平均降低误差6-8%。",
    "keywords": [
      "医疗时间序列",
      "基础模型",
      "连续时间建模",
      "预测误差降低",
      "不规则数据",
      "神经ODE"
    ],
    "triple": {
      "method": "连续时间旋转位置编码与专家层",
      "result": "预测误差平均降低6-8%",
      "contribution": "提出首个专用于医疗时间序列的统一基础模型"
    }
  },
  {
    "id": "e9e959147565",
    "venue": "NeurIPS",
    "year": 2025,
    "title": "Magical: Medical Lay Language Generation via Semantic Invariance and Layperson-tailored Adaptation",
    "url": "https://neurips.cc/virtual/2025/poster/116222",
    "abstract": "Medical Lay Language Generation (MLLG) plays a vital role in improving the accessibility of complex scientific content for broader audiences. Recent literature to MLLG commonly employ parameter-efficient fine-tuning methods such as Low-Rank Adaptation (LoRA) to fine-tuning large language models (LLMs) using paired expert-lay language datasets. However, LoRA struggles with the challenges posed by multi-source heterogeneous MLLG datasets. Specifically, through a series of exploratory experiments, we reveal that standard LoRA fail to meet the requirement for semantic fidelity and diverse lay-style generation in MLLG task. To address these limitations, we propose Magical, an asymmetric LoRA architecture tailored for MLLG under heterogeneous data scenarios. Magical employs a shared matrix A for abstractive summarization, along with multiple isolated matrices B for diverse lay-style generation. To preserve semantic fidelity during the lay language generation process, Magical introduces a Semantic Invariance Constraint to mitigate semantic subspace shifts on matrix A. Furthermore, to better adapt to diverse lay-style generation, Magical incorporates the Recommendation-guided Switch, an externally interface to prompt the LLM to switch between different matrices B. Experimental results on three real-world lay language generation datasets demonstrate that Magical consistently outperforms prompt-based methods, vanilla LoRA, and its recent variants, while also reducing trainable parameters by 31.66%. Our code is publicly available at https://github.com/tianlwang/Magical.git.",
    "summary_cn": "Magical提出非对称LoRA架构，通过语义不变约束和推荐引导切换，提升医学通俗语言生成的语义保真度和风格多样性，减少可训练参数。",
    "keywords": [
      "医学通俗语言生成",
      "非对称LoRA",
      "语义不变约束",
      "推荐引导切换",
      "参数高效微调",
      "异构数据"
    ],
    "triple": {
      "method": "非对称LoRA架构",
      "result": "性能优于现有方法，参数减少31.66%",
      "contribution": "提升语义保真与风格多样性"
    }
  },
  {
    "id": "f932cf9ddd38",
    "venue": "NeurIPS",
    "year": 2024,
    "title": "BenchX: A Unified Benchmark Framework for Medical Vision-Language Pretraining on Chest X-Rays",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2024/hash/0cb35e10bf7bb73d10c12414edbd63fd-Abstract-Datasets_and_Benchmarks_Track.html",
    "abstract": "Medical Vision-Language Pretraining (MedVLP) shows promise in learning generalizable and transferable visual representations from paired and unpaired medical images and reports. MedVLP can provide useful features to downstream tasks and facilitate adapting task-specific models to new setups using fewer examples. However, existing MedVLP methods often differ in terms of datasets, preprocessing, and finetuning implementations. This pose great challenges in evaluating how well a MedVLP method generalizes to various clinically-relevant tasks due to the lack of unified, standardized, and comprehensive benchmark. To fill this gap, we propose BenchX, a unified benchmark framework that enables head-to-head comparison and systematical analysis between MedVLP methods using public chest X-ray datasets. Specifically, BenchX is composed of three components: 1) Comprehensive datasets covering nine datasets and four medical tasks; 2) Benchmark suites to standardize data preprocessing, train-test splits, and parameter selection; 3) Unified finetuning protocols that accommodate heterogeneous MedVLP methods for consistent task adaptation in classification, segmentation, and report generation, respectively. Utilizing BenchX, we establish baselines for nine state-of-the-art MedVLP methods and found that the performance of some early MedVLP methods can be enhanced to surpass more recent ones, prompting a revisiting of the developments and conclusions from prior works in MedVLP. Our code are available at https://github.com/yangzhou12/BenchX.",
    "summary_cn": "提出BenchX统一基准框架，用于标准化评估胸部X光医学视觉-语言预训练方法，涵盖多数据集与任务，促进公平比较与性能提升。",
    "keywords": [
      "医学视觉-语言预训练",
      "胸部X光",
      "基准框架",
      "标准化评估",
      "多任务学习",
      "数据集整合"
    ],
    "triple": {
      "method": "构建统一基准框架BenchX",
      "result": "提升早期方法性能，超越新方法",
      "contribution": "填补MedVLP标准化评估空白"
    }
  },
  {
    "id": "7e4d422ecd27",
    "venue": "NeurIPS",
    "year": 2025,
    "title": "EWC-Guided Diffusion Replay for Exemplar-Free Continual Learning in Medical Imaging",
    "url": "https://neurips.cc/virtual/2025/123872",
    "abstract": "Medical imaging foundation models must adapt continually, but retraining is limited by privacy and cost. We propose an exemplar-free framework combining class-conditional diffusion replay (DDPMs) with synaptic stability from Elastic Weight Consolidation (EWC). A compact Vision Transformer backbone is evaluated across eight MedMNIST v2 tasks and CheXpert. Our method attains 0.851 AUROC on CheXpert, cuts forgetting by over 30\\% versus DER++, and approaches joint training (0.869), while preserving privacy and efficiency. Analysis links forgetting to replay fidelity and parameter stability, underscoring the complementary roles of DDPM and EWC. This establishes a scalable, privacy-preserving route for continual FM adaptation.",
    "summary_cn": "提出结合扩散重放与弹性权重巩固的免示例持续学习框架，在医学影像任务中减少遗忘超30%，接近联合训练性能，保护隐私与效率。",
    "keywords": [
      "持续学习",
      "扩散模型",
      "医学影像",
      "隐私保护",
      "弹性权重巩固",
      "免示例学习"
    ],
    "triple": {
      "method": "扩散重放与弹性权重巩固结合",
      "result": "减少遗忘超30%，接近联合训练性能",
      "contribution": "建立可扩展、隐私保护的持续适应路径"
    }
  },
  {
    "id": "320086c8f32a",
    "venue": "NeurIPS",
    "year": 2025,
    "title": "Towards Prospective Medical Image Reconstruction via Knowledge-Informed Dynamic Optimal Transport",
    "url": "https://neurips.cc/virtual/2025/poster/115475",
    "abstract": "Medical image reconstruction from measurement data is a vital but challenging inverse problem. Deep learning approaches have achieved promising results, but often requires paired measurement and high-quality images, which is typically simulated through a forward model, i.e., retrospective reconstruction. However, training on simulated pairs commonly leads to performance degradation on real prospective data due to the retrospective-to-prospective gap caused by incomplete imaging knowledge in simulation. To address this challenge, this paper introduces imaging Knowledge-Informed Dynamic Optimal Transport (KIDOT), a novel dynamic optimal transport framework with optimality in the sense of preserving consistency with imaging physics in transport, that conceptualizes reconstruction as finding a dynamic transport path. KIDOT learns from unpaired data by modeling reconstruction as a continuous evolution path from measurements to images, guided by an imaging knowledge-informed cost function and transport equation. This dynamic and knowledge-aware approach enhances robustness and better leverages unpaired data while respecting acquisition physics. Theoretically, we demonstrate that KIDOT naturally generalizes dynamic optimal transport, ensuring its mathematical rationale and solution existence. Extensive experiments on MRI and CT reconstruction demonstrate KIDOT's superior performance. Code is available at https://github.com/TaoranZheng717/KIDOT.",
    "summary_cn": "提出KIDOT框架，利用动态最优传输和成像物理知识，从非配对数据中学习医学图像重建，减少模拟与真实数据间的差距，提升MRI和CT重建性能。",
    "keywords": [
      "医学图像重建",
      "动态最优传输",
      "非配对学习",
      "成像物理知识",
      "MRI",
      "CT"
    ],
    "triple": {
      "method": "知识引导动态最优传输框架",
      "result": "在MRI和CT重建中表现优异",
      "contribution": "减少模拟与真实数据差距，提升重建鲁棒性"
    }
  },
  {
    "id": "e454ab4a4512",
    "venue": "NeurIPS",
    "year": 2025,
    "title": "SHF: Symmetrical Hierarchical Forest with Pretrained Vision Transformer Encoder for High-Resolution Medical Segmentation",
    "url": "https://neurips.cc/virtual/2025/poster/120046",
    "abstract": "The NeurIPS Logo above may be used on presentations. Right-click and choose\n                            download. It is a vector graphic and may be used at any scale.",
    "summary_cn": "提出SHF模型，结合预训练视觉Transformer编码器与对称分层森林，用于高分辨率医学图像分割，提升精度与效率。",
    "keywords": [
      "医学图像分割",
      "视觉Transformer",
      "对称分层森林",
      "高分辨率",
      "预训练编码器",
      "深度学习"
    ],
    "triple": {
      "method": "对称分层森林与预训练视觉Transformer编码器",
      "result": "提升高分辨率医学图像分割精度与效率",
      "contribution": "提出SHF模型，优化分割性能"
    }
  },
  {
    "id": "b7bc29fe3664",
    "venue": "NeurIPS",
    "year": 2024,
    "title": "Conditioning 3D Diffusion Models with 2D Images: Towards Standardized OCT Volumes through En Face-Informed Super-Resolution",
    "url": "https://neurips.cc/virtual/2024/106859",
    "abstract": "High anisotropy in volumetric medical images can lead to the inconsistent quantification of anatomical and pathological structures. Particularly in optical coherence tomography (OCT), slice spacing can substantially vary across and within datasets, studies, and clinical practices. We propose to standardize OCT volumes to less anisotropic volumes by conditioning 3D diffusion models with en face scanning laser ophthalmoscopy (SLO) imaging data, a 2D modality already commonly available in clinical practice. We trained and evaluated on data from the multicenter and multimodal MACUSTAR study. While upsampling the number of slices by a factor of 8, our method outperforms tricubic interpolation and diffusion models without en face conditioning in terms of perceptual similarity metrics. Qualitative results demonstrate improved coherence and structural similarity. Our approach allows for better informed generative decisions, potentially reducing hallucinations. We hope this work will provide the next step towards standardized high-quality volumetric imaging, enabling more consistent quantifications.",
    "summary_cn": "本研究提出利用2D SLO图像条件化3D扩散模型，对OCT体积进行超分辨率标准化，以减少各向异性，提升图像质量和一致性。",
    "keywords": [
      "光学相干断层扫描",
      "扩散模型",
      "超分辨率",
      "图像标准化",
      "各向异性",
      "生成模型"
    ],
    "triple": {
      "method": "基于2D SLO图像条件化的3D扩散模型",
      "result": "在感知相似性指标上优于三三次插值和无条件扩散模型",
      "contribution": "推动OCT体积标准化，减少幻觉，提升量化一致性"
    }
  },
  {
    "id": "c24b236dd37b",
    "venue": "NeurIPS",
    "year": 2024,
    "title": "Do Histopathological Foundation Models Eliminate Batch Effects? A Comparative Study",
    "url": "https://neurips.cc/virtual/2024/103891",
    "abstract": "Deep learning has led to remarkable advancements in computational histopathology, e.g., in diagnostics, biomarker prediction, and outcome prognosis. Yet, the lack of annotated data and the impact of batch effects, e.g., systematic technical data differences across hospitals, hamper model robustness and generalization. Recent histopathological foundation models --- pretrained on millions to billions of images --- have been reported to improve generalization performances on various downstream tasks. However, it has not been systematically assessed whether they fully eliminate batch effects. In this study, we empirically show that the feature embeddings of the foundation models still contain distinct hospital signatures that can lead to biased predictions and misclassifications. We further find that the signatures are not removed by stain normalization methods, dominate distances in feature space, and are evident across various principal components. Our work provides a novel perspective on the evaluation of medical foundation models, paving the way for more robust pretraining strategies and downstream predictors.",
    "summary_cn": "研究发现，组织病理学基础模型虽提升下游任务性能，但未能完全消除医院间的批次效应，可能导致预测偏差。",
    "keywords": [
      "基础模型",
      "批次效应",
      "组织病理学",
      "特征嵌入",
      "泛化性能",
      "医院签名"
    ],
    "triple": {
      "method": "实证分析特征嵌入",
      "result": "模型仍含医院签名，影响预测",
      "contribution": "为医学基础模型评估提供新视角"
    }
  },
  {
    "id": "3e70bb65b2ed",
    "venue": "NeurIPS",
    "year": 2024,
    "title": "RespLLM: Unifying Audio and Text with Multimodal LLMs for Generalized Respiratory Health Prediction",
    "url": "https://neurips.cc/virtual/2024/103923",
    "abstract": "The high incidence and mortality rates associated with respiratory diseases underscores the importance of early screening. Machine learning models can automate clinical consultations and auscultation, offering vital support in this area. However, the data involved, spanning demographics, medical history, symptoms, and respiratory audio, are heterogeneous and complex. Existing approaches are insufficient and lack generalizability, as they typically rely on limited training data, basic fusion techniques, and task-specific design. In this paper, we propose RespLLM, a novel multimodal large language model (LLM) framework that unifies text and audio representations for respiratory health prediction. RespLLM leverages the extensive prior knowledge of pretrained LLMs and enables effective audio-text fusion through cross-modal attentions. Instruction tuning is employed to integrate diverse data from multiple sources, ensuring generalizability and versatility of the model. Experiments on five real-world datasets demonstrate that RespLLM outperforms leading baselines by an average of 4.6% on trained tasks, 7.9% on unseen datasets, and facilitates zero-shot predictions for new tasks. Our work lays the foundation for multimodal models that can perceive, listen to, and understand heterogeneous data, paving the way for scalable respiratory health diagnosis.",
    "summary_cn": "提出RespLLM多模态大模型，统一文本与音频表征，通过跨模态注意力和指令调优提升呼吸健康预测的泛化能力，在多个数据集上表现优异。",
    "keywords": [
      "多模态大模型",
      "呼吸健康预测",
      "音频-文本融合",
      "跨模态注意力",
      "指令调优",
      "泛化能力"
    ],
    "triple": {
      "method": "跨模态注意力与指令调优",
      "result": "在训练任务上平均提升4.6%，未见数据集上提升7.9%",
      "contribution": "提出统一多模态框架，增强泛化与零样本预测能力"
    }
  },
  {
    "id": "0c9f30b0e1dd",
    "venue": "NeurIPS",
    "year": 2024,
    "title": "Memorize and Rank: Elevating Large Language Models for Clinical Diagnosis Prediction",
    "url": "https://neurips.cc/virtual/2024/106864",
    "abstract": "Clinical diagnosis prediction models, when provided with a patient's medical history, aim to detect potential diseases early, facilitating timely intervention and improving prognostic outcomes. However, the inherent scarcity of patient data and large disease candidate space often pose challenges in developing satisfactory models for this intricate task. The exploration of leveraging modern Large Language Models (LLMs) for encapsulating clinical decision processes has been limited. We introduce MERA, a clinical diagnosis prediction model that bridges pertaining natural language knowledge with medical practice. We apply hierarchical contrastive learning on a disease candidate ranking list to alleviate the large decision space issue. With concept memorization through fine-tuning, we bridge the natural language clinical knowledge with medical codes. Experimental results on MIMIC datasets show that MERA achieves the state-of-the-art for diagnosis prediction.",
    "summary_cn": "MERA模型通过分层对比学习和概念记忆，将自然语言知识融入医疗实践，提升临床诊断预测性能，在MIMIC数据集上达到最优。",
    "keywords": [
      "临床诊断预测",
      "大型语言模型",
      "分层对比学习",
      "概念记忆",
      "MIMIC数据集",
      "疾病候选排序"
    ],
    "triple": {
      "method": "分层对比学习与概念记忆微调",
      "result": "在MIMIC数据集上实现最优诊断预测",
      "contribution": "融合自然语言知识与医疗实践，缓解大数据稀缺和决策空间大问题"
    }
  },
  {
    "id": "708aa6325c43",
    "venue": "NeurIPS",
    "year": 2024,
    "title": "2:30 - 3:30 PM: M3H: Multimodal Multitask ML for Healthcare",
    "url": "https://neurips.cc/virtual/2024/109305",
    "abstract": "M3H: Multimodal Multitask ML for Healthcare, NIPS Oral for AIM-FM Workshop",
    "summary_cn": "M3H提出多模态多任务机器学习框架，应用于医疗健康领域，提升模型性能与效率。",
    "keywords": [
      "多模态学习",
      "多任务学习",
      "机器学习",
      "医疗健康",
      "NIPS",
      "AIM-FM"
    ],
    "triple": {
      "method": "多模态多任务机器学习框架",
      "result": "提升医疗模型性能与效率",
      "contribution": "推动医疗AI应用发展"
    }
  },
  {
    "id": "7d0ba7d3ce9b",
    "venue": "NeurIPS",
    "year": 2025,
    "title": "Cognitive Machine Learning for Reducing Survey Fatigue in Clinical Trials",
    "url": "https://neurips.cc/virtual/2025/133792",
    "abstract": "Clinical trials are essential to advancing medical knowledge, but current data collection methods impose heavy cognitive demands on participants [1]. A major challenge is survey fatigue, particularly in electronic patient-reported outcomes (ePROs), where repeated questionnaires lead to habituation, superficial responses, and disengagement [2]. These effects undermine data reliability and erode trust in the trial process. We propose a cognitive machine learning framework that addresses this problem by embedding insights from cognitive science into ML-driven trial design [3].    We created a phased roadmap for combining cognitive science and ML in clinical research. Phase 1 introduces lightweight cognitive overlays into ePRO systems: adaptive phrasing, variable timing, and reinforcement learning-based adjustments designed to minimize cognitive load [1] while sustaining engagement through strategies analogous to cognitive behavioral techniques such as reframing and gradual exposure [4]. Phase 2 integrates a patient-first cognitive ML model trained with behavioral priors (e.g., Centaur-style models [5]) to sustain engagement and predict dropouts. This model delivers calibrated summaries and defers low-confidence cases to clinicians. Phase 3 reframes trials as cognitive ecosystems, where participant engagement metrics—such as attention, comprehension, and trust—become first-class endpoints alongside biomarkers.   We illustrate the feasibility of our approach through a survey fatigue case study. A conversational overlay adaptively rephrases questions, detects hesitation cues, and introduces cognitive scaffolding to sustain attention. Outputs are mapped back into standard ePRO formats with auditable traces. Early results suggest this approach can reduce missingness, improve response quality, and lower dropout risks while preserving regulatory compliance and clinician oversight [6].   Our work represents both an epistemic and ethical shift. It reframes survey responses not as noisy compliance data but as cognitively mediated signals. And by modeling participant comprehension, attention, and engagement, cognitive ML provides pathways to reduce attrition, improve inclusivity, and strengthen patient trust. This case study demonstrates how cognitively aware trials can enhance reliability, equity, and participant-centeredness.  References  [1] J. Sweller. Cognitive load during problem solving: effects on learning. Cognitive Science, 12(2):257–285, 1988.  [2] S. Rolstad, J. Adler, and A. Rydén. Response burden and questionnaire length: is shorter better? A review and meta-analysis. Value in Health, 14(8):1101–1108, 2011.  [3] A. Tversky and D. Kahneman. Judgment under uncertainty: heuristics and biases. Science, 185(4157):1124–1131, 1974.  [4] F. Lieder and T. L. Griffiths. Resource-rational analysis: understanding human cognition as the optimal use of limited computational resources. Behavioral and Brain Sciences, 43:e1, 2020.  [5] M. Binz, E. Akata, M. Bethge, et al. A foundation model to predict and capture human cognition. Nature, 644:1002–1009, 2025. doi:10.1038/s41586-025-09215-4.  [6] E. Basch, A. M. Deal, A. C. Dueck, H. I. Scher, M. G. Kris, C. Hudis, and D. Schrag. Overall survival results of a trial assessing patient-reported outcomes for symptom monitoring during routine cancer treatment. JAMA, 318(2):197–198, 2017.",
    "summary_cn": "提出认知机器学习框架，通过自适应提问、认知支架等技术减少临床试验中的问卷疲劳，提升数据质量和参与者参与度。",
    "keywords": [
      "认知机器学习",
      "问卷疲劳",
      "临床试验",
      "电子患者报告结局",
      "参与度",
      "自适应系统"
    ],
    "triple": {
      "method": "认知科学+机器学习的三阶段框架",
      "result": "减少数据缺失、提升回答质量、降低退出风险",
      "contribution": "将问卷响应重构为认知信号，提升试验可靠性与以患者为中心"
    }
  },
  {
    "id": "c4cf6ed2c0a8",
    "venue": "NeurIPS",
    "year": 2025,
    "title": "Subclass-Aware Inclusive Classifier via Repulsive Hidden Strata",
    "url": "https://neurips.cc/virtual/2025/133906",
    "abstract": "Classification models in machine learning are typically trained with coarse-grained class labels,  which overlook fine-grained subclass variations. This phenomenon, known as hidden stratification [1], results in asymmetric performance; models excel on dominant subclasses but struggle on rare or underrepresented ones. Such biases critically undermine fairness and robustness, especially in safety-sensitive applications such as medical imaging. We introduce Subclass-Aware Inclusive Classification (SAIC), a framework shown in Figure 1 that explicitly addresses hidden stratification. SAIC operates in two stages: (i) unsupervised subclass identification using a repulsive point process (k-DPP [2]) to uncover diverse and representative latent subclasses without prior assumptions, and (ii) subclass-aware classification with Group Distributionally Robust Optimization (GDRO), which emphasizes minimizing worst-case subclass loss. Extensive experiments on four benchmark datasets (MNIST, CIFAR-10, Waterbirds, and CelebA) show that SAIC consistently improves robustness without compromising overall accuracy. Specifically, we compare against K-means- and GMM-generated subclasses [3, 1] and also give the accuracy obtained using true subclass labels, as given in Table 1. Beyond overall accuracy, SAIC’s clustering module demonstrates superior subclass identification, closely matching true subclass counts, preserving rare subclass purity, and maintaining moderate runtime efficiency. SAIC provides a scalable solution to hidden stratification by combining diversity-aware subclass discovery with robust optimization, thereby enhancing fairness and reliability in high-stakes classification tasks.",
    "summary_cn": "提出SAIC框架解决隐藏分层问题，通过无监督子类识别与分布鲁棒优化，提升模型在罕见子类上的性能，增强公平性与鲁棒性。",
    "keywords": [
      "隐藏分层",
      "子类识别",
      "分布鲁棒优化",
      "公平性",
      "无监督学习",
      "分类模型"
    ],
    "triple": {
      "method": "k-DPP子类识别与GDRO优化",
      "result": "提升罕见子类性能，保持整体精度",
      "contribution": "增强分类公平性与鲁棒性"
    }
  },
  {
    "id": "6cd205bb3ced",
    "venue": "NeurIPS",
    "year": 2025,
    "title": "Beyond Average Value Function in Precision Medicine: Maximum Probability-Driven Reinforcement Learning for  Survival Analysis",
    "url": "https://neurips.cc/virtual/2025/poster/118678",
    "abstract": "Constructing multistage optimal decisions for alternating recurrent event data is critically important in medical and healthcare research. Current reinforcement learning (RL) algorithms have only been applied to time-to-event data, with the objective of maximizing expected survival time. However, alternating recurrent event data has a different structure, which motivates us to model the probability and frequency of event occurrences rather than a single terminal outcome.  In this paper, we introduce an RL framework specifically designed for alternating recurrent event data. Our goal is to maximize the probability that the duration between consecutive events exceeds a clinically meaningful threshold. To achieve this, we identify a lower bound of this probability, which transforms the problem into maximizing a cumulative sum of log probabilities, thus enabling direct application of standard RL algorithms. We establish the theoretical properties of the resulting optimal policy and demonstrate through numerical experiments that our proposed algorithm yields a larger probability of that the time between events exceeds a critical threshold compared with existing state-of-the-art algorithms.",
    "summary_cn": "本文提出一种强化学习框架，针对交替复发事件数据，旨在最大化事件间隔超过临床阈值的概率，而非传统生存时间。",
    "keywords": [
      "强化学习",
      "生存分析",
      "交替复发事件",
      "概率最大化",
      "精准医疗",
      "最优策略"
    ],
    "triple": {
      "method": "提出最大化事件间隔概率的强化学习框架",
      "result": "算法在数值实验中优于现有方法，提高事件间隔超过阈值的概率",
      "contribution": "为交替复发事件数据提供新强化学习目标，扩展精准医疗应用"
    }
  },
  {
    "id": "d373039325a3",
    "venue": "NeurIPS",
    "year": 2025,
    "title": "Fine-Tuning Large Language Models on EHR Data for Early Endometriosis Diagnosis in Adolescents",
    "url": "https://neurips.cc/virtual/2025/133856",
    "abstract": "Fine-Tuning Large Language Models on EHR Data for Early Endometriosis Diagnosis in Adolescents  Background. Endometriosis is a chronic condition affecting approximately 10% of women, with adolescents often experiencing diagnostic delays up to three times longer than adults.1 Contributing factors include a lack of non-invasive diagnostic methods and symptom overlap with other conditions. Genetic variants may influence the risk of developing endometriosis2-4, but integrating genomic data into clinical decision-making remains underexplored. This project aims to combine large language models (LLMs) with genetic biomarkers to develop multi-modal, fine-tuned models for early and accurate detection of endometriosis in adolescents.  Methods. We collected 7,221 clinical notes from the Mount Sinai Data Warehouse (MSDW) for 125 patients (ages 13–19) diagnosed with endometriosis. A subset of 700 notes from 26 patients was annotated by medical experts using standardized keyword-based criteria. These notes were segmented, tokenized, and encoded. The LLMs LLaMA, Gemma, GatorTron, and GPT-4o were fine-tuned for clinical natural language processing (NLP) tasks and evaluated using accuracy, precision, recall, and F1 metrics. In addition to the clinical notes, we obtained access to the Mount Sinai Million Health Discoveries Program for genetic data and are in the process of conducting a genome-wide association study (GWAS) to identify endometriosis-associated markers. These genomic features, including SNPs and polygenic risk scores (PRS), will be integrated as structured inputs alongside unstructured clinical text to improve predictive accuracy and support personalized diagnosis.  Results. Across nine clinical symptom categories, GPT-4o-chat achieved the strongest overall performance, with an average F1 of 0.486 and accuracy of 0.871. Gemma performed moderately (F1: 0.261, Acc: 0.869), while LLaMA showed comparable accuracy (0.905) but a lower F1 (0.258). GatorTron obtained the lowest F1 (0.189) despite competitive accuracy (0.887). For endometriosis classification specifically, all models achieved relatively high accuracy (>0.62), though GPT-4o-chat led with the highest F1 (0.771). These results indicate that GPT-4o-chat provided the best balance between precision and recall. However, performance varied widely by symptom, with models excelling in structured signals like pelvic tenderness but struggling on more non-specific categories such as GI symptoms. LLaMa exhibited the longest training and evaluation times.   Discussion. This study is one of the first systematic evaluations of open-source, trainable LLMs for NLP-based detection of adolescent endometriosis from unstructured EHR data. GPT-4o-chat demonstrated the strongest performance while maintaining computational efficiency, underscoring its potential scalability in resource-constrained clinical settings. By fine-tuning LLMs using EHR notes, these models can uncover subtle diagnostic patterns that are often missed in conventional workflows. Incorporating genetic information following GWAS will further enhance predictive power by enabling the models to capture both symptomatic presentation and underlying biological risk. Together, these findings highlight the promise of multi-modal frameworks to support earlier, more accurate diagnoses, guide personalized treatment strategies, and advance equitable care.  References DiVasta, A. D., Vitonis, A. F., Laufer, M. R., & Missmer, S. A. (2018). Spectrum of symptoms in women diagnosed with endometriosis during adolescence vs adulthood. American journal of obstetrics and gynecology, 218(3), 324.e1–324.e11.  Mackenzie, S. C., et al. (2024). Genome-wide association reveals a locus in neuregulin 3 associated with gabapentin efficacy in women with chronic pelvic pain. iScience, 27(8), 110370.  Dimitrakov, J., & Guthrie, D. (2009). Genetics and phenotyping of urological chronic pelvic pain syndrome. The Journal of urology, 181(4), 1550–1557. Li, Y. Z., & Ji, R. R. (2024). Gene therapy for chronic pain management. Cell reports. Medicine, 5(10), 101756.",
    "summary_cn": "研究通过微调大语言模型分析青少年子宫内膜异位症患者的电子健康记录，结合基因组数据提升早期诊断准确性。GPT-4o-chat表现最佳，为多模态诊断框架提供支持。",
    "keywords": [
      "子宫内膜异位症",
      "大语言模型",
      "电子健康记录",
      "基因组数据",
      "早期诊断",
      "青少年"
    ],
    "triple": {
      "method": "微调LLMs分析EHR与基因组数据",
      "result": "GPT-4o-chat在症状分类中F1达0.486，诊断F1为0.771",
      "contribution": "提出多模态框架，提升青少年子宫内膜异位症早期诊断准确性"
    }
  },
  {
    "id": "505d1efbe590",
    "venue": "NeurIPS",
    "year": 2024,
    "title": "Jogging the Memory of Unlearned LLMs Through Targeted Relearning Attacks",
    "url": "https://neurips.cc/virtual/2024/106237",
    "abstract": "Machine unlearning is a promising approach to mitigate undesirable memorization of training data in ML models. However, in this work we show that existing approaches for unlearning in LLMs are surprisingly susceptible to a simple set of targeted relearning attacks . With access to only a small and potentially loosely related set of data, we find that we can “jog” the memory of unlearned models to reverse the effects of unlearning. For example, we show that relearning on public medical articles can lead an unlearned LLM to output harmful knowledge about bioweapons, relearning general wiki information about the book series Harry Potter can force the model to output verbatim memorized text. We formalize this unlearning-relearning pipeline, explore the attack across three popular unlearning benchmarks, and discuss future works and guidelines that result from our study.",
    "summary_cn": "研究发现，现有大语言模型遗忘方法易受针对性再学习攻击，少量数据即可恢复遗忘内容，揭示遗忘机制脆弱性。",
    "keywords": [
      "大语言模型",
      "遗忘机制",
      "再学习攻击",
      "数据安全",
      "模型脆弱性",
      "记忆恢复"
    ],
    "triple": {
      "method": "针对性再学习攻击",
      "result": "少量数据可逆转遗忘效果",
      "contribution": "揭示遗忘机制脆弱性并提出指南"
    }
  },
  {
    "id": "428b45577e68",
    "venue": "NeurIPS",
    "year": 2024,
    "title": "Towards Conversational AI for Spina Bifida Care",
    "url": "https://neurips.cc/virtual/2024/104984",
    "abstract": "Spina Bifida (SB) is a complex neural tube defect that presents multifaceted healthcare challenges requiring multidisciplinary management. While advances in foundation models (FMs), offer promising avenues for enhancing SB care through intelligent, context-aware support, existing models struggle to accurately identify and reason about SB's diverse symptoms. This study benchmarks eight widely used large language models (LLMs) through qualitative and quantitative evaluations, focusing on their ability to address the unique medical challenges of SB. We introduce an \\textit{inverse prompting} technique designed to guide LLMs through a step-wise diagnostic process by incorporating a predefined symptom set relevant to SB, thereby preventing premature conclusions and improving diagnostic reasoning. Our evaluations reveal significant limitations in the LLMs' abilities to accurately diagnose SB-related conditions, underscoring the need for specialized approaches. Building on these findings, we propose a novel framework that integrates a structured, symptom-based knowledge base specific to SB, enhancing the models' contextual understanding and reasoning capabilities. This work highlights the potential of tailored AI solutions in improving access to care for individuals with SB, particularly in populations where gaps in knowledgeable providers persist. By addressing the shortcomings of general-purpose LLMs, our suggested framework aims to streamline SB care and improve patient outcomes, paving the way for more effective AI-assisted healthcare interventions in complex chronic conditions.",
    "summary_cn": "本研究评估了八种大语言模型在脊柱裂护理中的表现，发现其诊断能力有限。提出了一种结合症状知识库的框架，通过逆提示技术提升模型推理，以改善复杂慢性病的AI辅助医疗。",
    "keywords": [
      "脊柱裂",
      "大语言模型",
      "逆提示",
      "诊断推理",
      "医疗AI",
      "症状知识库"
    ],
    "triple": {
      "method": "逆提示技术与症状知识库集成",
      "result": "模型诊断能力有限，但新框架提升推理",
      "contribution": "为脊柱裂护理提供定制化AI解决方案"
    }
  },
  {
    "id": "59ee1dbe611d",
    "venue": "NeurIPS",
    "year": 2024,
    "title": "Finding Interior Optimum of Black-box Constrained Objective with Bayesian Optimization",
    "url": "https://neurips.cc/virtual/2024/98894",
    "abstract": "Optimizing objectives under constraints, where both the objectives and constraints are black box functions, is a common scenario in real-world applications such as the design of medical therapies, industrial process optimization, and hyperparameter optimization. One popular approach to handle these complex scenarios is Bayesian Optimization (BO). However, when it comes to the theoretical understanding of constrained Bayesian optimization (CBO), the existing framework often relies on heuristics, approximations, or relaxation of objectives and, therefore, lacks the same level of theoretical guarantees as in canonical BO. In this paper, we exclude the boundary candidates that could be compromised by noise perturbation and aim to find the interior optimum of the black-box-constrained objective. We rely on the insight that optimizing the objective and learning the constraints can both help identify the high-confidence regions of interest (ROI) that potentially contain the interior optimum. We propose an efficient CBO framework that intersects the ROIs identified from each aspect on a discretized search space to determine the general ROI. Then, on the ROI, we optimize the acquisition functions, balancing the learning of the constraints and the optimization of the objective. We showcase the efficiency and robustness of our proposed CBO framework through the high probability regret bounds for the algorithm and extensive empirical evidence.",
    "summary_cn": "提出一种高效约束贝叶斯优化框架，通过排除边界候选点并交叉高置信兴趣区域，寻找黑盒约束目标的内点最优解，具有理论保证和实证验证。",
    "keywords": [
      "约束贝叶斯优化",
      "黑盒函数",
      "内点最优",
      "兴趣区域",
      "高概率遗憾界",
      "噪声鲁棒性"
    ],
    "triple": {
      "method": "交叉高置信兴趣区域并优化获取函数",
      "result": "实现高效鲁棒的内点最优解搜索",
      "contribution": "提供理论保证的约束贝叶斯优化框架"
    }
  },
  {
    "id": "5673ab0fea48",
    "venue": "NeurIPS",
    "year": 2024,
    "title": "Geometric Deep Learning with Quasiconformal Neural Networks: An Introduction",
    "url": "https://neurips.cc/virtual/2024/103474",
    "abstract": "We introduce Quasiconformal Neural Networks (QNNs), a novel framework that integrates quasiconformal maps into neural architectures, providing a rigorous mathematical basis for handling non-Euclidean data. QNNs control geometric distortions using bounded maximal dilatation across network layers, preserving essential data structures. We present theoretical results that guarantee the stability and geometric consistency of QNNs. This work opens new avenues in geometric deep learning, particularly for applications involving complex topologies, with significant implications for fields such as image registration and medical imaging.",
    "summary_cn": "本文提出拟共形神经网络（QNNs），将拟共形映射融入神经网络，为处理非欧几里得数据提供数学基础，控制几何失真并保证稳定性，推动几何深度学习在图像配准和医学成像等领域的应用。",
    "keywords": [
      "拟共形神经网络",
      "几何深度学习",
      "非欧几里得数据",
      "几何失真控制",
      "图像配准",
      "医学成像"
    ],
    "triple": {
      "method": "集成拟共形映射到神经网络",
      "result": "控制几何失真并保证稳定性",
      "contribution": "为处理非欧数据提供新框架"
    }
  },
  {
    "id": "3bdc431fa0d9",
    "venue": "NeurIPS",
    "year": 2024,
    "title": "AnyPrefer: An Automatic Framework for Preference Data Synthesis",
    "url": "https://neurips.cc/virtual/2024/106177",
    "abstract": "High-quality preference data is essential for aligning foundation models with human values through preference learning. However, manual annotation of such data is often time-consuming and costly. Recent methods adopt a self-rewarding approach, where the target model generates and annotates its own preference data, but this can lead to inaccuracies due to the reward model sharing weights with the target model, amplifying inherent biases. To address these issues, we propose Anyprefer, a framework designed to synthesize high-quality preference data for the target model. Anyprefer frames the data synthesis process as a cooperative two-player Markov Game, where the target model and a judge model collaborate. Here, a series of external tools are introduced to assist the judge model in accurately rewarding the target model’s responses, mitigating biases in the process. We also introduce a feedback mechanism to optimize prompts for both models, enhancing collaboration and improving data quality. The synthesized data is compiled into a new preference dataset, Anyprefer-V1, consisting of 58K high-quality preference pairs. Extensive experiments show that Anyprefer significantly improves model alignment across four applications, covering 21 datasets, achieving average improvements of 18.55 in five natural language generation datasets, 3.66 in nine vision-language understanding datasets, 30.05 in three medical image analysis datasets, and 14.50 in four visuo-motor control tasks.",
    "summary_cn": "Anyprefer框架通过双智能体协作与外部工具辅助，自动合成高质量偏好数据，提升基础模型对齐效果。",
    "keywords": [
      "偏好数据合成",
      "模型对齐",
      "双智能体协作",
      "外部工具辅助",
      "反馈机制",
      "Anyprefer-V1"
    ],
    "triple": {
      "method": "双智能体协作与外部工具辅助",
      "result": "合成58K高质量偏好对，模型对齐性能显著提升",
      "contribution": "提出自动偏好数据合成框架，减少人工标注依赖"
    }
  }
]