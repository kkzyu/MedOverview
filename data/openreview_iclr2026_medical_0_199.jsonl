{"venue": "ICLR", "search_title": "Cancer-Myth: Evaluating Large Language Models on Patient Questions with False Presuppositions", "full_title": "Cancer-Myth: Evaluating Large Language Models on Patient Questions with False Presuppositions", "url": "https://openreview.net/forum?id=fOXLhZIaUj", "year": 2026, "is_main_conference": true, "abstract_snippet": "Cancer patients are increasingly turning to large language models (LLMs) for medical information, making it critical to assess how well these models handle complex, personalized questions. \nHowever, current medical benchmarks focus on medical exams or consumer-searched questions and do not evaluate LLMs on real patient questions with patient details. \nIn this paper, we first have three hematology-oncology physicians evaluate cancer-related questions drawn from real patients. \nWhile LLM responses are generally accurate, the models frequently fail to recognize or address false presuppositions} in the questions, posing risks to safe medical decision-making.\nTo study this limitation systematically, we introduce Cancer-Myth, an expert-verified adversarial dataset of 585 cancer-related questions with false presuppositions.\nOn this benchmark, no frontier LLM---including GPT-5, Gemini-2.5-Pro, and Claude-4-Sonnet---corrects these false presuppositions more than $43\\%$ of the time.\nTo study mitigation strategies, we further construct a 150-question Cancer-Myth-NFP set, in which physicians confirm the absence of false presuppositions.\nWe find typical mitigation strategies, such as adding precautionary prompts with GEPA optimization, can raise accuracy on Cancer-Myth to $80\\%$, but at the cost of misidentifying presuppositions in $41\\%$ of Cancer-Myth-NFP questions and causing a $10\\%$ relative performance drop on other medical benchmarks.\nThese findings highlight a critical gap in the reliability of LLMs, show that prompting alone is not a reliable remedy for false presuppositions, and underscore the need for more robust safeguards in medical AI systems.", "abstract": "Cancer patients are increasingly turning to large language models (LLMs) for medical information, making it critical to assess how well these models handle complex, personalized questions. \nHowever, current medical benchmarks focus on medical exams or consumer-searched questions and do not evaluate LLMs on real patient questions with patient details. \nIn this paper, we first have three hematology-oncology physicians evaluate cancer-related questions drawn from real patients. \nWhile LLM responses are generally accurate, the models frequently fail to recognize or address false presuppositions} in the questions, posing risks to safe medical decision-making.\nTo study this limitation systematically, we introduce Cancer-Myth, an expert-verified adversarial dataset of 585 cancer-related questions with false presuppositions.\nOn this benchmark, no frontier LLM---including GPT-5, Gemini-2.5-Pro, and Claude-4-Sonnet---corrects these false presuppositions more than $43\\%$ of the time.\nTo study mitigation strategies, we further construct a 150-question Cancer-Myth-NFP set, in which physicians confirm the absence of false presuppositions.\nWe find typical mitigation strategies, such as adding precautionary prompts with GEPA optimization, can raise accuracy on Cancer-Myth to $80\\%$, but at the cost of misidentifying presuppositions in $41\\%$ of Cancer-Myth-NFP questions and causing a $10\\%$ relative performance drop on other medical benchmarks.\nThese findings highlight a critical gap in the reliability of LLMs, show that prompting alone is not a reliable remedy for false presuppositions, and underscore the need for more robust safeguards in medical AI systems.", "abstract_source_venue": "OpenReviewAPI", "abstract_source_url": "https://api2.openreview.net/notes?id=fOXLhZIaUj", "openreview_id": "fOXLhZIaUj", "openreview_forum_id": "fOXLhZIaUj", "authors": [], "pdf_url": "https://openreview.net/pdf/a21f83e80e84a8f0b6d4546a959c75408be46cd2.pdf", "summary_cn": "研究评估大语言模型处理癌症患者提问中错误预设的能力，发现前沿模型识别率不足43%，现有缓解策略效果有限且存在副作用。", "keywords": ["大语言模型", "癌症患者提问", "错误预设", "医疗AI安全", "对抗数据集", "缓解策略"], "triple": {"method": "构建专家验证的对抗数据集Cancer-Myth", "result": "前沿LLMs纠正错误预设率<43%，缓解策略效果有限且有副作用", "contribution": "揭示LLMs处理医疗错误预设的缺陷，警示单纯提示优化的局限性"}}
{"venue": "ICLR", "search_title": "MedAgent-Pro: Towards Evidence-based Multi-modal Medical Diagnosis via Reasoning Agentic Workflow", "full_title": "MedAgent-Pro: Towards Evidence-based Multi-modal Medical Diagnosis via Reasoning Agentic Workflow", "url": "https://openreview.net/forum?id=ZOuU0udyA4", "year": 2026, "is_main_conference": true, "abstract_snippet": "Modern clinical diagnosis relies on the comprehensive analysis of multi-modal patient data, drawing on medical expertise to ensure systematic and rigorous reasoning. Recent advances in Vision–Language Models (VLMs) and agent-based methods are reshaping medical diagnosis by effectively integrating multi-modal information. However, they often output direct answers and empirical-driven conclusions without clinical evidence supported by quantitative analysis, which compromises their reliability and hinders clinical usability. \nHere we propose MedAgent-Pro, an agentic reasoning paradigm that mirrors modern diagnosis principles via a hierarchical diagnostic workflow, consisting of disease-level standardized plan generation and patient-level personalized step-by-step reasoning. To support disease-level planning, a retrieval-augmented generation agent is designed to access medical guidelines for alignment with clinical standards.  For patient-level reasoning, MedAgent-Pro leverages professional tools such as visual models to take various actions to analyze multi-modal input, and performs evidence-based reflection to iteratively adjust memory, enforcing rigorous reasoning throughout the process. Extensive experiments across a wide range of anatomical regions, imaging modalities, and diseases demonstrate the superiority of MedAgent-Pro over mainstream VLMs, agentic systems and leading expert models. Ablation studies and expert evaluation further confirm its robustness and clinical relevance. Anonymized code link is available in the reproducibility statement.", "abstract": "Modern clinical diagnosis relies on the comprehensive analysis of multi-modal patient data, drawing on medical expertise to ensure systematic and rigorous reasoning. Recent advances in Vision–Language Models (VLMs) and agent-based methods are reshaping medical diagnosis by effectively integrating multi-modal information. However, they often output direct answers and empirical-driven conclusions without clinical evidence supported by quantitative analysis, which compromises their reliability and hinders clinical usability. \nHere we propose MedAgent-Pro, an agentic reasoning paradigm that mirrors modern diagnosis principles via a hierarchical diagnostic workflow, consisting of disease-level standardized plan generation and patient-level personalized step-by-step reasoning. To support disease-level planning, a retrieval-augmented generation agent is designed to access medical guidelines for alignment with clinical standards.  For patient-level reasoning, MedAgent-Pro leverages professional tools such as visual models to take various actions to analyze multi-modal input, and performs evidence-based reflection to iteratively adjust memory, enforcing rigorous reasoning throughout the process. Extensive experiments across a wide range of anatomical regions, imaging modalities, and diseases demonstrate the superiority of MedAgent-Pro over mainstream VLMs, agentic systems and leading expert models. Ablation studies and expert evaluation further confirm its robustness and clinical relevance. Anonymized code link is available in the reproducibility statement.", "abstract_source_venue": "OpenReviewAPI", "abstract_source_url": "https://api2.openreview.net/notes?id=ZOuU0udyA4", "openreview_id": "ZOuU0udyA4", "openreview_forum_id": "ZOuU0udyA4", "authors": [], "pdf_url": "https://openreview.net/pdf/e7110936d04fac257223b380ae256b851b59bfa3.pdf", "summary_cn": "提出MedAgent-Pro代理推理范式，通过分层诊断工作流整合多模态数据，实现基于证据的医学诊断，提升临床可靠性和实用性。", "keywords": ["医学诊断", "多模态", "代理推理", "证据驱动", "临床指南", "视觉语言模型"], "triple": {"method": "分层代理工作流（疾病级规划与患者级推理）", "result": "超越主流视觉语言模型和专家模型", "contribution": "提升诊断可靠性与临床相关性"}}
{"venue": "ICLR", "search_title": "Beyond Classification Accuracy:  Neural-MedBench and the Need for Deeper Reasoning Benchmarks", "full_title": "Beyond Classification Accuracy:  Neural-MedBench and the Need for Deeper Reasoning Benchmarks", "url": "https://openreview.net/forum?id=KKA59ai0x6", "year": 2026, "is_main_conference": true, "abstract_snippet": "Recent advances in vision-language models (VLMs) have achieved remarkable performance on standard medical benchmarks, yet their true clinical reasoning ability remains unclear. Existing datasets predominantly emphasize classification accuracy, creating an evaluation illusion in which models appear proficient while still failing at high-stakes diagnostic reasoning.\nWe introduce \\texttt{Neural-MedBench}, a compact yet reasoning-intensive benchmark specifically designed to probe the limits of multimodal clinical reasoning in neurology. Neural-MedBench integrates multi-sequence MRI scans, structured electronic health records, and clinical notes, and encompasses three core task families: differential diagnosis, lesion recognition, and rationale generation. To ensure reliable evaluation, we develop a hybrid scoring pipeline that combines LLM-based graders, clinician validation, and semantic similarity metrics.\nThrough systematic evaluation of state-of-the-art VLMs, including GPT-4o, Claude-4, and MedGemma, we observe a sharp performance drop compared to conventional datasets. Error analysis shows that reasoning failures, rather than perceptual errors, dominate model shortcomings.\nOur findings highlight the necessity of a Two-Axis Evaluation Framework: breadth-oriented large datasets for statistical generalization, and depth-oriented, compact benchmarks such as Neural-MedBench for reasoning fidelity. We release Neural-MedBench as an open and extensible diagnostic testbed, which guides the expansion of future benchmarks and enables rigorous yet cost-effective assessment of clinically trustworthy AI.", "abstract": "Recent advances in vision-language models (VLMs) have achieved remarkable performance on standard medical benchmarks, yet their true clinical reasoning ability remains unclear. Existing datasets predominantly emphasize classification accuracy, creating an evaluation illusion in which models appear proficient while still failing at high-stakes diagnostic reasoning.\nWe introduce \\texttt{Neural-MedBench}, a compact yet reasoning-intensive benchmark specifically designed to probe the limits of multimodal clinical reasoning in neurology. Neural-MedBench integrates multi-sequence MRI scans, structured electronic health records, and clinical notes, and encompasses three core task families: differential diagnosis, lesion recognition, and rationale generation. To ensure reliable evaluation, we develop a hybrid scoring pipeline that combines LLM-based graders, clinician validation, and semantic similarity metrics.\nThrough systematic evaluation of state-of-the-art VLMs, including GPT-4o, Claude-4, and MedGemma, we observe a sharp performance drop compared to conventional datasets. Error analysis shows that reasoning failures, rather than perceptual errors, dominate model shortcomings.\nOur findings highlight the necessity of a Two-Axis Evaluation Framework: breadth-oriented large datasets for statistical generalization, and depth-oriented, compact benchmarks such as Neural-MedBench for reasoning fidelity. We release Neural-MedBench as an open and extensible diagnostic testbed, which guides the expansion of future benchmarks and enables rigorous yet cost-effective assessment of clinically trustworthy AI.", "abstract_source_venue": "OpenReviewAPI", "abstract_source_url": "https://api2.openreview.net/notes?id=KKA59ai0x6", "openreview_id": "KKA59ai0x6", "openreview_forum_id": "KKA59ai0x6", "authors": [], "pdf_url": "https://openreview.net/pdf/39b1448e43d0d09d35c213c1470436fa8ffa3066.pdf", "summary_cn": "提出Neural-MedBench基准，评估神经学多模态临床推理能力，发现现有模型在深度推理任务上表现显著下降，强调需结合广度与深度评估框架。", "keywords": ["临床推理", "多模态基准", "神经学", "评估框架", "视觉语言模型", "诊断准确性"], "triple": {"method": "构建Neural-MedBench基准与混合评分流程", "result": "模型在推理任务上表现显著下降，错误主要由推理失败导致", "contribution": "提出两轴评估框架，促进临床可信AI的严格评估"}}
{"venue": "ICLR", "search_title": "How Do Medical MLLMs Fail?  A Study on Visual Grounding in Medical Images", "full_title": "How Do Medical MLLMs Fail?  A Study on Visual Grounding in Medical Images", "url": "https://openreview.net/forum?id=dXshexyFKx", "year": 2026, "is_main_conference": true, "abstract_snippet": "Generalist multimodal large language models (MLLMs) have achieved impressive performance across a wide range of vision-language tasks. However, their performance on medical tasks—particularly in zero-shot settings where generalization is critical—remains suboptimal. A key research gap is the limited understanding of why medical MLLMs underperform in medical image interpretation.\n**In this work**, we present a pioneering systematic investigation into the visual grounding capabilities of state-of-the-art medical MLLMs. To disentangle *visual grounding* from *semantic grounding*, we design VGMED, a novel evaluation dataset developed with expert clinical guidance, explicitly assessing the visual grounding capability of medical MLLMs. \nWe introduce new quantitative metrics and conduct detailed qualitative analyses. Our study across **eight** state-of-the-art (SOTA) medical MLLMs validates that they often fail to ground their predictions in clinically relevant image regions. We note that this finding is specific to medical image analysis; in contrast, prior work has shown that MLLMs are capable of grounding their predictions in the correct image regions when applied to natural scene images.\nMotivated by these findings, we propose VGRefine, a simple yet effective inference-time method that refines attention distribution to improve visual grounding in medical settings. Our approach achieves SOTA performance across  6 diverse Med-VQA benchmarks (over 110K VQA samples from 8 imaging modalities) \nwithout requiring additional training or external expert models.  Overall, our work, for the first time, systematically validates inadequate visual grounding as one of the key contributing factors for medical MLLMs' under-performance.\nCode and additional experiments are included in the Supp.", "abstract": "Generalist multimodal large language models (MLLMs) have achieved impressive performance across a wide range of vision-language tasks. However, their performance on medical tasks—particularly in zero-shot settings where generalization is critical—remains suboptimal. A key research gap is the limited understanding of why medical MLLMs underperform in medical image interpretation.\n**In this work**, we present a pioneering systematic investigation into the visual grounding capabilities of state-of-the-art medical MLLMs. To disentangle *visual grounding* from *semantic grounding*, we design VGMED, a novel evaluation dataset developed with expert clinical guidance, explicitly assessing the visual grounding capability of medical MLLMs. \nWe introduce new quantitative metrics and conduct detailed qualitative analyses. Our study across **eight** state-of-the-art (SOTA) medical MLLMs validates that they often fail to ground their predictions in clinically relevant image regions. We note that this finding is specific to medical image analysis; in contrast, prior work has shown that MLLMs are capable of grounding their predictions in the correct image regions when applied to natural scene images.\nMotivated by these findings, we propose VGRefine, a simple yet effective inference-time method that refines attention distribution to improve visual grounding in medical settings. Our approach achieves SOTA performance across  6 diverse Med-VQA benchmarks (over 110K VQA samples from 8 imaging modalities) \nwithout requiring additional training or external expert models.  Overall, our work, for the first time, systematically validates inadequate visual grounding as one of the key contributing factors for medical MLLMs' under-performance.\nCode and additional experiments are included in the Supp.", "abstract_source_venue": "OpenReviewAPI", "abstract_source_url": "https://api2.openreview.net/notes?id=dXshexyFKx", "openreview_id": "dXshexyFKx", "openreview_forum_id": "dXshexyFKx", "authors": [], "pdf_url": "https://openreview.net/pdf/7cb8fa11ac03e56e2f46f49fda6b98a1212d9360.pdf", "summary_cn": "本研究系统评估了医学MLLMs在视觉定位上的不足，并提出VGRefine方法提升性能，在多个医学VQA基准上实现最优。", "keywords": ["医学MLLMs", "视觉定位", "评估数据集", "VGRefine", "医学图像", "零样本学习"], "triple": {"method": "设计VGMED数据集与VGRefine推理方法", "result": "医学MLLMs视觉定位不足，VGRefine在6个基准上达最优", "contribution": "首次系统验证视觉定位不足是医学MLLMs性能差的关键因素"}}
{"venue": "ICLR", "search_title": "Anatomy-aware Representation Learning for Medical Ultrasound", "full_title": "Anatomy-aware Representation Learning for Medical Ultrasound", "url": "https://openreview.net/forum?id=5ThIWuDkEf", "year": 2026, "is_main_conference": true, "abstract_snippet": "Diagnostic accuracy of ultrasound imaging is limited by qualitative variability and its reliance on the expertise of medical professionals. Such challenges increase demand for computer-aided diagnostic systems that enhance diagnostic accuracy and efficiency. However, the unique texture and structural attributes of ultrasound images, and the scarcity of large-scale ultrasound datasets hinder the effective application of conventional machine learning methodologies. To address the challenges, we propose Anatomy-aware Representation Learning (ARL), a novel self-supervised representation learning framework specifically designed for medical ultrasound imaging. ARL incorporates an anatomy-adaptive Vision Transformer (A-ViT). The A-ViT is parameterized, using the proposed large-scale medical ultrasound dataset, to provide anatomy-aware feature representations. Through extensive experiments across various ultrasound-based diagnostic tasks, including breast and thyroid cancer, cardiac view classification, and gallbladder tumor and COVID-19 identification, we demonstrate that ARL significantly outperforms existing self-supervised learning baselines. The experiments demonstrate the potential of ARL in advancing medical ultrasound diagnostics by providing anatomy-specific feature representation", "abstract": "Diagnostic accuracy of ultrasound imaging is limited by qualitative variability and its reliance on the expertise of medical professionals. Such challenges increase demand for computer-aided diagnostic systems that enhance diagnostic accuracy and efficiency. However, the unique texture and structural attributes of ultrasound images, and the scarcity of large-scale ultrasound datasets hinder the effective application of conventional machine learning methodologies. To address the challenges, we propose Anatomy-aware Representation Learning (ARL), a novel self-supervised representation learning framework specifically designed for medical ultrasound imaging. ARL incorporates an anatomy-adaptive Vision Transformer (A-ViT). The A-ViT is parameterized, using the proposed large-scale medical ultrasound dataset, to provide anatomy-aware feature representations. Through extensive experiments across various ultrasound-based diagnostic tasks, including breast and thyroid cancer, cardiac view classification, and gallbladder tumor and COVID-19 identification, we demonstrate that ARL significantly outperforms existing self-supervised learning baselines. The experiments demonstrate the potential of ARL in advancing medical ultrasound diagnostics by providing anatomy-specific feature representation", "abstract_source_venue": "OpenReviewAPI", "abstract_source_url": "https://api2.openreview.net/notes?id=5ThIWuDkEf", "openreview_id": "5ThIWuDkEf", "openreview_forum_id": "5ThIWuDkEf", "authors": [], "pdf_url": "https://openreview.net/pdf/3052db98077cd2b176bd3c54a499e2af51fe13a6.pdf", "summary_cn": "提出解剖感知表示学习框架，结合解剖自适应视觉Transformer，在多种超声诊断任务中显著优于现有自监督方法，提升诊断准确性。", "keywords": ["解剖感知表示学习", "自监督学习", "医学超声", "视觉Transformer", "诊断任务", "特征表示"], "triple": {"method": "解剖自适应视觉Transformer", "result": "在多种超声诊断任务中表现优异", "contribution": "提升超声诊断准确性"}}
{"venue": "ICLR", "search_title": "Can Large Language Models Match the Conclusions of Systematic Reviews?", "full_title": "Can Large Language Models Match the Conclusions of Systematic Reviews?", "url": "https://openreview.net/forum?id=uIJyYkOgAy", "year": 2026, "is_main_conference": true, "abstract_snippet": "Systematic reviews (SR), in which experts summarize and analyze evidence across\nindividual studies to provide insights on a specialized topic, are a cornerstone\nfor evidence-based clinical decision-making, research, and policy. Given the exponential growth of scientific articles, there is growing interest in using large\nlanguage models (LLMs) to automate SR generation. However, the ability of\nLLMs to critically assess evidence and reason across multiple documents to provide recommendations at the same proficiency as domain experts remains poorly\ncharacterized. We therefore ask: Can LLMs match the conclusions of systematic\nreviews written by clinical experts when given access to the same studies?\nTo explore this question, we present MedEvidence, a benchmark pairing findings\nfrom 100 SRs with the studies they are based on. We benchmark 24 LLMs on\nMedEvidence, including reasoning, non-reasoning, medical specialist, and models\nacross varying sizes (from 7B-700B). Through our systematic evaluation, we find\nthat reasoning does not necessarily improve performance, larger models do not\nconsistently yield greater gains, and knowledge-based fine-tuning degrades accuracy on MedEvidence. Instead, most models exhibit similar behavior: performance\ntends to degrade as token length increases, their responses show overconfidence,\nand, contrary to human experts, all models show a lack of scientific skepticism\ntoward low-quality findings. These results suggest that more work is still required\nbefore LLMs can reliably match the observations from expert-conducted SRs, even\nthough these systems are already deployed and being used by clinicians. We release our codebase\nand benchmark\nto the broader research community to further\ninvestigate LLM-based SR systems.", "abstract": "Systematic reviews (SR), in which experts summarize and analyze evidence across\nindividual studies to provide insights on a specialized topic, are a cornerstone\nfor evidence-based clinical decision-making, research, and policy. Given the exponential growth of scientific articles, there is growing interest in using large\nlanguage models (LLMs) to automate SR generation. However, the ability of\nLLMs to critically assess evidence and reason across multiple documents to provide recommendations at the same proficiency as domain experts remains poorly\ncharacterized. We therefore ask: Can LLMs match the conclusions of systematic\nreviews written by clinical experts when given access to the same studies?\nTo explore this question, we present MedEvidence, a benchmark pairing findings\nfrom 100 SRs with the studies they are based on. We benchmark 24 LLMs on\nMedEvidence, including reasoning, non-reasoning, medical specialist, and models\nacross varying sizes (from 7B-700B). Through our systematic evaluation, we find\nthat reasoning does not necessarily improve performance, larger models do not\nconsistently yield greater gains, and knowledge-based fine-tuning degrades accuracy on MedEvidence. Instead, most models exhibit similar behavior: performance\ntends to degrade as token length increases, their responses show overconfidence,\nand, contrary to human experts, all models show a lack of scientific skepticism\ntoward low-quality findings. These results suggest that more work is still required\nbefore LLMs can reliably match the observations from expert-conducted SRs, even\nthough these systems are already deployed and being used by clinicians. We release our codebase\nand benchmark\nto the broader research community to further\ninvestigate LLM-based SR systems.", "abstract_source_venue": "OpenReviewAPI", "abstract_source_url": "https://api2.openreview.net/notes?id=uIJyYkOgAy", "openreview_id": "uIJyYkOgAy", "openreview_forum_id": "uIJyYkOgAy", "authors": [], "pdf_url": "https://openreview.net/pdf/e64591af8710a96009a869c36280c05af2acd40d.pdf", "summary_cn": "研究评估大语言模型能否匹配专家系统综述的结论，发现模型在证据评估上存在局限，如过度自信和缺乏科学怀疑，尚无法可靠替代专家。", "keywords": ["大语言模型", "系统综述", "医学证据评估", "基准测试", "临床决策", "自动化生成"], "triple": {"method": "构建MedEvidence基准，评估24个LLM", "result": "模型表现受限于文本长度、过度自信且缺乏科学怀疑", "contribution": "揭示LLM在匹配专家系统综述结论上的不足，提供开放基准促进研究"}}
{"venue": "ICLR", "search_title": "MedGMAE: Gaussian Masked Autoencoders for Medical Volumetric Representation Learning", "full_title": "MedGMAE: Gaussian Masked Autoencoders for Medical Volumetric Representation Learning", "url": "https://openreview.net/forum?id=Z2XIRLv535", "year": 2026, "is_main_conference": true, "abstract_snippet": "Self-supervised pre-training has emerged as a critical paradigm for learning transferable representations from unlabeled medical volumetric data. Masked autoencoder based methods have garnered significant attention, yet their application to volumetric medical image faces fundamental limitations from the discrete voxel-level reconstruction objective, which neglects comprehensive anatomical structure continuity. To address this challenge, We propose MedGMAE, a novel framework that replaces traditional voxel reconstruction with 3D Gaussian primitives reconstruction as new perspectives on representation learning. Our approach learns to predict complete sets of 3D Gaussian parameters as semantic abstractions to represent the entire 3D volume, from sparse visible image patches. MedGMAE demonstrates dual utility across medical imaging applications. For representation learning, sparse Gaussian prediction produces superior encoder representations that outperform traditional MAE baselines on downstream segmentation, classification, and registration tasks. For volumetric reconstruction, the Gaussian decoder leverages pretrained anatomical priors to accelerate 3D CT volume reconstruction convergence. Extensive experiments across multiple medical imaging datasets demonstrate that our approach achieves superior performance, establishing a new paradigm for medical image pre-training. Code will be released soon.", "abstract": "Self-supervised pre-training has emerged as a critical paradigm for learning transferable representations from unlabeled medical volumetric data. Masked autoencoder based methods have garnered significant attention, yet their application to volumetric medical image faces fundamental limitations from the discrete voxel-level reconstruction objective, which neglects comprehensive anatomical structure continuity. To address this challenge, We propose MedGMAE, a novel framework that replaces traditional voxel reconstruction with 3D Gaussian primitives reconstruction as new perspectives on representation learning. Our approach learns to predict complete sets of 3D Gaussian parameters as semantic abstractions to represent the entire 3D volume, from sparse visible image patches. MedGMAE demonstrates dual utility across medical imaging applications. For representation learning, sparse Gaussian prediction produces superior encoder representations that outperform traditional MAE baselines on downstream segmentation, classification, and registration tasks. For volumetric reconstruction, the Gaussian decoder leverages pretrained anatomical priors to accelerate 3D CT volume reconstruction convergence. Extensive experiments across multiple medical imaging datasets demonstrate that our approach achieves superior performance, establishing a new paradigm for medical image pre-training. Code will be released soon.", "abstract_source_venue": "OpenReviewAPI", "abstract_source_url": "https://api2.openreview.net/notes?id=Z2XIRLv535", "openreview_id": "Z2XIRLv535", "openreview_forum_id": "Z2XIRLv535", "authors": [], "pdf_url": "https://openreview.net/pdf/c8a8ac67bb2b3ddd8bcb4bc1e35d3a649f009034.pdf", "summary_cn": "提出MedGMAE框架，用3D高斯基元重建替代传统体素重建，从稀疏图像块学习医学体积表示，提升下游任务性能并加速CT重建。", "keywords": ["自监督学习", "掩码自编码器", "3D高斯基元", "医学体积图像", "表示学习", "CT重建"], "triple": {"method": "3D高斯基元重建", "result": "下游任务性能提升，CT重建加速", "contribution": "提出医学图像预训练新范式"}}
{"venue": "ICLR", "search_title": "Medical Interpretability and Knowledge Maps of Large Language Models", "full_title": "Medical Interpretability and Knowledge Maps of Large Language Models", "url": "https://openreview.net/forum?id=BhqFWlYKUi", "year": 2026, "is_main_conference": true, "abstract_snippet": "We present a systematic study of medical-domain interpretability in Large Language Models (LLMs). We study how the LLMs both represent and process medical knowledge through four different interpretability techniques: (1) UMAP projections of intermediate activations, (2) gradient-based saliency with respect to the model weights, (3) layer lesioning/removal and (4) activation patching. We present knowledge maps of five LLMs which show, at a coarse-resolution, where knowledge about patient's ages, medical symptoms, diseases and drugs is stored in the models. In particular for Llama3.3-70B, we find that most medical knowledge is processed in the first half of the model's layers. In addition, we find several interesting phenomena: (i) age is often encoded in a non-linear and sometimes discontinuous manner at intermediate layers in the models, (ii) the disease progression representation is non-monotonic and circular at certain layers of the model, (iii) in Llama, drugs cluster better by medical specialty rather than mechanism of action, especially for Llama and (iv) Gemma-27B and MedGemma-27B have activations that collapse at intermediate layers but recover by the final layers. These results can guide future research on fine-tuning, un-learning or de-biasing LLMs for medical tasks by suggesting at which layers in the model these techniques should be applied. We attached our source code to the paper for reproducibility.", "abstract": "We present a systematic study of medical-domain interpretability in Large Language Models (LLMs). We study how the LLMs both represent and process medical knowledge through four different interpretability techniques: (1) UMAP projections of intermediate activations, (2) gradient-based saliency with respect to the model weights, (3) layer lesioning/removal and (4) activation patching. We present knowledge maps of five LLMs which show, at a coarse-resolution, where knowledge about patient's ages, medical symptoms, diseases and drugs is stored in the models. In particular for Llama3.3-70B, we find that most medical knowledge is processed in the first half of the model's layers. In addition, we find several interesting phenomena: (i) age is often encoded in a non-linear and sometimes discontinuous manner at intermediate layers in the models, (ii) the disease progression representation is non-monotonic and circular at certain layers of the model, (iii) in Llama, drugs cluster better by medical specialty rather than mechanism of action, especially for Llama and (iv) Gemma-27B and MedGemma-27B have activations that collapse at intermediate layers but recover by the final layers. These results can guide future research on fine-tuning, un-learning or de-biasing LLMs for medical tasks by suggesting at which layers in the model these techniques should be applied. We attached our source code to the paper for reproducibility.", "abstract_source_venue": "OpenReviewAPI", "abstract_source_url": "https://api2.openreview.net/notes?id=BhqFWlYKUi", "openreview_id": "BhqFWlYKUi", "openreview_forum_id": "BhqFWlYKUi", "authors": [], "pdf_url": "https://openreview.net/pdf/3099e3d88d3c3c3e889a42f9a81f08b16dfa679e.pdf", "summary_cn": "本研究系统探索大语言模型在医学领域的可解释性，通过四种技术分析模型如何存储和处理医学知识，发现医学知识主要集中在前半层，并揭示年龄、疾病进展等编码特性。", "keywords": ["大语言模型", "医学可解释性", "知识图谱", "层间分析", "激活映射", "模型优化"], "triple": {"method": "UMAP投影、梯度显著性、层切除、激活修补", "result": "医学知识集中在前半层，年龄编码非线性，疾病表示非单调，药物按专科聚类", "contribution": "为医学任务微调、去偏提供层间指导"}}
{"venue": "ICLR", "search_title": "Identity-Free Deferral For Unseen Experts", "full_title": "Identity-Free Deferral For Unseen Experts", "url": "https://openreview.net/forum?id=4YG9ufFg58", "year": 2026, "is_main_conference": true, "abstract_snippet": "Learning to Defer (L2D) improves AI reliability in decision-critical environments, such as healthcare, by training a model to either make its own prediction or delerejector the decision to a human expert. A key challenge is adapting to unseen experts: those who were not involved during the system's training process. Current methods for this task, however, can falter when unseen experts are out-of-distribution (OOD) relative to the training population. We identify a core architectural flaw as the cause: they learn identity-conditioned policies by processing class-indexed signals in fixed coordinates, creating shortcuts that violate the problem's inherent permutation symmetry. We introduce Identity-Free Deferral (IFD), an architecture that enforces this symmetry by construction. From a few-shot context, IFD builds a query-independent Bayesian competence profile for each expert. It then supplies the deferral rejector with a low-dimensional, role-indexed state containing only structural information, such as the model's confidence in its top-ranked class and the expert's estimated skill for that same role, which obscures absolute class identities. We train IFD using an uncertainty-aware, context-only objective that removes the need for expensive query-time expert labels. We formally prove the permutation invariance of our approach, contrasting it with the generic non-invariance of standard population encoders. Experiments on medical imaging benchmarks and ImageNet-16H with real human annotators show that IFD consistently improves generalization to unseen experts, with significant gains in OOD settings, all while using fewer annotations than competing methods.", "abstract": "Learning to Defer (L2D) improves AI reliability in decision-critical environments, such as healthcare, by training a model to either make its own prediction or delerejector the decision to a human expert. A key challenge is adapting to unseen experts: those who were not involved during the system's training process. Current methods for this task, however, can falter when unseen experts are out-of-distribution (OOD) relative to the training population. We identify a core architectural flaw as the cause: they learn identity-conditioned policies by processing class-indexed signals in fixed coordinates, creating shortcuts that violate the problem's inherent permutation symmetry. We introduce Identity-Free Deferral (IFD), an architecture that enforces this symmetry by construction. From a few-shot context, IFD builds a query-independent Bayesian competence profile for each expert. It then supplies the deferral rejector with a low-dimensional, role-indexed state containing only structural information, such as the model's confidence in its top-ranked class and the expert's estimated skill for that same role, which obscures absolute class identities. We train IFD using an uncertainty-aware, context-only objective that removes the need for expensive query-time expert labels. We formally prove the permutation invariance of our approach, contrasting it with the generic non-invariance of standard population encoders. Experiments on medical imaging benchmarks and ImageNet-16H with real human annotators show that IFD consistently improves generalization to unseen experts, with significant gains in OOD settings, all while using fewer annotations than competing methods.", "abstract_source_venue": "OpenReviewAPI", "abstract_source_url": "https://api2.openreview.net/notes?id=4YG9ufFg58", "openreview_id": "4YG9ufFg58", "openreview_forum_id": "4YG9ufFg58", "authors": [], "pdf_url": "https://openreview.net/pdf/2b84d8ab31b821240426f0893469ad42c0049b23.pdf", "summary_cn": "提出身份无关延迟（IFD）架构，通过强制排列对称性，提升AI在医疗等关键决策环境中对未见专家的泛化能力，减少标注需求。", "keywords": ["学习延迟", "未见专家", "排列对称性", "贝叶斯能力分析", "医疗影像", "分布外泛化"], "triple": {"method": "构建查询无关的贝叶斯能力档案，提供低维角色索引状态", "result": "在医疗影像和ImageNet-16H基准上提升对未见专家的泛化性能，尤其在分布外场景表现更佳", "contribution": "提出身份无关延迟架构，通过强制排列对称性解决未见专家适应问题，减少标注成本"}}
{"venue": "ICLR", "search_title": "MedVR: Annotation-Free Medical Visual Reasoning via Agentic Reinforcement Learning", "full_title": "MedVR: Annotation-Free Medical Visual Reasoning via Agentic Reinforcement Learning", "url": "https://openreview.net/forum?id=cK35kNVm5r", "year": 2026, "is_main_conference": true, "abstract_snippet": "Medical Vision-Language Models (VLMs) hold immense promise for complex clinical tasks, but their reasoning capabilities are often constrained by text-only paradigms that fail to ground inferences in visual evidence. This limitation not only curtails performance on tasks requiring fine-grained visual analysis but also introduces risks of visual hallucination in safety-critical applications. Thus, we introduce MedVR, a novel reinforcement learning framework that enables annotation-free visual reasoning for medical VLMs. Its core innovation lies in two synergistic mechanisms: Entropy-guided Visual Regrounding (EVR) uses model uncertainty to direct exploration, while Consensus-based Credit Assignment (CCA) distills pseudo-supervision from rollout agreement. Without any human annotations for intermediate steps, MedVR achieves state-of-the-art performance on diverse public medical VQA benchmarks, significantly outperforming existing models. By learning to reason directly with visual evidence, MedVR promotes the robustness and transparency essential for accelerating the clinical deployment of medical AI.", "abstract": "Medical Vision-Language Models (VLMs) hold immense promise for complex clinical tasks, but their reasoning capabilities are often constrained by text-only paradigms that fail to ground inferences in visual evidence. This limitation not only curtails performance on tasks requiring fine-grained visual analysis but also introduces risks of visual hallucination in safety-critical applications. Thus, we introduce MedVR, a novel reinforcement learning framework that enables annotation-free visual reasoning for medical VLMs. Its core innovation lies in two synergistic mechanisms: Entropy-guided Visual Regrounding (EVR) uses model uncertainty to direct exploration, while Consensus-based Credit Assignment (CCA) distills pseudo-supervision from rollout agreement. Without any human annotations for intermediate steps, MedVR achieves state-of-the-art performance on diverse public medical VQA benchmarks, significantly outperforming existing models. By learning to reason directly with visual evidence, MedVR promotes the robustness and transparency essential for accelerating the clinical deployment of medical AI.", "abstract_source_venue": "OpenReviewAPI", "abstract_source_url": "https://api2.openreview.net/notes?id=cK35kNVm5r", "openreview_id": "cK35kNVm5r", "openreview_forum_id": "cK35kNVm5r", "authors": [], "pdf_url": "https://openreview.net/pdf/4afcb83673664833a6bdb762a4f9a5988b6c7ddd.pdf", "summary_cn": "MedVR通过强化学习实现无标注医学视觉推理，利用熵引导视觉重定位和共识信用分配，在医学VQA基准上取得最优性能，提升临床AI的鲁棒性和透明度。", "keywords": ["医学视觉语言模型", "强化学习", "视觉推理", "无标注学习", "医学VQA", "鲁棒性"], "triple": {"method": "熵引导视觉重定位与共识信用分配", "result": "医学VQA基准上性能最优", "contribution": "提升临床AI鲁棒性与透明度"}}
{"venue": "ICLR", "search_title": "MMedAgent-RL: Optimizing Multi-Agent Collaboration for Multimodal Medical Reasoning", "full_title": "MMedAgent-RL: Optimizing Multi-Agent Collaboration for Multimodal Medical Reasoning", "url": "https://openreview.net/forum?id=2awntLXwR6", "year": 2026, "is_main_conference": true, "abstract_snippet": "Medical Large Vision-Language Models (Med-LVLMs) have shown strong potential in multimodal diagnostic tasks. However, existing single-agent models struggle to generalize across diverse medical specialties, limiting their performance. Recent efforts introduce multi-agent collaboration frameworks inspired by clinical workflows, where general practitioners (GPs) and specialists interact in a fixed sequence. Despite improvements, these static pipelines lack flexibility and adaptability in reasoning. To address this, we propose MMedAgent-RL, a reinforcement learning (RL)-based multi-agent framework that enables dynamic, optimized collaboration among medical agents. Specifically, we train two GP agents based on Qwen2.5-VL via RL: the triage doctor learns to assign patients to appropriate specialties, while the attending physician integrates the judgments from multi-specialists and its own knowledge to make final decisions. To address the inconsistency in specialist outputs, we introduce a curriculum learning (CL)-guided RL strategy with dynamic entropy regulation, progressively teaching the attending physician to balance between imitating specialists and correcting their mistakes. Experiments on five medical VQA benchmarks demonstrate that MMedAgent-RL outperforms both open-source and proprietary Med-LVLMs. Notably, it achieves an average performance gain of 23.6\\% over strong baselines.", "abstract": "Medical Large Vision-Language Models (Med-LVLMs) have shown strong potential in multimodal diagnostic tasks. However, existing single-agent models struggle to generalize across diverse medical specialties, limiting their performance. Recent efforts introduce multi-agent collaboration frameworks inspired by clinical workflows, where general practitioners (GPs) and specialists interact in a fixed sequence. Despite improvements, these static pipelines lack flexibility and adaptability in reasoning. To address this, we propose MMedAgent-RL, a reinforcement learning (RL)-based multi-agent framework that enables dynamic, optimized collaboration among medical agents. Specifically, we train two GP agents based on Qwen2.5-VL via RL: the triage doctor learns to assign patients to appropriate specialties, while the attending physician integrates the judgments from multi-specialists and its own knowledge to make final decisions. To address the inconsistency in specialist outputs, we introduce a curriculum learning (CL)-guided RL strategy with dynamic entropy regulation, progressively teaching the attending physician to balance between imitating specialists and correcting their mistakes. Experiments on five medical VQA benchmarks demonstrate that MMedAgent-RL outperforms both open-source and proprietary Med-LVLMs. Notably, it achieves an average performance gain of 23.6\\% over strong baselines.", "abstract_source_venue": "OpenReviewAPI", "abstract_source_url": "https://api2.openreview.net/notes?id=2awntLXwR6", "openreview_id": "2awntLXwR6", "openreview_forum_id": "2awntLXwR6", "authors": [], "pdf_url": "https://openreview.net/pdf/3703f8cac22754f38c452da028e9c4d2a92fe408.pdf", "summary_cn": "提出MMedAgent-RL强化学习框架，优化多智能体协作进行医学多模态推理，在五个医学VQA基准上显著超越现有模型。", "keywords": ["多智能体协作", "强化学习", "医学多模态推理", "课程学习", "医学视觉问答"], "triple": {"method": "基于强化学习的动态多智能体协作框架", "result": "在五个医学VQA基准上平均性能提升23.6%", "contribution": "提出课程学习引导的强化学习策略，优化多专家协作决策"}}
{"venue": "ICLR", "search_title": "MedAraBench: Large-scale Arabic Medical Question Answering Dataset and Benchmark", "full_title": "MedAraBench: Large-scale Arabic Medical Question Answering Dataset and Benchmark", "url": "https://openreview.net/forum?id=1BXojAgNrg", "year": 2026, "is_main_conference": true, "abstract_snippet": "Arabic remains one of the most underrepresented languages in natural language processing research, particularly in medical applications, due to the limited availability of open-source data and benchmarks. The lack of resources hinders efforts to evaluate and advance the multilingual capabilities of Large Language Models (LLMs). In this paper, we introduce MedAraBench, a large-scale dataset consisting of Arabic multiple-choice question-answer pairs across various medical specialties. We constructed the dataset by manually digitizing a large repository of academic materials created by medical professionals in the Arabic-speaking region. We then conducted extensive preprocessing and split the dataset into training and test sets to support future research efforts in the area. To assess the quality of the data, we adopted two frameworks, namely expert human evaluation and LLM-as-a-judge. Our dataset is diverse and of high quality, spanning 19 specialties and five difficulty levels. For benchmarking purposes, we assessed the performance of eight state-of-the-art open-source and proprietary models, such as GPT-5, Gemini 2.0 Flash, and Claude 4-Sonnet. Our findings highlight the need for further domain-specific enhancements. We release the dataset and evaluation scripts to broaden the diversity of medical data benchmarks, expand the scope of evaluation suites for LLMs, and enhance the multilingual capabilities of models for deployment in clinical settings.", "abstract": "Arabic remains one of the most underrepresented languages in natural language processing research, particularly in medical applications, due to the limited availability of open-source data and benchmarks. The lack of resources hinders efforts to evaluate and advance the multilingual capabilities of Large Language Models (LLMs). In this paper, we introduce MedAraBench, a large-scale dataset consisting of Arabic multiple-choice question-answer pairs across various medical specialties. We constructed the dataset by manually digitizing a large repository of academic materials created by medical professionals in the Arabic-speaking region. We then conducted extensive preprocessing and split the dataset into training and test sets to support future research efforts in the area. To assess the quality of the data, we adopted two frameworks, namely expert human evaluation and LLM-as-a-judge. Our dataset is diverse and of high quality, spanning 19 specialties and five difficulty levels. For benchmarking purposes, we assessed the performance of eight state-of-the-art open-source and proprietary models, such as GPT-5, Gemini 2.0 Flash, and Claude 4-Sonnet. Our findings highlight the need for further domain-specific enhancements. We release the dataset and evaluation scripts to broaden the diversity of medical data benchmarks, expand the scope of evaluation suites for LLMs, and enhance the multilingual capabilities of models for deployment in clinical settings.", "abstract_source_venue": "OpenReviewAPI", "abstract_source_url": "https://api2.openreview.net/notes?id=1BXojAgNrg", "openreview_id": "1BXojAgNrg", "openreview_forum_id": "1BXojAgNrg", "authors": [], "pdf_url": "https://openreview.net/pdf/84fcd1a697860dbc63db8aa5b1fc055edf61dd1d.pdf", "summary_cn": "本文推出MedAraBench，一个大规模阿拉伯语医学问答数据集，涵盖19个专科和5个难度级别，用于评估大语言模型在医学领域的多语言能力。", "keywords": ["阿拉伯语医学问答", "多语言大语言模型", "医学数据集", "专家评估", "模型基准测试", "临床部署"], "triple": {"method": "手动数字化医学资料构建数据集，采用专家和LLM评估质量", "result": "数据集高质量且多样，评估显示现有模型需领域增强", "contribution": "提供开源数据集与基准，促进LLM多语言医学应用发展"}}
{"venue": "ICLR", "search_title": "M3CoTBench: Benchmark Chain-of-Thought of MLLMs in Medical Image Understanding", "full_title": "M3CoTBench: Benchmark Chain-of-Thought of MLLMs in Medical Image Understanding", "url": "https://openreview.net/forum?id=S7KyLgHqJf", "year": 2026, "is_main_conference": true, "abstract_snippet": "Chain-of-Thought (CoT) reasoning has proven effective in enhancing large language models by encouraging step-by-step intermediate reasoning, and recent advances have extended this paradigm to Multimodal Large Language Models (MLLMs). In the medical domain, where diagnostic decisions depend on nuanced visual cues and sequential reasoning, CoT aligns naturally with clinical thinking processes. However, Current benchmarks for medical image understanding generally focus on the final answer while ignoring the reasoning path. An opaque process lacks reliable bases for judgment, making it difficult to assist doctors in diagnosis. \nTo address this gap, we introduce a new M3CoTBench benchmark specifically designed to evaluate the correctness, efficiency, impact, and consistency of CoT reasoning in medical image understanding. M3CoTBench features  (1) a diverse, multi-level difficulty dataset covering 24 examination types, (2) 13 varying-difficulty tasks,  (3) a suite of CoT-specific evaluation metrics (correctness, efficiency, impact, and consistency) tailored to clinical reasoning,  and (4) a performance analysis of multiple MLLMs. M3CoTBench systematically evaluates CoT reasoning across diverse medical imaging tasks, revealing current limitations of MLLMs in generating reliable and clinically interpretable reasoning, and aims to foster the development of transparent, trustworthy, and diagnostically accurate AI systems for healthcare.", "abstract": "Chain-of-Thought (CoT) reasoning has proven effective in enhancing large language models by encouraging step-by-step intermediate reasoning, and recent advances have extended this paradigm to Multimodal Large Language Models (MLLMs). In the medical domain, where diagnostic decisions depend on nuanced visual cues and sequential reasoning, CoT aligns naturally with clinical thinking processes. However, Current benchmarks for medical image understanding generally focus on the final answer while ignoring the reasoning path. An opaque process lacks reliable bases for judgment, making it difficult to assist doctors in diagnosis. \nTo address this gap, we introduce a new M3CoTBench benchmark specifically designed to evaluate the correctness, efficiency, impact, and consistency of CoT reasoning in medical image understanding. M3CoTBench features  (1) a diverse, multi-level difficulty dataset covering 24 examination types, (2) 13 varying-difficulty tasks,  (3) a suite of CoT-specific evaluation metrics (correctness, efficiency, impact, and consistency) tailored to clinical reasoning,  and (4) a performance analysis of multiple MLLMs. M3CoTBench systematically evaluates CoT reasoning across diverse medical imaging tasks, revealing current limitations of MLLMs in generating reliable and clinically interpretable reasoning, and aims to foster the development of transparent, trustworthy, and diagnostically accurate AI systems for healthcare.", "abstract_source_venue": "OpenReviewAPI", "abstract_source_url": "https://api2.openreview.net/notes?id=S7KyLgHqJf", "openreview_id": "S7KyLgHqJf", "openreview_forum_id": "S7KyLgHqJf", "authors": [], "pdf_url": "https://openreview.net/pdf/6f6b51141d2fbf8154142e47841c71d2c0d1ec99.pdf", "summary_cn": "提出M3CoTBench基准，评估多模态大语言模型在医学图像理解中的思维链推理，涵盖多任务与评价指标，揭示模型局限性，促进医疗AI透明化发展。", "keywords": ["思维链推理", "多模态大语言模型", "医学图像理解", "基准评估", "临床推理", "医疗AI"], "triple": {"method": "构建多任务、多难度数据集与评价指标", "result": "揭示MLLMs在可靠临床推理上的局限", "contribution": "推动透明可信医疗AI系统发展"}}
{"venue": "ICLR", "search_title": "Improving 2D Diffusion Models for 3D Medical Imaging with Inter‑Slice Consistent Stochasticity", "full_title": "Improving 2D Diffusion Models for 3D Medical Imaging with Inter‑Slice Consistent Stochasticity", "url": "https://openreview.net/forum?id=R5ETdN6ifA", "year": 2026, "is_main_conference": true, "abstract_snippet": "3D medical imaging is in high demand and essential for clinical diagnosis and scientific research. Currently, diffusion models have become an effective tool for medical imaging reconstruction thanks to their ability to learn rich, high‑quality data priors.  However, learning the 3D data distribution with diffusion models in medical imaging is challenging, not only due to the difficulties in data collection but also because of the significant computational burden during model training. A common compromise is to train the diffusion model on 2D data priors and reconstruct stacked 2D slices to address 3D medical inverse problems. However, the intrinsic randomness of diffusion sampling causes severe inter‑slice discontinuities of reconstructed 3D volumes. Existing methods often enforce continuity regularizations along the $z$‑axis, which introduces sensitive hyper‑parameters and may lead to over-smoothing results. In this work, we revisit the origin of stochasticity in diffusion sampling and introduce Inter‑Slice Consistent Stochasticity (ISCS), a simple yet effective strategy that encourages inter‑slice consistency during diffusion sampling. Our key idea is to control the consistency of stochastic noise components during diffusion sampling, thereby aligning their sampling trajectories without adding any new loss terms or optimization steps.  Importantly, the proposed ISCS is plug‑and‑play and can be dropped into any 2D‑trained diffusion‑based 3D reconstruction pipeline without additional computational cost. Experiments on several medical imaging problems show that our method can effectively improve the performance of medical 3D imaging problems based on 2D diffusion models. Our findings suggest that controlling inter‑slice stochasticity is a principled and practically attractive route toward high‑fidelity 3D medical imaging with 2D diffusion priors. The code is available at: [https://anonymous.4open.science/r/ICLR-ISCS-3281](https://anonymous.4open.science/r/ICLR-ISCS-3281).", "abstract": "3D medical imaging is in high demand and essential for clinical diagnosis and scientific research. Currently, diffusion models have become an effective tool for medical imaging reconstruction thanks to their ability to learn rich, high‑quality data priors.  However, learning the 3D data distribution with diffusion models in medical imaging is challenging, not only due to the difficulties in data collection but also because of the significant computational burden during model training. A common compromise is to train the diffusion model on 2D data priors and reconstruct stacked 2D slices to address 3D medical inverse problems. However, the intrinsic randomness of diffusion sampling causes severe inter‑slice discontinuities of reconstructed 3D volumes. Existing methods often enforce continuity regularizations along the $z$‑axis, which introduces sensitive hyper‑parameters and may lead to over-smoothing results. In this work, we revisit the origin of stochasticity in diffusion sampling and introduce Inter‑Slice Consistent Stochasticity (ISCS), a simple yet effective strategy that encourages inter‑slice consistency during diffusion sampling. Our key idea is to control the consistency of stochastic noise components during diffusion sampling, thereby aligning their sampling trajectories without adding any new loss terms or optimization steps.  Importantly, the proposed ISCS is plug‑and‑play and can be dropped into any 2D‑trained diffusion‑based 3D reconstruction pipeline without additional computational cost. Experiments on several medical imaging problems show that our method can effectively improve the performance of medical 3D imaging problems based on 2D diffusion models. Our findings suggest that controlling inter‑slice stochasticity is a principled and practically attractive route toward high‑fidelity 3D medical imaging with 2D diffusion priors. The code is available at: [https://anonymous.4open.science/r/ICLR-ISCS-3281](https://anonymous.4open.science/r/ICLR-ISCS-3281).", "abstract_source_venue": "OpenReviewAPI", "abstract_source_url": "https://api2.openreview.net/notes?id=R5ETdN6ifA", "openreview_id": "R5ETdN6ifA", "openreview_forum_id": "R5ETdN6ifA", "authors": [], "pdf_url": "https://openreview.net/pdf/b06641f87b59e63130e23a7ff694f90bc746596a.pdf", "summary_cn": "提出ISCS方法，通过控制扩散采样中的噪声一致性，解决2D扩散模型重建3D医学影像时的切片间不连续问题，无需额外计算成本。", "keywords": ["3D医学影像", "扩散模型", "切片一致性", "随机噪声控制", "图像重建", "计算效率"], "triple": {"method": "控制扩散采样噪声一致性", "result": "提升3D重建质量与切片连续性", "contribution": "无需额外训练的即插即用方案"}}
{"venue": "ICLR", "search_title": "Synthesizing High-Quality Visual Question Answering from Medical Documents with Generator-Verifier LMMs", "full_title": "Synthesizing High-Quality Visual Question Answering from Medical Documents with Generator-Verifier LMMs", "url": "https://openreview.net/forum?id=ULMWcNduE3", "year": 2026, "is_main_conference": true, "abstract_snippet": "Large Multimodal Models (LMMs) are increasingly capable of answering medical questions that require joint reasoning over images and text, yet training general medical VQA systems is impeded by the lack of large, openly usable, high-quality corpora. We present \\textbf{MedVLSynther}, a rubric-guided generator-verifier framework that synthesizes high-quality multiple-choice VQA items directly from open biomedical literature by conditioning on figures, captions, and in-text references. The generator produces self-contained stems and parallel, mutually exclusive options under a machine-checkable JSON schema; a multi-stage verifier enforces essential gates (self-containment, single correct answer, clinical validity, image-text consistency), awards fine-grained positive points, and penalizes common failure modes before acceptance. Applying this pipeline to PubMed Central yields \\textit{MedVLSynther-13K}: 13,087 audited questions over 14,803 images spanning 13 imaging modalities and 28 anatomical regions. Training open-weight LMMs with reinforcement learning using verifiable rewards improves accuracy across six medical VQA benchmarks, achieving averages of 55.85 (3B) and 57.56 (7B), with up to 77.21 on VQA-RAD and 66.36 on PathVQA, outperforming strong medical LMMs. Ablations verify that both generation and verification are necessary and that more verified data consistently helps, and a targeted contamination analysis detects no leakage from evaluation suites. By operating entirely on open literature and open-weight models, MedVLSynther offers an auditable, reproducible, and privacy-preserving path to scalable medical VQA training data.", "abstract": "Large Multimodal Models (LMMs) are increasingly capable of answering medical questions that require joint reasoning over images and text, yet training general medical VQA systems is impeded by the lack of large, openly usable, high-quality corpora. We present \\textbf{MedVLSynther}, a rubric-guided generator-verifier framework that synthesizes high-quality multiple-choice VQA items directly from open biomedical literature by conditioning on figures, captions, and in-text references. The generator produces self-contained stems and parallel, mutually exclusive options under a machine-checkable JSON schema; a multi-stage verifier enforces essential gates (self-containment, single correct answer, clinical validity, image-text consistency), awards fine-grained positive points, and penalizes common failure modes before acceptance. Applying this pipeline to PubMed Central yields \\textit{MedVLSynther-13K}: 13,087 audited questions over 14,803 images spanning 13 imaging modalities and 28 anatomical regions. Training open-weight LMMs with reinforcement learning using verifiable rewards improves accuracy across six medical VQA benchmarks, achieving averages of 55.85 (3B) and 57.56 (7B), with up to 77.21 on VQA-RAD and 66.36 on PathVQA, outperforming strong medical LMMs. Ablations verify that both generation and verification are necessary and that more verified data consistently helps, and a targeted contamination analysis detects no leakage from evaluation suites. By operating entirely on open literature and open-weight models, MedVLSynther offers an auditable, reproducible, and privacy-preserving path to scalable medical VQA training data.", "abstract_source_venue": "OpenReviewAPI", "abstract_source_url": "https://api2.openreview.net/notes?id=ULMWcNduE3", "openreview_id": "ULMWcNduE3", "openreview_forum_id": "ULMWcNduE3", "authors": [], "pdf_url": "https://openreview.net/pdf/07267a23e557728e5f3739f58171c4213ddc7ddc.pdf", "summary_cn": "提出MedVLSynther框架，通过生成-验证流程从开放生物医学文献合成高质量医学视觉问答数据，训练LMM模型在多个基准测试中表现优异。", "keywords": ["医学视觉问答", "数据合成", "生成-验证框架", "大型多模态模型", "强化学习", "开放文献"], "triple": {"method": "基于生成-验证框架从文献合成VQA数据", "result": "训练LMM在六个医学VQA基准上提升准确率", "contribution": "提供可扩展、可审计的医学VQA训练数据生成方法"}}
{"venue": "ICLR", "search_title": "CRONOS: Continuous time reconstruction for 4D medical longitudinal series", "full_title": "CRONOS: Continuous time reconstruction for 4D medical longitudinal series", "url": "https://openreview.net/forum?id=XxqdbYD74l", "year": 2026, "is_main_conference": true, "abstract_snippet": "Forecasting how 3D medical scans evolve along time is important for disease progression, treatment planning, and developmental assessment. Yet existing models either rely on a single prior scan, fixed grid times, or target global labels, which limits voxel-level forecasting under irregular sampling. We present CRONOS, a unified framework for many-to-one prediction from multiple past scans that supports both discrete (grid-based) and continuous (real-valued) timestamps in one model, to the best of our knowledge the first to achieve continuous sequence-to-image forecasting for 3D medical data. CRONOS learns a spatio-temporal velocity field that transports context volumes toward a target volume at an arbitrary time, while operating directly in 3D voxel space. Across three public datasets spanning Cine-MRI, perfusion CT, and longitudinal MRI, CRONOS outperforms other baselines, while remaining computationally competitive. We will release code and evaluation protocols to enable reproducible, multi-dataset benchmarking of multi-context, continuous-time forecasting.", "abstract": "Forecasting how 3D medical scans evolve along time is important for disease progression, treatment planning, and developmental assessment. Yet existing models either rely on a single prior scan, fixed grid times, or target global labels, which limits voxel-level forecasting under irregular sampling. We present CRONOS, a unified framework for many-to-one prediction from multiple past scans that supports both discrete (grid-based) and continuous (real-valued) timestamps in one model, to the best of our knowledge the first to achieve continuous sequence-to-image forecasting for 3D medical data. CRONOS learns a spatio-temporal velocity field that transports context volumes toward a target volume at an arbitrary time, while operating directly in 3D voxel space. Across three public datasets spanning Cine-MRI, perfusion CT, and longitudinal MRI, CRONOS outperforms other baselines, while remaining computationally competitive. We will release code and evaluation protocols to enable reproducible, multi-dataset benchmarking of multi-context, continuous-time forecasting.", "abstract_source_venue": "OpenReviewAPI", "abstract_source_url": "https://api2.openreview.net/notes?id=XxqdbYD74l", "openreview_id": "XxqdbYD74l", "openreview_forum_id": "XxqdbYD74l", "authors": [], "pdf_url": "https://openreview.net/pdf/b3aa5ba5021b8762beb2cce46d9b07cb76ff541f.pdf", "summary_cn": "CRONOS提出首个支持连续时间戳的3D医学影像预测框架，通过时空速度场实现多对一预测，在多个数据集上优于基线方法。", "keywords": ["连续时间预测", "3D医学影像", "时空速度场", "多对一预测", "纵向序列", "疾病进展"], "triple": {"method": "学习时空速度场", "result": "在三个数据集上优于基线", "contribution": "首个支持连续时间戳的3D医学预测框架"}}
{"venue": "ICLR", "search_title": "AttTok: Marrying Attribute Tokens with Generative Pre-trained Vision-Language Models towards Medical Image Understanding", "full_title": "AttTok: Marrying Attribute Tokens with Generative Pre-trained Vision-Language Models towards Medical Image Understanding", "url": "https://openreview.net/forum?id=UjSoF5CM09", "year": 2026, "is_main_conference": true, "abstract_snippet": "Recent generative pre-trained vision–language (GPTv) models have achieved remarkable success in multi-modal understanding, inspiring their adaptation to medical imaging tasks such as disease diagnosis and visual question answering (VQA). However, current instruction-tuned GPTv models suffer from two key challenges: (1) medical attributes (e.g., disease names, severity grades) are encoded as plain text tokens, collapsing semantically distinct concepts into nearly identical textual sequences; and (2) inadequate textual supervision weakens visual representation learning, leading to severe inter-attribute confusion and misaligned vision–language embeddings. To address these limitations, we introduce attribute tokens (AttTok), a set of pre‑defined special tokens that uniquely encode clinical attributes (e.g., imaging modality, diagnosis, severity) within a structured token space. Complemented by attribute‑centric embedding books, AttTok serves as anchor points for aligning both visual and textual modalities into a shared, discriminative representation space. Building on this foundation, we design two key components: an attribute‑centric cross attention (ACC) adapter, which breaks the vision‑to‑text information‑flow bottleneck and enriches the visual encoder with discriminative attribute knowledge, and an attribute‑centric matching (ACM) loss, which enforces robust multi‑modal alignment centered on the attribute tokens. Extensive experiments on five medical classification benchmarks and three VQA datasets demonstrate that AttTok substantially improves both discriminative accuracy and medical knowledge reasoning, establishing a new paradigm for medical GPTv models with clinically discriminative understanding.", "abstract": "Recent generative pre-trained vision–language (GPTv) models have achieved remarkable success in multi-modal understanding, inspiring their adaptation to medical imaging tasks such as disease diagnosis and visual question answering (VQA). However, current instruction-tuned GPTv models suffer from two key challenges: (1) medical attributes (e.g., disease names, severity grades) are encoded as plain text tokens, collapsing semantically distinct concepts into nearly identical textual sequences; and (2) inadequate textual supervision weakens visual representation learning, leading to severe inter-attribute confusion and misaligned vision–language embeddings. To address these limitations, we introduce attribute tokens (AttTok), a set of pre‑defined special tokens that uniquely encode clinical attributes (e.g., imaging modality, diagnosis, severity) within a structured token space. Complemented by attribute‑centric embedding books, AttTok serves as anchor points for aligning both visual and textual modalities into a shared, discriminative representation space. Building on this foundation, we design two key components: an attribute‑centric cross attention (ACC) adapter, which breaks the vision‑to‑text information‑flow bottleneck and enriches the visual encoder with discriminative attribute knowledge, and an attribute‑centric matching (ACM) loss, which enforces robust multi‑modal alignment centered on the attribute tokens. Extensive experiments on five medical classification benchmarks and three VQA datasets demonstrate that AttTok substantially improves both discriminative accuracy and medical knowledge reasoning, establishing a new paradigm for medical GPTv models with clinically discriminative understanding.", "abstract_source_venue": "OpenReviewAPI", "abstract_source_url": "https://api2.openreview.net/notes?id=UjSoF5CM09", "openreview_id": "UjSoF5CM09", "openreview_forum_id": "UjSoF5CM09", "authors": [], "pdf_url": "https://openreview.net/pdf/62f6bcc030ff6b7c05a473fe65715fee129d35f9.pdf", "summary_cn": "提出AttTok方法，通过属性令牌和嵌入书增强GPTv模型，解决医学属性混淆问题，提升分类和问答性能。", "keywords": ["属性令牌", "视觉-语言模型", "医学图像理解", "跨模态对齐", "生成预训练"], "triple": {"method": "引入属性令牌和嵌入书，结合ACC适配器和ACM损失", "result": "在多个医学数据集上提升准确性和推理能力", "contribution": "建立临床可区分的医学GPTv模型新范式"}}
{"venue": "ICLR", "search_title": "ProstaTD: Bridging Surgical Triplet from Classification to Fully Supervised Detection", "full_title": "ProstaTD: Bridging Surgical Triplet from Classification to Fully Supervised Detection", "url": "https://openreview.net/forum?id=0NkXZ98BjJ", "year": 2026, "is_main_conference": true, "abstract_snippet": "Surgical triplet detection is a critical task in surgical video analysis, with significant implications for performance assessment and training novice surgeons. However, existing datasets like CholecT50 lack precise spatial bounding box annotations, rendering triplet classification at the image level insufficient for practical applications. The inclusion of bounding box annotations is essential to make this task meaningful, as they provide the spatial context necessary for accurate analysis and improved model generalizability. To address these shortcomings, we introduce ProstaTD, a large-scale, multi-institutional dataset for surgical triplet detection, developed from the technically demanding domain of robot-assisted prostatectomy. ProstaTD offers clinically defined temporal boundaries and high-precision bounding box annotations for each structured triplet activity. The dataset comprises 71,775 video frames and 196,490 annotated triplet instances, collected from 21 surgeries performed across multiple institutions, reflecting a broad range of surgical practices and intraoperative conditions. The annotation process was conducted under rigorous medical supervision and involved more than 60 contributors, including practicing surgeons and medically trained annotators, through multiple iterative phases of labeling and verification. To further facilitate future general-purpose surgical annotation, we developed two tailored labeling tools to improve efficiency and scalability in our annotation workflows. In addition, we created a surgical triplet detection evaluation toolkit that enables standardized and reproducible performance assessment across studies. ProstaTD is the largest and most diverse surgical triplet dataset to date, moving the field from simple classification to full detection with precise spatial and temporal boundaries and thereby providing a robust foundation for fair benchmarking.", "abstract": "Surgical triplet detection is a critical task in surgical video analysis, with significant implications for performance assessment and training novice surgeons. However, existing datasets like CholecT50 lack precise spatial bounding box annotations, rendering triplet classification at the image level insufficient for practical applications. The inclusion of bounding box annotations is essential to make this task meaningful, as they provide the spatial context necessary for accurate analysis and improved model generalizability. To address these shortcomings, we introduce ProstaTD, a large-scale, multi-institutional dataset for surgical triplet detection, developed from the technically demanding domain of robot-assisted prostatectomy. ProstaTD offers clinically defined temporal boundaries and high-precision bounding box annotations for each structured triplet activity. The dataset comprises 71,775 video frames and 196,490 annotated triplet instances, collected from 21 surgeries performed across multiple institutions, reflecting a broad range of surgical practices and intraoperative conditions. The annotation process was conducted under rigorous medical supervision and involved more than 60 contributors, including practicing surgeons and medically trained annotators, through multiple iterative phases of labeling and verification. To further facilitate future general-purpose surgical annotation, we developed two tailored labeling tools to improve efficiency and scalability in our annotation workflows. In addition, we created a surgical triplet detection evaluation toolkit that enables standardized and reproducible performance assessment across studies. ProstaTD is the largest and most diverse surgical triplet dataset to date, moving the field from simple classification to full detection with precise spatial and temporal boundaries and thereby providing a robust foundation for fair benchmarking.", "abstract_source_venue": "OpenReviewAPI", "abstract_source_url": "https://api2.openreview.net/notes?id=0NkXZ98BjJ", "openreview_id": "0NkXZ98BjJ", "openreview_forum_id": "0NkXZ98BjJ", "authors": [], "pdf_url": "https://openreview.net/pdf/2c909942f08c64bdaeed29ff3e4f40b0c007aefc.pdf", "summary_cn": "ProstaTD是首个大规模、多机构的手术三元组检测数据集，提供精确时空边界框标注，推动领域从分类转向全监督检测，支持标准化评估。", "keywords": ["手术三元组检测", "数据集", "边界框标注", "机器人辅助前列腺切除术", "多机构", "评估工具包"], "triple": {"method": "开发ProstaTD数据集与标注工具", "result": "包含71,775帧和196,490标注实例，支持精确检测", "contribution": "提供首个大规模手术三元组检测基准，促进领域发展"}}
{"venue": "ICLR", "search_title": "Towards Text-Mask Consistency in Medical Image Segmentation", "full_title": "Towards Text-Mask Consistency in Medical Image Segmentation", "url": "https://openreview.net/forum?id=riOevy2RwZ", "year": 2026, "is_main_conference": true, "abstract_snippet": "Vision-language models for medical image segmentation often produce masks that conflict with the accompanying text, especially under multi-site/multi-lesion descriptions. We trace this failure to two factors: (i) highly templated and repetitive clinical language causes one-to-one hard contrastive learning to yield numerous false negatives, weakening cross-modal alignment; and (ii) predominantly vision-driven, one-way cross-attention lacks a language-dominant, spatially aware pathway, hindering effective injection of textual semantics into the spatial visual domain. To this end, we propose Consistency-enhanced Two-stage Segmentation (C2Seg). In the pretraining stage, Cluster-aware Contrastive Learning uses a frozen strong baseline to construct an intra-batch text similarity matrix as soft labels, thereby alleviating false negative conflicts and producing more discriminative visual representations. In the fusion stage, we introduce a Bidirectional Complementary Attention Module, where each modality dominates attention along its own path, fostering deep interaction and structural consistency between visual and textual representations. In order to enhance the expressive power of multimodal features, we further adopt KAN-based Attention Gating. Without updating the language encoder, our approach significantly improves text-mask consistency and segmentation accuracy on four public medical imaging datasets. Code is provided in the supplementary material.", "abstract": "Vision-language models for medical image segmentation often produce masks that conflict with the accompanying text, especially under multi-site/multi-lesion descriptions. We trace this failure to two factors: (i) highly templated and repetitive clinical language causes one-to-one hard contrastive learning to yield numerous false negatives, weakening cross-modal alignment; and (ii) predominantly vision-driven, one-way cross-attention lacks a language-dominant, spatially aware pathway, hindering effective injection of textual semantics into the spatial visual domain. To this end, we propose Consistency-enhanced Two-stage Segmentation (C2Seg). In the pretraining stage, Cluster-aware Contrastive Learning uses a frozen strong baseline to construct an intra-batch text similarity matrix as soft labels, thereby alleviating false negative conflicts and producing more discriminative visual representations. In the fusion stage, we introduce a Bidirectional Complementary Attention Module, where each modality dominates attention along its own path, fostering deep interaction and structural consistency between visual and textual representations. In order to enhance the expressive power of multimodal features, we further adopt KAN-based Attention Gating. Without updating the language encoder, our approach significantly improves text-mask consistency and segmentation accuracy on four public medical imaging datasets. Code is provided in the supplementary material.", "abstract_source_venue": "OpenReviewAPI", "abstract_source_url": "https://api2.openreview.net/notes?id=riOevy2RwZ", "openreview_id": "riOevy2RwZ", "openreview_forum_id": "riOevy2RwZ", "authors": [], "pdf_url": "https://openreview.net/pdf/134e94d0dd7e5251464f348554a30d11034c3f4f.pdf", "summary_cn": "针对医学图像分割中文本与掩码不一致问题，提出C2Seg方法，通过聚类感知对比学习和双向互补注意力模块，提升跨模态对齐与分割精度。", "keywords": ["医学图像分割", "视觉语言模型", "跨模态对齐", "对比学习", "注意力机制", "文本掩码一致性"], "triple": {"method": "C2Seg（聚类感知对比学习与双向互补注意力）", "result": "提升文本掩码一致性与分割准确率", "contribution": "增强多模态医学图像分割的跨模态对齐"}}
{"venue": "ICLR", "search_title": "CARE: Towards Clinical Accountability in Multi-Modal Medical Reasoning with an Evidence-Grounded Agentic Framework", "full_title": "CARE: Towards Clinical Accountability in Multi-Modal Medical Reasoning with an Evidence-Grounded Agentic Framework", "url": "https://openreview.net/forum?id=whRAOJiyHM", "year": 2026, "is_main_conference": true, "abstract_snippet": "Large visual language models (VLMs) have shown strong multi-modal medical reasoning ability, but most operate as end-to-end black boxes, diverging from clinicians’ evidence-based, staged workflows and hindering clinical accountability. Complementarily, expert visual grounding models can accurately localize regions of interest (ROIs), providing explicit, reliable evidence that improves both reasoning accuracy and trust. In this paper, we introduce **CARE**, advancing **C**linical **A**ccountability in multi-modal medical **R**easoning with an **E**vidence-grounded agentic framework. Unlike existing approaches that couple grounding and reasoning within a single generalist model, CARE decomposes the task into coordinated sub-modules to reduce shortcut learning and hallucination: a compact VLM proposes relevant medical entities; an expert entity-referring segmentation model produces pixel-level ROI evidence; and a grounded VLM reasons over the full image augmented by ROI hints. The VLMs are optimized with reinforcement learning with verifiable rewards to align answers with supporting evidence. Furthermore, a VLM coordinator plans tool invocation and reviews evidence-answer consistency, providing agentic control and final verification. Evaluated on standard medical VQA benchmarks, our **CARE-Flow** (coordinator-free) improves average accuracy by **10.9%** over the same size (10B) state-of-the-art (SOTA). With dynamic planning and answer review, our **CARE-Coord** yields a further gain, outperforming the heavily pre-trained SOTA by **5.2%**. Our experiments demonstrate that an agentic framework that emulates clinical workflows, incorporating decoupled specialized models and explicit evidence, yields more accurate and accountable medical AI.", "abstract": "Large visual language models (VLMs) have shown strong multi-modal medical reasoning ability, but most operate as end-to-end black boxes, diverging from clinicians’ evidence-based, staged workflows and hindering clinical accountability. Complementarily, expert visual grounding models can accurately localize regions of interest (ROIs), providing explicit, reliable evidence that improves both reasoning accuracy and trust. In this paper, we introduce **CARE**, advancing **C**linical **A**ccountability in multi-modal medical **R**easoning with an **E**vidence-grounded agentic framework. Unlike existing approaches that couple grounding and reasoning within a single generalist model, CARE decomposes the task into coordinated sub-modules to reduce shortcut learning and hallucination: a compact VLM proposes relevant medical entities; an expert entity-referring segmentation model produces pixel-level ROI evidence; and a grounded VLM reasons over the full image augmented by ROI hints. The VLMs are optimized with reinforcement learning with verifiable rewards to align answers with supporting evidence. Furthermore, a VLM coordinator plans tool invocation and reviews evidence-answer consistency, providing agentic control and final verification. Evaluated on standard medical VQA benchmarks, our **CARE-Flow** (coordinator-free) improves average accuracy by **10.9%** over the same size (10B) state-of-the-art (SOTA). With dynamic planning and answer review, our **CARE-Coord** yields a further gain, outperforming the heavily pre-trained SOTA by **5.2%**. Our experiments demonstrate that an agentic framework that emulates clinical workflows, incorporating decoupled specialized models and explicit evidence, yields more accurate and accountable medical AI.", "abstract_source_venue": "OpenReviewAPI", "abstract_source_url": "https://api2.openreview.net/notes?id=whRAOJiyHM", "openreview_id": "whRAOJiyHM", "openreview_forum_id": "whRAOJiyHM", "authors": [], "pdf_url": "https://openreview.net/pdf/5755ff044220ed351936c6120448bd372cd6498d.pdf", "summary_cn": "CARE框架通过分解任务、引入专家分割模型和强化学习，提升多模态医疗推理的准确性和临床可问责性，在基准测试中显著优于现有方法。", "keywords": ["多模态医疗推理", "临床可问责性", "证据基础框架", "专家分割模型", "强化学习", "智能体协调"], "triple": {"method": "分解任务为实体提议、分割和推理模块，结合强化学习与智能体协调", "result": "CARE-Flow准确率提升10.9%，CARE-Coord进一步超越SOTA 5.2%", "contribution": "提出模拟临床工作流的智能体框架，增强医疗AI的准确性与可问责性"}}
{"venue": "ICLR", "search_title": "Resp-Agent: An Agent-Based System for Multimodal Respiratory Sound Generation and Disease Diagnosis", "full_title": "Resp-Agent: An Agent-Based System for Multimodal Respiratory Sound Generation and Disease Diagnosis", "url": "https://openreview.net/forum?id=ZkoojtEm3W", "year": 2026, "is_main_conference": true, "abstract_snippet": "Deep learning-based respiratory auscultation is currently hindered by two fundamental disconnects: the representation gap, where compressing signals into spectrograms discards transient acoustic events and clinical context, and the data gap, characterized by severe class imbalance and scarcity. To bridge these gaps, we present **_Resp-Agent_**, an autonomous multimodal system orchestrated by a novel Active Adversarial Curriculum Agent (Thinker-A²CA). Unlike static pipelines, the Thinker-A²CA acts as a central controller that actively identifies diagnostic weaknesses and schedules targeted synthesis in a closed loop. Under this unified orchestration, we propose two specialized architectural solutions. First, to address the representation gap, we introduce a Modality Weaving Diagnoser. This module moves beyond standard fusion by explicitly interleaving electronic health records (EHR) with audio tokens and employs Strategic Global Attention to capture long-range clinical dependencies while retaining sensitivity to millisecond-level transient events via sparse audio anchors. Second, to resolve the data gap, we design a Flow Matching Generator that retools a text-only Large Language Model (LLM) via modality injection. Guided by the Thinker-A²CA, this generator decouples pathological content from acoustic style to programmatically synthesize high-fidelity, hard-to-diagnose samples that remedy the system’s boundary errors. To support this work, we construct **_Resp-229k_**, a benchmark corpus of 229k recordings paired with LLM-distilled clinical narratives. Extensive experiments demonstrate that our agentic co-design consistently outperforms prior approaches, advancing robust and deployable respiratory intelligence. Data and code will be released upon acceptance.", "abstract": "Deep learning-based respiratory auscultation is currently hindered by two fundamental disconnects: the representation gap, where compressing signals into spectrograms discards transient acoustic events and clinical context, and the data gap, characterized by severe class imbalance and scarcity. To bridge these gaps, we present **_Resp-Agent_**, an autonomous multimodal system orchestrated by a novel Active Adversarial Curriculum Agent (Thinker-A²CA). Unlike static pipelines, the Thinker-A²CA acts as a central controller that actively identifies diagnostic weaknesses and schedules targeted synthesis in a closed loop. Under this unified orchestration, we propose two specialized architectural solutions. First, to address the representation gap, we introduce a Modality Weaving Diagnoser. This module moves beyond standard fusion by explicitly interleaving electronic health records (EHR) with audio tokens and employs Strategic Global Attention to capture long-range clinical dependencies while retaining sensitivity to millisecond-level transient events via sparse audio anchors. Second, to resolve the data gap, we design a Flow Matching Generator that retools a text-only Large Language Model (LLM) via modality injection. Guided by the Thinker-A²CA, this generator decouples pathological content from acoustic style to programmatically synthesize high-fidelity, hard-to-diagnose samples that remedy the system’s boundary errors. To support this work, we construct **_Resp-229k_**, a benchmark corpus of 229k recordings paired with LLM-distilled clinical narratives. Extensive experiments demonstrate that our agentic co-design consistently outperforms prior approaches, advancing robust and deployable respiratory intelligence. Data and code will be released upon acceptance.", "abstract_source_venue": "OpenReviewAPI", "abstract_source_url": "https://api2.openreview.net/notes?id=ZkoojtEm3W", "openreview_id": "ZkoojtEm3W", "openreview_forum_id": "ZkoojtEm3W", "authors": [], "pdf_url": "https://openreview.net/pdf/651f4a33c71e5e7e4bb800b593c05b685f65f94d.pdf", "summary_cn": "提出Resp-Agent系统，通过主动对抗课程代理协调多模态诊断与生成，解决呼吸音诊断中的表示与数据鸿沟，提升诊断性能。", "keywords": ["呼吸音诊断", "多模态学习", "主动对抗课程", "数据生成", "表示学习", "电子健康记录"], "triple": {"method": "主动对抗课程代理协调多模态诊断与生成", "result": "系统性能超越现有方法", "contribution": "解决表示与数据鸿沟，提升呼吸音诊断鲁棒性"}}
{"venue": "ICLR", "search_title": "Random Anchors with Low-rank Decorrelated Learning: A Minimalist Pipeline for Class-Incremental Medical Image Classification", "full_title": "Random Anchors with Low-rank Decorrelated Learning: A Minimalist Pipeline for Class-Incremental Medical Image Classification", "url": "https://openreview.net/forum?id=mduCc7XKXH", "year": 2026, "is_main_conference": true, "abstract_snippet": "Class-incremental learning (CIL) in medical image-guided diagnosis requires models to preserve knowledge of historical disease classes while adapting to emerging categories. Pre-trained models (PTMs) with well-generalized features provide a strong foundation, yet most PTM-based CIL strategies, such as prompt tuning, task-specific adapters and model mixtures, rely on increasingly complex designs. While effective in general-domain benchmarks, these methods falter in medical imaging, where low intra-class variability and high inter-domain shifts (from scanners, protocols and institutions) make CIL particularly prone to representation collapse and domain misalignment. Under such conditions, we find that lightweight representation calibration strategies, often dismissed in general-domain CIL for their modest gains, can be remarkably effective for adapting PTMs in medical settings. To this end, we introduce Random Anchors with Low-rank Decorrelated Learning (RA-LDL), a minimalist representation-based framework that combines (a) PTM-based feature extraction with optional ViT-Adapter tuning, (b) feature calibration via frozen Random Anchor projection and a single-session-trained Low-Rank Projection (LRP), and (c) analytical closed-form decorrelated learning. The entire pipeline requires only one training session and minimal task-specific tuning, making it appealing for efficient deployment. Despite its simplicity, RA-LDL achieves consistent and substantial improvements across both general-domain and medical-specific PTMs, and outperforms recent state-of-the-art methods on four diverse medical imaging datasets. These results highlight that minimalist representation recalibration, rather than complex architectural modifications, can unlock the underexplored potential of PTMs in medical CIL. We hope this work establishes a practical and extensible foundation for future research in class-incremental image-guided diagnosis. Code will be made publicly available.", "abstract": "Class-incremental learning (CIL) in medical image-guided diagnosis requires models to preserve knowledge of historical disease classes while adapting to emerging categories. Pre-trained models (PTMs) with well-generalized features provide a strong foundation, yet most PTM-based CIL strategies, such as prompt tuning, task-specific adapters and model mixtures, rely on increasingly complex designs. While effective in general-domain benchmarks, these methods falter in medical imaging, where low intra-class variability and high inter-domain shifts (from scanners, protocols and institutions) make CIL particularly prone to representation collapse and domain misalignment. Under such conditions, we find that lightweight representation calibration strategies, often dismissed in general-domain CIL for their modest gains, can be remarkably effective for adapting PTMs in medical settings. To this end, we introduce Random Anchors with Low-rank Decorrelated Learning (RA-LDL), a minimalist representation-based framework that combines (a) PTM-based feature extraction with optional ViT-Adapter tuning, (b) feature calibration via frozen Random Anchor projection and a single-session-trained Low-Rank Projection (LRP), and (c) analytical closed-form decorrelated learning. The entire pipeline requires only one training session and minimal task-specific tuning, making it appealing for efficient deployment. Despite its simplicity, RA-LDL achieves consistent and substantial improvements across both general-domain and medical-specific PTMs, and outperforms recent state-of-the-art methods on four diverse medical imaging datasets. These results highlight that minimalist representation recalibration, rather than complex architectural modifications, can unlock the underexplored potential of PTMs in medical CIL. We hope this work establishes a practical and extensible foundation for future research in class-incremental image-guided diagnosis. Code will be made publicly available.", "abstract_source_venue": "OpenReviewAPI", "abstract_source_url": "https://api2.openreview.net/notes?id=mduCc7XKXH", "openreview_id": "mduCc7XKXH", "openreview_forum_id": "mduCc7XKXH", "authors": [], "pdf_url": "https://openreview.net/pdf/4635dbbdf87223e050131fe34fbd5ace48241da8.pdf", "summary_cn": "提出RA-LDL框架，通过随机锚点和低秩解相关学习，以极简设计实现医学图像类增量分类，在多个数据集上超越现有方法。", "keywords": ["类增量学习", "医学图像分类", "预训练模型", "表示校准", "低秩投影", "解相关学习"], "triple": {"method": "随机锚点与低秩解相关学习", "result": "在四个医学影像数据集上超越先进方法", "contribution": "为医学类增量学习提供极简有效框架"}}
{"venue": "ICLR", "search_title": "Rethinking Model Calibration through Spectral Entropy Regularization in Medical Image Segmentation", "full_title": "Rethinking Model Calibration through Spectral Entropy Regularization in Medical Image Segmentation", "url": "https://openreview.net/forum?id=SOFSVaZXSj", "year": 2026, "is_main_conference": true, "abstract_snippet": "Deep neural networks for medical image segmentation often produce overconfident predictions, posing clinical risks due to miscalibrated uncertainty estimates. In this work, we rethink model calibration from a frequency-domain perspective and identify two critical factors causing miscalibration: spectral bias, where models overemphasize low-frequency components, and confidence saturation, which suppresses overall power spectral density in confidence maps. To address these challenges, we propose a novel frequency-aware calibration framework integrating spectral entropy regularization and power spectral smoothing. The spectral entropy term promotes a balanced frequency spectrum and enhances overall spectral power,  enabling better modeling of high-frequency boundary and low-frequency structural uncertainty. The smoothing module stabilizes frequency-wise statistics across training batches, reducing sample-specific fluctuations. Extensive experiments on six public medical imaging datasets and multiple segmentation architectures demonstrate that our approach consistently improves calibration metrics without sacrificing segmentation accuracy.", "abstract": "Deep neural networks for medical image segmentation often produce overconfident predictions, posing clinical risks due to miscalibrated uncertainty estimates. In this work, we rethink model calibration from a frequency-domain perspective and identify two critical factors causing miscalibration: spectral bias, where models overemphasize low-frequency components, and confidence saturation, which suppresses overall power spectral density in confidence maps. To address these challenges, we propose a novel frequency-aware calibration framework integrating spectral entropy regularization and power spectral smoothing. The spectral entropy term promotes a balanced frequency spectrum and enhances overall spectral power,  enabling better modeling of high-frequency boundary and low-frequency structural uncertainty. The smoothing module stabilizes frequency-wise statistics across training batches, reducing sample-specific fluctuations. Extensive experiments on six public medical imaging datasets and multiple segmentation architectures demonstrate that our approach consistently improves calibration metrics without sacrificing segmentation accuracy.", "abstract_source_venue": "OpenReviewAPI", "abstract_source_url": "https://api2.openreview.net/notes?id=SOFSVaZXSj", "openreview_id": "SOFSVaZXSj", "openreview_forum_id": "SOFSVaZXSj", "authors": [], "pdf_url": "https://openreview.net/pdf/3edf752b836f423bf28a6a0183ab178f54a9dc61.pdf", "summary_cn": "提出基于频域视角的医学图像分割模型校准框架，通过谱熵正则化和功率谱平滑解决谱偏置和置信饱和问题，提升不确定性估计准确性。", "keywords": ["模型校准", "医学图像分割", "谱熵正则化", "频域分析", "不确定性估计", "深度学习"], "triple": {"method": "谱熵正则化与功率谱平滑", "result": "校准指标提升，分割精度未下降", "contribution": "频域视角校准框架"}}
{"venue": "ICLR", "search_title": "COMPASS: Robust Feature Conformal Prediction for Medical Segmentation Metrics", "full_title": "COMPASS: Robust Feature Conformal Prediction for Medical Segmentation Metrics", "url": "https://openreview.net/forum?id=uBy4TCgGiT", "year": 2026, "is_main_conference": true, "abstract_snippet": "In clinical applications, the utility of segmentation models is often based on the accuracy of derived downstream metrics such as organ size, rather than by the pixel-level accuracy of the segmentation masks themselves. Thus, uncertainty quantification for such metrics is crucial for decision-making. Conformal prediction (CP) is a popular framework to derive such principled uncertainty guarantees, but applying CP naively to the final scalar metric is inefficient because it treats the complex, non-linear segmentation-to-metric pipeline as a black box. We introduce COMPASS, a practical framework that generates efficient, metric-based CP intervals for image segmentation models by leveraging the inductive biases of their underlying deep neural networks. COMPASS performs calibration directly in the model's representation space by perturbing intermediate features along low-dimensional subspaces maximally sensitive to the target metric. We prove that COMPASS achieves valid marginal coverage under the assumption of exchangeability. Empirically, we demonstrate that COMPASS produces significantly tighter intervals than traditional CP baselines on four medical image segmentation tasks for area estimation of skin lesions and anatomical structures. Furthermore, we show that leveraging learned internal features to estimate importance weights allows COMPASS to also recover target coverage under covariate shifts. COMPASS paves the way for practical, metric-based uncertainty quantification for medical image segmentation.", "abstract": "In clinical applications, the utility of segmentation models is often based on the accuracy of derived downstream metrics such as organ size, rather than by the pixel-level accuracy of the segmentation masks themselves. Thus, uncertainty quantification for such metrics is crucial for decision-making. Conformal prediction (CP) is a popular framework to derive such principled uncertainty guarantees, but applying CP naively to the final scalar metric is inefficient because it treats the complex, non-linear segmentation-to-metric pipeline as a black box. We introduce COMPASS, a practical framework that generates efficient, metric-based CP intervals for image segmentation models by leveraging the inductive biases of their underlying deep neural networks. COMPASS performs calibration directly in the model's representation space by perturbing intermediate features along low-dimensional subspaces maximally sensitive to the target metric. We prove that COMPASS achieves valid marginal coverage under the assumption of exchangeability. Empirically, we demonstrate that COMPASS produces significantly tighter intervals than traditional CP baselines on four medical image segmentation tasks for area estimation of skin lesions and anatomical structures. Furthermore, we show that leveraging learned internal features to estimate importance weights allows COMPASS to also recover target coverage under covariate shifts. COMPASS paves the way for practical, metric-based uncertainty quantification for medical image segmentation.", "abstract_source_venue": "OpenReviewAPI", "abstract_source_url": "https://api2.openreview.net/notes?id=uBy4TCgGiT", "openreview_id": "uBy4TCgGiT", "openreview_forum_id": "uBy4TCgGiT", "authors": [], "pdf_url": "https://openreview.net/pdf/be0d7cb1bd9858b9f20a14c4bdbd95eeefc05290.pdf", "summary_cn": "COMPASS提出一种医学图像分割不确定性量化框架，通过在特征空间扰动生成高效置信区间，提升下游指标预测的鲁棒性。", "keywords": ["医学图像分割", "不确定性量化", "保形预测", "特征扰动", "下游指标", "协变量偏移"], "triple": {"method": "特征空间扰动与保形预测", "result": "置信区间更紧，覆盖目标指标", "contribution": "提升医学分割指标不确定性量化效率"}}
{"venue": "ICLR", "search_title": "Repurposing Foundation Model for Generalizable Medical Time Series Classification", "full_title": "Repurposing Foundation Model for Generalizable Medical Time Series Classification", "url": "https://openreview.net/forum?id=wNEzRYiyZM", "year": 2026, "is_main_conference": true, "abstract_snippet": "Medical time series (MedTS) classification suffers from poor generalizability\nin real-world deployment due to inter- and intra-dataset heterogeneity, such as varying\nnumbers of channels, signal lengths, task definitions, and patient characteristics.\n% implicit patient characteristics, variable channel configurations, time series lengths, and diagnostic tasks.\nTo address this, we propose FORMED, a novel framework for repurposing a backbone foundation model, pre-trained on generic time series, to enable highly generalizable MedTS classification on unseen datasets.\nFORMED combines the backbone with a novel classifier comprising two components: (1) task-specific channel embeddings and label queries, dynamically sized to match any number of channels and target classes, and (2) a shared decoding attention layer, jointly trained across datasets to capture medical domain knowledge through task-agnostic feature-query interactions. After repurposing, FORMED achieves seamless adaptation to unseen MedTS datasets through lightweight label query training (0.1\\% of parameters), eliminating the need for full fine-tuning or architectural redesign.\nWe evaluate FORMED on 5 diverse MedTS datasets, benchmarking against 11 Task-Specific Models (TSM) and 4 Task-Specific Adaptation (TSA) methods. Our results demonstrate FORMED's dominant performance, achieving up to 35\\% absolute improvement in F1-score (on ADFTD dataset) over specialized baselines.\nBy decoupling domain-invariant representation learning from task-specific adaptation, FORMED establishes a scalable and resource-efficient paradigm for foundation model repurposing in healthcare. This approach prioritizes clinical adaptability over rigid task-centric design, offering a practical pathway for real-world implementation.", "abstract": "Medical time series (MedTS) classification suffers from poor generalizability\nin real-world deployment due to inter- and intra-dataset heterogeneity, such as varying\nnumbers of channels, signal lengths, task definitions, and patient characteristics.\n% implicit patient characteristics, variable channel configurations, time series lengths, and diagnostic tasks.\nTo address this, we propose FORMED, a novel framework for repurposing a backbone foundation model, pre-trained on generic time series, to enable highly generalizable MedTS classification on unseen datasets.\nFORMED combines the backbone with a novel classifier comprising two components: (1) task-specific channel embeddings and label queries, dynamically sized to match any number of channels and target classes, and (2) a shared decoding attention layer, jointly trained across datasets to capture medical domain knowledge through task-agnostic feature-query interactions. After repurposing, FORMED achieves seamless adaptation to unseen MedTS datasets through lightweight label query training (0.1\\% of parameters), eliminating the need for full fine-tuning or architectural redesign.\nWe evaluate FORMED on 5 diverse MedTS datasets, benchmarking against 11 Task-Specific Models (TSM) and 4 Task-Specific Adaptation (TSA) methods. Our results demonstrate FORMED's dominant performance, achieving up to 35\\% absolute improvement in F1-score (on ADFTD dataset) over specialized baselines.\nBy decoupling domain-invariant representation learning from task-specific adaptation, FORMED establishes a scalable and resource-efficient paradigm for foundation model repurposing in healthcare. This approach prioritizes clinical adaptability over rigid task-centric design, offering a practical pathway for real-world implementation.", "abstract_source_venue": "OpenReviewAPI", "abstract_source_url": "https://api2.openreview.net/notes?id=wNEzRYiyZM", "openreview_id": "wNEzRYiyZM", "openreview_forum_id": "wNEzRYiyZM", "authors": [], "pdf_url": "https://openreview.net/pdf/b8992f1f53acaa99ad82b32ab99ea8f24e9e2451.pdf", "summary_cn": "提出FORMED框架，通过重用基础模型和动态分类器，实现医疗时间序列分类的高泛化性，仅需少量参数适应新数据集。", "keywords": ["医疗时间序列分类", "基础模型重用", "泛化性", "动态分类器", "轻量适应", "跨数据集学习"], "triple": {"method": "重用基础模型与动态分类器", "result": "在5个数据集上优于基线，F1分数提升达35%", "contribution": "建立可扩展、资源高效的医疗基础模型重用范式"}}
{"venue": "ICLR", "search_title": "ATPO: ADAPTIVE TREE POLICY OPTIMIZATION FOR MULTI-TURN MEDICAL DIALOGUE", "full_title": "ATPO: ADAPTIVE TREE POLICY OPTIMIZATION FOR MULTI-TURN MEDICAL DIALOGUE", "url": "https://openreview.net/forum?id=2bv3B8B9bl", "year": 2026, "is_main_conference": true, "abstract_snippet": "Effective information seeking in multi-turn medical dialogues is critical for accurate diagnosis, especially when dealing with incomplete information. Aligning Large Language Models (LLMs) for these interactive scenarios is challenging due to the uncertainty inherent in user-agent interactions, which we formulate as a Hierarchical Markov Decision Process (H-MDP). While conventional Reinforcement Learning (RL) methods like Group Relative Policy Optimization (GRPO) struggle with long-horizon credit assignment and Proximal Policy Optimization (PPO) suffers from unstable value estimation in this context, we propose a novel uncertainty-aware Adaptive Tree Policy Optimization (ATPO) algorithm. Our method adaptively allocates the rollout budget to states with high uncertainty, quantified by a composite metric of Bellman error and action-value variance. This strategy enables more accurate value estimation, while fostering more efficient and diverse exploration. To mitigate the high computational cost of tree-based RL, we introduce two key optimizations: an uncertainty-guided pruning mechanism to minimize the number of rollouts, and an asynchronous search architecture that leverages KV cache reuse to maximize inference throughput. Extensive experiments on three public medical dialogue benchmarks demonstrate that our algorithm significantly outperforms several strong baselines, culminating in Qwen3-8B model surpassing the much larger GPT-4o (+0.92% accuracy).", "abstract": "Effective information seeking in multi-turn medical dialogues is critical for accurate diagnosis, especially when dealing with incomplete information. Aligning Large Language Models (LLMs) for these interactive scenarios is challenging due to the uncertainty inherent in user-agent interactions, which we formulate as a Hierarchical Markov Decision Process (H-MDP). While conventional Reinforcement Learning (RL) methods like Group Relative Policy Optimization (GRPO) struggle with long-horizon credit assignment and Proximal Policy Optimization (PPO) suffers from unstable value estimation in this context, we propose a novel uncertainty-aware Adaptive Tree Policy Optimization (ATPO) algorithm. Our method adaptively allocates the rollout budget to states with high uncertainty, quantified by a composite metric of Bellman error and action-value variance. This strategy enables more accurate value estimation, while fostering more efficient and diverse exploration. To mitigate the high computational cost of tree-based RL, we introduce two key optimizations: an uncertainty-guided pruning mechanism to minimize the number of rollouts, and an asynchronous search architecture that leverages KV cache reuse to maximize inference throughput. Extensive experiments on three public medical dialogue benchmarks demonstrate that our algorithm significantly outperforms several strong baselines, culminating in Qwen3-8B model surpassing the much larger GPT-4o (+0.92% accuracy).", "abstract_source_venue": "OpenReviewAPI", "abstract_source_url": "https://api2.openreview.net/notes?id=2bv3B8B9bl", "openreview_id": "2bv3B8B9bl", "openreview_forum_id": "2bv3B8B9bl", "authors": [], "pdf_url": "https://openreview.net/pdf/395d01fff7735f828ef2f7ac13031b841b7af2f4.pdf", "summary_cn": "提出ATPO算法，通过自适应分配探索预算和优化计算，提升多轮医疗对话中LLM的信息寻求能力，在多个基准上超越基线模型。", "keywords": ["自适应树策略优化", "多轮医疗对话", "不确定性感知", "强化学习", "大语言模型", "计算优化"], "triple": {"method": "ATPO算法（自适应树策略优化）", "result": "在三个医疗对话基准上超越基线，Qwen3-8B模型准确率超过GPT-4o", "contribution": "提升多轮医疗对话中LLM的信息寻求准确性和效率"}}
{"venue": "ICLR", "search_title": "Cross-Timestep: 3D Diffusion Model with Trans-temporal Memory LSTM and Adaptive Priori Decoding Strategy for Medical Segmentation", "full_title": "Cross-Timestep: 3D Diffusion Model with Trans-temporal Memory LSTM and Adaptive Priori Decoding Strategy for Medical Segmentation", "url": "https://openreview.net/forum?id=TE3asYO8PQ", "year": 2026, "is_main_conference": true, "abstract_snippet": "Diffusion models have recently demonstrated significant robustness in medical image segmentation, effectively accommodating variations across different imaging styles. However, their applications remain limited due to: (i) current successes being primarily confined to 2D segmentation tasks—we observe that diffusion models tend to collapse at the early stage when applied to 3D medical tasks; and (ii) the inherently isolated iteration along timesteps during training and inference. To tackle these limitations, we propose a novel framework named Cross-Timestep, which incorporates two key innovations: an Adaptive Priori Decoding Strategy (APDS) and a trans-temporal memory LSTM (tLSTM) mechanism. (i) The APDS provides prior guidance during the diffusion process by employing a Priori Decoder(PD) that focuses solely on the conditional branch, successfully stabilizing the reverse diffusion process. (ii) The tLSTM integrates convolution and linear layers into the LSTM gating structure, and enhances the memory cell mechanism to retain temporal state, explicitly preserving and propagating continuous temporal states across timesteps. Experimental results demonstrate that Cross-Timestep performs favorably on heterogeneous 3D medical datasets. Three experiments further analyze the collapse phenomenon in 3D medical diffusion models and validate that APDS effectively prevents initial-stage collapse without excessively constraining the model, while tLSTM facilitates the performance and scalability of diffusion models.", "abstract": "Diffusion models have recently demonstrated significant robustness in medical image segmentation, effectively accommodating variations across different imaging styles. However, their applications remain limited due to: (i) current successes being primarily confined to 2D segmentation tasks—we observe that diffusion models tend to collapse at the early stage when applied to 3D medical tasks; and (ii) the inherently isolated iteration along timesteps during training and inference. To tackle these limitations, we propose a novel framework named Cross-Timestep, which incorporates two key innovations: an Adaptive Priori Decoding Strategy (APDS) and a trans-temporal memory LSTM (tLSTM) mechanism. (i) The APDS provides prior guidance during the diffusion process by employing a Priori Decoder(PD) that focuses solely on the conditional branch, successfully stabilizing the reverse diffusion process. (ii) The tLSTM integrates convolution and linear layers into the LSTM gating structure, and enhances the memory cell mechanism to retain temporal state, explicitly preserving and propagating continuous temporal states across timesteps. Experimental results demonstrate that Cross-Timestep performs favorably on heterogeneous 3D medical datasets. Three experiments further analyze the collapse phenomenon in 3D medical diffusion models and validate that APDS effectively prevents initial-stage collapse without excessively constraining the model, while tLSTM facilitates the performance and scalability of diffusion models.", "abstract_source_venue": "OpenReviewAPI", "abstract_source_url": "https://api2.openreview.net/notes?id=TE3asYO8PQ", "openreview_id": "TE3asYO8PQ", "openreview_forum_id": "TE3asYO8PQ", "authors": [], "pdf_url": "https://openreview.net/pdf/16572ba3a99ea04f2196d08d1577d9b17d94324f.pdf", "summary_cn": "提出Cross-Timestep框架，结合自适应先验解码策略和跨时间记忆LSTM，解决3D医学扩散模型早期崩溃问题，提升分割鲁棒性。", "keywords": ["3D医学分割", "扩散模型", "自适应先验解码", "跨时间记忆LSTM", "模型稳定性", "异构数据集"], "triple": {"method": "自适应先验解码策略(APDS)与跨时间记忆LSTM(tLSTM)", "result": "在异构3D医学数据集上表现优异，有效防止早期崩溃", "contribution": "提升3D扩散模型稳定性与性能"}}
{"venue": "ICLR", "search_title": "Dual-Kernel Adapter: Expanding Spatial Horizons for Data-Constrained Medical Image Analysis", "full_title": "Dual-Kernel Adapter: Expanding Spatial Horizons for Data-Constrained Medical Image Analysis", "url": "https://openreview.net/forum?id=Z6KGt1veeP", "year": 2026, "is_main_conference": true, "abstract_snippet": "Adapters have become a widely adopted strategy for efficient fine-tuning of foundation models, particularly in resource-constrained settings. However, their performance under extreme data scarcity—common in medical imaging due to high annotation costs, privacy regulations, and fragmented datasets—remains underexplored. In this work, we present the first comprehensive study of adapter-based fine-tuning for vision foundation models in low-data medical imaging scenarios. We find that, contrary to their promise, conventional Adapters can degrade performance under severe data constraints, performing even worse than simple linear probing when trained on less than 1\\% of the corresponding training data. Through systematic analysis, we identify a sharp reduction in Effective Receptive Field (ERF) as a key factor behind this degradation. Motivated by these findings, we propose the Dual-Kernel Adapter (DKA), a lightweight module that expands spatial context via large-kernel convolutions while preserving local detail with small-kernel counterparts. Extensive experiments across diverse classification and segmentation benchmarks show that DKA significantly outperforms existing Adapter methods, establishing new leading results in both data-constrained and data-rich regimes.", "abstract": "Adapters have become a widely adopted strategy for efficient fine-tuning of foundation models, particularly in resource-constrained settings. However, their performance under extreme data scarcity—common in medical imaging due to high annotation costs, privacy regulations, and fragmented datasets—remains underexplored. In this work, we present the first comprehensive study of adapter-based fine-tuning for vision foundation models in low-data medical imaging scenarios. We find that, contrary to their promise, conventional Adapters can degrade performance under severe data constraints, performing even worse than simple linear probing when trained on less than 1\\% of the corresponding training data. Through systematic analysis, we identify a sharp reduction in Effective Receptive Field (ERF) as a key factor behind this degradation. Motivated by these findings, we propose the Dual-Kernel Adapter (DKA), a lightweight module that expands spatial context via large-kernel convolutions while preserving local detail with small-kernel counterparts. Extensive experiments across diverse classification and segmentation benchmarks show that DKA significantly outperforms existing Adapter methods, establishing new leading results in both data-constrained and data-rich regimes.", "abstract_source_venue": "OpenReviewAPI", "abstract_source_url": "https://api2.openreview.net/notes?id=Z6KGt1veeP", "openreview_id": "Z6KGt1veeP", "openreview_forum_id": "Z6KGt1veeP", "authors": [], "pdf_url": "https://openreview.net/pdf/48fb73f36e655637acbff413c9b570c35666be74.pdf", "summary_cn": "针对医学图像数据稀缺问题，提出双核适配器（DKA），通过大小核卷积扩展空间感受野，在低数据和高数据场景下均优于现有适配器方法。", "keywords": ["医学图像分析", "适配器微调", "数据稀缺", "感受野", "双核卷积", "轻量模块"], "triple": {"method": "双核适配器（DKA）", "result": "在分类和分割任务中显著超越现有适配器方法", "contribution": "提升低数据下视觉基础模型性能"}}
{"venue": "ICLR", "search_title": "Medical thinking with multiple images", "full_title": "Medical thinking with multiple images", "url": "https://openreview.net/forum?id=h2p5eOFpcF", "year": 2026, "is_main_conference": true, "abstract_snippet": "Large language models and vision-language models score high on many medical QA benchmarks; however, real-world clinical reasoning remains challenging because cases often involve multiple images and require cross-view fusion. We present MedThinkVQA, a benchmark that asks models to think with multiple images: read each image, merge evidence across views, and pick a diagnosis with stepwise supervision. We make three parts explicit: multi-image questions, expert-annotated stepwise supervision, and beyond-accuracy evaluation. Only MedThinkVQA combines all these parts in one expert-annotated benchmark. The dataset has 8,481 cases in total, with 751 test cases, and on average 6.51 images per case; it is expert-annotated and, at this level, larger and more image-dense than prior work (earlier maxima < 1.43 images per case). On the test set, GPT-5 achieves 57.39% accuracy, approximately 15 percentage points below the strongest result on the most challenging prior benchmark of a similar kind, while other strong models are lower (Qwen2.5-VL-32B: 39.54%, MedGemma-27B: 37.55%, InternVL3.5-38B: 43.14%). Giving expert findings and summaries brings clear gains, but using models' self-generated ones brings small or negative gains. Step-level evaluation shows where models stumble: errors center on image reading and cross-view integration in both decisive and non-decisive steps (>70%); when a step is decisive for the final choice, reasoning slips become more common (32.26%), while scenario and pure-knowledge slips are relatively rare (<10%). These patterns isolate and quantify the core obstacle: extracting and integrating cross-image evidence, rather than language-only inference.", "abstract": "Large language models and vision-language models score high on many medical QA benchmarks; however, real-world clinical reasoning remains challenging because cases often involve multiple images and require cross-view fusion. We present MedThinkVQA, a benchmark that asks models to think with multiple images: read each image, merge evidence across views, and pick a diagnosis with stepwise supervision. We make three parts explicit: multi-image questions, expert-annotated stepwise supervision, and beyond-accuracy evaluation. Only MedThinkVQA combines all these parts in one expert-annotated benchmark. The dataset has 8,481 cases in total, with 751 test cases, and on average 6.51 images per case; it is expert-annotated and, at this level, larger and more image-dense than prior work (earlier maxima < 1.43 images per case). On the test set, GPT-5 achieves 57.39% accuracy, approximately 15 percentage points below the strongest result on the most challenging prior benchmark of a similar kind, while other strong models are lower (Qwen2.5-VL-32B: 39.54%, MedGemma-27B: 37.55%, InternVL3.5-38B: 43.14%). Giving expert findings and summaries brings clear gains, but using models' self-generated ones brings small or negative gains. Step-level evaluation shows where models stumble: errors center on image reading and cross-view integration in both decisive and non-decisive steps (>70%); when a step is decisive for the final choice, reasoning slips become more common (32.26%), while scenario and pure-knowledge slips are relatively rare (<10%). These patterns isolate and quantify the core obstacle: extracting and integrating cross-image evidence, rather than language-only inference.", "abstract_source_venue": "OpenReviewAPI", "abstract_source_url": "https://api2.openreview.net/notes?id=h2p5eOFpcF", "openreview_id": "h2p5eOFpcF", "openreview_forum_id": "h2p5eOFpcF", "authors": [], "pdf_url": "https://openreview.net/pdf/70b16123ab381aec4897235755e6f1e6926423dc.pdf", "summary_cn": "提出MedThinkVQA基准，要求模型处理多图像临床推理，包含专家标注的逐步监督。测试显示现有模型在图像阅读和跨视图融合上存在显著困难，准确率较低。", "keywords": ["多图像推理", "临床基准", "跨视图融合", "逐步监督", "医学VQA", "专家标注"], "triple": {"method": "构建MedThinkVQA基准，含多图像问题与逐步监督", "result": "GPT-5准确率57.39%，模型在图像阅读与跨视图融合步骤错误率高", "contribution": "首次结合多图像、逐步监督与超准确率评估，量化多图像推理核心障碍"}}
{"venue": "ICLR", "search_title": "Boosting Medical Visual Understanding From Multi-Granular Language Learning", "full_title": "Boosting Medical Visual Understanding From Multi-Granular Language Learning", "url": "https://openreview.net/forum?id=ccjukmExrB", "year": 2026, "is_main_conference": true, "abstract_snippet": "Recent advances in image-text pretraining have significantly enhanced visual understanding by aligning visual and textual representations. Contrastive Language-Image Pretraining (CLIP) has played a pivotal role in multimodal learning. However, its focus on single-label, single-granularity alignment limits its effectiveness in complex domains such as medical imaging, where images often correspond to multiple labels across different levels of granularity. To address this, we propose Multi-Granular Language Learning (MGLL), a contrastive learning framework designed to improve both multi-label and cross-granularity alignment. MGLL leverages structured multi-label supervision, integrates textual descriptions across granularities, and introduces soft-label supervision with point-wise constraints to enhance alignment. MGLL employs smooth Kullback–Leibler (KL) divergence to ensure cross-granularity consistency while maintaining computational efficiency as a plug-and-play module for vision-language models. Pretrained on our constructed large-scale multi-granular datasets and evaluated across multiple datasets, MGLL outperforms other state-of-the-art methods in downstream tasks. The code will be available on GitHub.", "abstract": "Recent advances in image-text pretraining have significantly enhanced visual understanding by aligning visual and textual representations. Contrastive Language-Image Pretraining (CLIP) has played a pivotal role in multimodal learning. However, its focus on single-label, single-granularity alignment limits its effectiveness in complex domains such as medical imaging, where images often correspond to multiple labels across different levels of granularity. To address this, we propose Multi-Granular Language Learning (MGLL), a contrastive learning framework designed to improve both multi-label and cross-granularity alignment. MGLL leverages structured multi-label supervision, integrates textual descriptions across granularities, and introduces soft-label supervision with point-wise constraints to enhance alignment. MGLL employs smooth Kullback–Leibler (KL) divergence to ensure cross-granularity consistency while maintaining computational efficiency as a plug-and-play module for vision-language models. Pretrained on our constructed large-scale multi-granular datasets and evaluated across multiple datasets, MGLL outperforms other state-of-the-art methods in downstream tasks. The code will be available on GitHub.", "abstract_source_venue": "OpenReviewAPI", "abstract_source_url": "https://api2.openreview.net/notes?id=ccjukmExrB", "openreview_id": "ccjukmExrB", "openreview_forum_id": "ccjukmExrB", "authors": [], "pdf_url": "https://openreview.net/pdf/a1b1dc07736c81471788af3503db007b695c4246.pdf", "summary_cn": "提出多粒度语言学习框架MGLL，通过多标签监督和跨粒度文本整合，提升医学图像理解能力，在多个数据集上表现优异。", "keywords": ["多粒度学习", "医学图像理解", "对比学习", "多标签对齐", "跨粒度一致性", "视觉语言模型"], "triple": {"method": "多粒度对比学习框架MGLL", "result": "下游任务性能超越现有方法", "contribution": "提升医学图像多标签跨粒度对齐"}}
{"venue": "ICLR", "search_title": "Missingness Bias Calibration in Feature Attribution Explanations", "full_title": "Missingness Bias Calibration in Feature Attribution Explanations", "url": "https://openreview.net/forum?id=9AbJO130G8", "year": 2026, "is_main_conference": true, "abstract_snippet": "Popular explanation methods often produce unreliable feature importance scores due to \"missingness bias\", a systematic distortion that arises when models are probed with ablated, out-of-distribution inputs.\nExisting solutions treat this as a deep representational flaw that requires expensive retraining or architectural modifications.\nIn this work, we challenge this assumption and show that missingness bias can be effectively treated as a superficial artifact of the model's output space.\nWe introduce MCal, a lightweight post-hoc method that corrects this bias by fine-tuning a simple linear head on the outputs of a frozen base model.\nSurprisingly, we find this simple correction consistently reduces missingness bias and is competitive with, or even outperforms, prior heavyweight approaches across diverse medical benchmarks spanning vision, language, and tabular domains.", "abstract": "Popular explanation methods often produce unreliable feature importance scores due to \"missingness bias\", a systematic distortion that arises when models are probed with ablated, out-of-distribution inputs.\nExisting solutions treat this as a deep representational flaw that requires expensive retraining or architectural modifications.\nIn this work, we challenge this assumption and show that missingness bias can be effectively treated as a superficial artifact of the model's output space.\nWe introduce MCal, a lightweight post-hoc method that corrects this bias by fine-tuning a simple linear head on the outputs of a frozen base model.\nSurprisingly, we find this simple correction consistently reduces missingness bias and is competitive with, or even outperforms, prior heavyweight approaches across diverse medical benchmarks spanning vision, language, and tabular domains.", "abstract_source_venue": "OpenReviewAPI", "abstract_source_url": "https://api2.openreview.net/notes?id=9AbJO130G8", "openreview_id": "9AbJO130G8", "openreview_forum_id": "9AbJO130G8", "authors": [], "pdf_url": "https://openreview.net/pdf/a477e5d0544adc8930aaf12e4797ca7b6ad03c2d.pdf", "summary_cn": "本文提出MCal方法，通过微调线性头校正特征归因中的缺失性偏差，无需重新训练模型，在多种医学任务中表现优异。", "keywords": ["缺失性偏差", "特征归因", "后处理校正", "医学机器学习", "模型解释性"], "triple": {"method": "微调线性头进行后处理校正", "result": "有效减少偏差，性能优于或媲美现有方法", "contribution": "提出轻量级方法解决特征归因偏差问题"}}
{"venue": "ICLR", "search_title": "Johnson-Lindenstrauss Lemma Guided Network for Efficient 3D Medical Segmentation", "full_title": "Johnson-Lindenstrauss Lemma Guided Network for Efficient 3D Medical Segmentation", "url": "https://openreview.net/forum?id=fmWlDfCFMR", "year": 2026, "is_main_conference": true, "abstract_snippet": "Lightweight 3D medical image segmentation remains constrained by a fundamental \"efficiency / robustness conflict\", particularly when processing complex anatomical structures and heterogeneous modalities. In this paper, we study how to redesign the framework based on the characteristics of high-dimensional 3D images, and explore data synergy to overcome the fragile representation of lightweight methods. Our approach, VeloxSeg, begins with a deployable and extensible dual-stream CNN-Transformer architecture composed of Paired Window Attention (PWA) and Johnson-Lindenstrauss lemma-guided convolution (JLC). For each 3D image, we invoke a \"glance-and-focus\" principle, where PWA rapidly retrieves multi-scale information, and JLC ensures robust local feature extraction with minimal parameters, significantly enhancing the model's ability to operate with low computational budget. Followed by an extension of the dual-stream architecture that incorporates modal interaction into the multi-scale image-retrieval process, VeloxSeg efficiently models heterogeneous modalities. Finally, Spatially Decoupled Knowledge Transfer (SDKT) via Gram matrices injects the texture prior extracted by a self-supervised network into the segmentation network, yielding stronger representations than baselines at no extra inference cost. Experimental results on multimodal benchmarks show that VeloxSeg achieves a 26\\% Dice improvement, alongside increasing GPU throughput by 11x, CPU by 48x, and reducing training peak GPU memory usage by 1/20, inference by 1/24.", "abstract": "Lightweight 3D medical image segmentation remains constrained by a fundamental \"efficiency / robustness conflict\", particularly when processing complex anatomical structures and heterogeneous modalities. In this paper, we study how to redesign the framework based on the characteristics of high-dimensional 3D images, and explore data synergy to overcome the fragile representation of lightweight methods. Our approach, VeloxSeg, begins with a deployable and extensible dual-stream CNN-Transformer architecture composed of Paired Window Attention (PWA) and Johnson-Lindenstrauss lemma-guided convolution (JLC). For each 3D image, we invoke a \"glance-and-focus\" principle, where PWA rapidly retrieves multi-scale information, and JLC ensures robust local feature extraction with minimal parameters, significantly enhancing the model's ability to operate with low computational budget. Followed by an extension of the dual-stream architecture that incorporates modal interaction into the multi-scale image-retrieval process, VeloxSeg efficiently models heterogeneous modalities. Finally, Spatially Decoupled Knowledge Transfer (SDKT) via Gram matrices injects the texture prior extracted by a self-supervised network into the segmentation network, yielding stronger representations than baselines at no extra inference cost. Experimental results on multimodal benchmarks show that VeloxSeg achieves a 26\\% Dice improvement, alongside increasing GPU throughput by 11x, CPU by 48x, and reducing training peak GPU memory usage by 1/20, inference by 1/24.", "abstract_source_venue": "OpenReviewAPI", "abstract_source_url": "https://api2.openreview.net/notes?id=fmWlDfCFMR", "openreview_id": "fmWlDfCFMR", "openreview_forum_id": "fmWlDfCFMR", "authors": [], "pdf_url": "https://openreview.net/pdf/f5f3f2eaf8238a7826e2ac164d45dc3f018ba9d9.pdf", "summary_cn": "提出VeloxSeg轻量3D医学分割网络，结合双流CNN-Transformer与知识迁移，在提升分割精度的同时大幅降低计算与内存开销。", "keywords": ["3D医学分割", "轻量网络", "双流架构", "知识迁移", "多模态", "高效推理"], "triple": {"method": "双流CNN-Transformer与知识迁移", "result": "Dice提升26%，推理速度提升11-48倍，内存减少1/20-1/24", "contribution": "解决轻量3D分割的效率与鲁棒性冲突"}}
{"venue": "ICLR", "search_title": "You Point, I Learn: Online Adaptation of Interactive Segmentation Models for Handling Distribution Shifts in Medical Imaging", "full_title": "You Point, I Learn: Online Adaptation of Interactive Segmentation Models for Handling Distribution Shifts in Medical Imaging", "url": "https://openreview.net/forum?id=n0vHjCiLD2", "year": 2026, "is_main_conference": true, "abstract_snippet": "Interactive segmentation uses real-time user inputs, such as mouse clicks, to iteratively refine model predictions. Although not originally designed to address distribution shifts, this paradigm naturally lends itself to such challenges. In medical imaging, where distribution shifts are common, interactive methods can use user inputs to guide models towards improved predictions.\nMoreover, once a model is deployed, user corrections can be used to adapt the network parameters to the new data distribution, mitigating distribution shift. Based on these insights, we aim to develop a practical, effective method for improving the adaptive capabilities of interactive segmentation models to new data distributions in medical imaging.  Firstly, we found that strengthening the model's responsiveness to clicks is important for the initial training process. Moreover, we show that by treating the post-interaction user-refined model output as pseudo-ground-truth, we can design a lean, practical online adaptation method that enables a model to learn effectively across sequential test images. The framework includes two components: (i) a Post-Interaction adaptation process, updating the model after the user has completed interactive refinement of an image, and (ii) a Mid-Interaction adaptation process, updating incrementally after each click. Both processes include a Click-Centered Gaussian loss that strengthens the model's reaction to clicks and enhances focus on user-guided, clinically relevant regions. Experiments on 5 fundus and 4 brain‑MRI databases show that our approach consistently outperforms existing methods under diverse distribution shifts, including unseen imaging modalities and pathologies.\nCode and pretrained models will be released upon publication.", "abstract": "Interactive segmentation uses real-time user inputs, such as mouse clicks, to iteratively refine model predictions. Although not originally designed to address distribution shifts, this paradigm naturally lends itself to such challenges. In medical imaging, where distribution shifts are common, interactive methods can use user inputs to guide models towards improved predictions.\nMoreover, once a model is deployed, user corrections can be used to adapt the network parameters to the new data distribution, mitigating distribution shift. Based on these insights, we aim to develop a practical, effective method for improving the adaptive capabilities of interactive segmentation models to new data distributions in medical imaging.  Firstly, we found that strengthening the model's responsiveness to clicks is important for the initial training process. Moreover, we show that by treating the post-interaction user-refined model output as pseudo-ground-truth, we can design a lean, practical online adaptation method that enables a model to learn effectively across sequential test images. The framework includes two components: (i) a Post-Interaction adaptation process, updating the model after the user has completed interactive refinement of an image, and (ii) a Mid-Interaction adaptation process, updating incrementally after each click. Both processes include a Click-Centered Gaussian loss that strengthens the model's reaction to clicks and enhances focus on user-guided, clinically relevant regions. Experiments on 5 fundus and 4 brain‑MRI databases show that our approach consistently outperforms existing methods under diverse distribution shifts, including unseen imaging modalities and pathologies.\nCode and pretrained models will be released upon publication.", "abstract_source_venue": "OpenReviewAPI", "abstract_source_url": "https://api2.openreview.net/notes?id=n0vHjCiLD2", "openreview_id": "n0vHjCiLD2", "openreview_forum_id": "n0vHjCiLD2", "authors": [], "pdf_url": "https://openreview.net/pdf/d108a2a3b541e3ad83fd01d0cd44feb2eebc1f36.pdf", "summary_cn": "提出在线自适应交互式分割方法，利用用户点击修正模型，增强对医学图像分布偏移的适应性，在眼底和脑MRI数据上表现优异。", "keywords": ["交互式分割", "在线自适应", "分布偏移", "医学图像", "用户点击", "高斯损失"], "triple": {"method": "点击中心高斯损失与在线自适应", "result": "在多个数据库上超越现有方法", "contribution": "提升模型对分布偏移的适应性"}}
{"venue": "ICLR", "search_title": "Decentralized Attention Fails Centralized Signals: Rethinking Transformers for Medical Time Series", "full_title": "Decentralized Attention Fails Centralized Signals: Rethinking Transformers for Medical Time Series", "url": "https://openreview.net/forum?id=oZJFY2BQt2", "year": 2026, "is_main_conference": true, "abstract_snippet": "Accurate analysis of Medical time series (MedTS) data, such as Electroencephalography (EEG) and Electrocardiography (ECG), plays a pivotal role in healthcare applications, including the diagnosis of brain and heart diseases. MedTS data typically exhibits two critical patterns: **temporal dependencies** within individual channels and **channel dependencies** across multiple channels. While recent advances in deep learning have leveraged Transformer-based models to effectively capture temporal dependencies, they often struggle to model channel dependencies. This limitation stems from a structural mismatch: ***MedTS signals are inherently centralized, whereas the Transformer's attention is decentralized***, making it less effective at capturing global synchronization and unified waveform patterns. To bridge this gap, we propose **CoTAR** (Core Token Aggregation-Redistribution), a centralized MLP-based module tailored to replace the decentralized attention. Instead of allowing all tokens to interact directly, as in attention, CoTAR introduces a global core token that acts as a proxy to facilitate the inter-token interaction, thereby enforcing a centralized aggregation and redistribution strategy. This design not only better aligns with the centralized nature of MedTS signals but also reduces computational complexity from quadratic to linear. Experiments on five benchmarks validate the superiority of our method in both effectiveness and efficiency, achieving up to a **12.13%** improvement on the APAVA dataset, with merely 33% memory usage and 20% inference time compared to the previous state-of-the-art. Code and all training scripts are available in this [**Link**](https://anonymous.4open.science/r/TeCh-24).", "abstract": "Accurate analysis of Medical time series (MedTS) data, such as Electroencephalography (EEG) and Electrocardiography (ECG), plays a pivotal role in healthcare applications, including the diagnosis of brain and heart diseases. MedTS data typically exhibits two critical patterns: **temporal dependencies** within individual channels and **channel dependencies** across multiple channels. While recent advances in deep learning have leveraged Transformer-based models to effectively capture temporal dependencies, they often struggle to model channel dependencies. This limitation stems from a structural mismatch: ***MedTS signals are inherently centralized, whereas the Transformer's attention is decentralized***, making it less effective at capturing global synchronization and unified waveform patterns. To bridge this gap, we propose **CoTAR** (Core Token Aggregation-Redistribution), a centralized MLP-based module tailored to replace the decentralized attention. Instead of allowing all tokens to interact directly, as in attention, CoTAR introduces a global core token that acts as a proxy to facilitate the inter-token interaction, thereby enforcing a centralized aggregation and redistribution strategy. This design not only better aligns with the centralized nature of MedTS signals but also reduces computational complexity from quadratic to linear. Experiments on five benchmarks validate the superiority of our method in both effectiveness and efficiency, achieving up to a **12.13%** improvement on the APAVA dataset, with merely 33% memory usage and 20% inference time compared to the previous state-of-the-art. Code and all training scripts are available in this [**Link**](https://anonymous.4open.science/r/TeCh-24).", "abstract_source_venue": "OpenReviewAPI", "abstract_source_url": "https://api2.openreview.net/notes?id=oZJFY2BQt2", "openreview_id": "oZJFY2BQt2", "openreview_forum_id": "oZJFY2BQt2", "authors": [], "pdf_url": "https://openreview.net/pdf/80970c2cd39d934142d21e9e2f6390b98de793b1.pdf", "summary_cn": "针对医疗时间序列信号集中化特点，提出CoTAR模块替代Transformer分散注意力，通过核心令牌聚合再分配策略，提升性能并降低计算复杂度。", "keywords": ["医疗时间序列", "Transformer", "注意力机制", "CoTAR", "计算效率", "信号处理"], "triple": {"method": "CoTAR模块（核心令牌聚合再分配）", "result": "性能提升12.13%，内存和推理时间大幅减少", "contribution": "提出集中化方法更好匹配医疗信号特性"}}
{"venue": "ICLR", "search_title": "K-Prism: A Knowledge-Guided and Prompt Integrated Universal Medical Image Segmentation Model", "full_title": "K-Prism: A Knowledge-Guided and Prompt Integrated Universal Medical Image Segmentation Model", "url": "https://openreview.net/forum?id=gvRf95K4im", "year": 2026, "is_main_conference": true, "abstract_snippet": "Medical image segmentation is fundamental to clinical decision-making, yet existing models remain fragmented. They are usually trained on single knowledge sources and specific to individual tasks, modalities, or organs. This fragmentation contrasts sharply with clinical practice, where experts seamlessly integrate diverse knowledge: anatomical priors from training, exemplar-based reasoning from reference cases, and iterative refinement through real-time interaction. We present $\\textbf{K-Prism}$, a unified segmentation framework that mirrors this clinical flexibility by systematically integrating three knowledge paradigms: (i) $\\textit{semantic priors}$ learned from annotated datasets, (ii) $\\textit{in-context knowledge}$ from few-shot reference examples, and (iii) $\\textit{interactive feedback}$ from user inputs like clicks or scribbles. Our key insight is that these heterogeneous knowledge sources can be encoded into a dual-prompt representation: 1-D sparse prompts defining $\\textit{what}$ to segment and 2-D dense prompts indicating $\\textit{where}$ to attend, which are then dynamically routed through a Mixture-of-Experts (MoE) decoder. This design enables flexible switching between paradigms and joint training across diverse tasks without architectural modifications. Comprehensive experiments on 18 public datasets spanning diverse modalities (CT, MRI, X-ray, pathology, ultrasound, etc.) demonstrate that K-Prism achieves state-of-the-art performance across semantic, in-context, and interactive segmentation settings. Code will be released upon publication.", "abstract": "Medical image segmentation is fundamental to clinical decision-making, yet existing models remain fragmented. They are usually trained on single knowledge sources and specific to individual tasks, modalities, or organs. This fragmentation contrasts sharply with clinical practice, where experts seamlessly integrate diverse knowledge: anatomical priors from training, exemplar-based reasoning from reference cases, and iterative refinement through real-time interaction. We present $\\textbf{K-Prism}$, a unified segmentation framework that mirrors this clinical flexibility by systematically integrating three knowledge paradigms: (i) $\\textit{semantic priors}$ learned from annotated datasets, (ii) $\\textit{in-context knowledge}$ from few-shot reference examples, and (iii) $\\textit{interactive feedback}$ from user inputs like clicks or scribbles. Our key insight is that these heterogeneous knowledge sources can be encoded into a dual-prompt representation: 1-D sparse prompts defining $\\textit{what}$ to segment and 2-D dense prompts indicating $\\textit{where}$ to attend, which are then dynamically routed through a Mixture-of-Experts (MoE) decoder. This design enables flexible switching between paradigms and joint training across diverse tasks without architectural modifications. Comprehensive experiments on 18 public datasets spanning diverse modalities (CT, MRI, X-ray, pathology, ultrasound, etc.) demonstrate that K-Prism achieves state-of-the-art performance across semantic, in-context, and interactive segmentation settings. Code will be released upon publication.", "abstract_source_venue": "OpenReviewAPI", "abstract_source_url": "https://api2.openreview.net/notes?id=gvRf95K4im", "openreview_id": "gvRf95K4im", "openreview_forum_id": "gvRf95K4im", "authors": [], "pdf_url": "https://openreview.net/pdf/1b0797940c7421dd109dfb45d43517b8b17ca647.pdf", "summary_cn": "K-Prism提出统一医学图像分割框架，集成语义先验、上下文知识与交互反馈，通过双提示与MoE解码器实现多任务SOTA性能。", "keywords": ["医学图像分割", "知识引导", "提示集成", "混合专家", "多模态", "统一框架"], "triple": {"method": "双提示表示与MoE解码器", "result": "在18个数据集上实现SOTA", "contribution": "提出统一分割框架K-Prism"}}
{"venue": "ICLR", "search_title": "Modeling the Density of Pixel-level Self-supervised Embeddings for Unsupervised Pathology Segmentation in Medical CT", "full_title": "Modeling the Density of Pixel-level Self-supervised Embeddings for Unsupervised Pathology Segmentation in Medical CT", "url": "https://openreview.net/forum?id=i7YnUW0uWg", "year": 2026, "is_main_conference": true, "abstract_snippet": "Accurate detection of all pathological findings in 3D medical images remains a significant challenge, as supervised models are limited to detecting only the few pathology classes annotated in existing datasets. To address this, we frame pathology detection as an unsupervised visual anomaly segmentation (UVAS) problem, leveraging the inherent rarity of pathological patterns compared to healthy ones. We enhance the existing density-based UVAS framework with two key innovations: (1) dense self-supervised learning for feature extraction, eliminating the need for supervised pretraining, and (2) learned, masking-invariant dense features as conditioning variables, replacing hand-crafted positional encodings. Trained on over 30,000 unlabeled 3D CT volumes, our fully self-supervised model, Screener, outperforms existing UVAS methods on four large-scale test datasets comprising 1,820 scans with diverse pathologies. Furthermore, in a low-shot supervised fine-tuning setting, Screener surpasses existing self-supervised pretraining methods, establishing it as a state-of-the-art foundation for pathology segmentation. The code and pretrained models will be made publicly available.", "abstract": "Accurate detection of all pathological findings in 3D medical images remains a significant challenge, as supervised models are limited to detecting only the few pathology classes annotated in existing datasets. To address this, we frame pathology detection as an unsupervised visual anomaly segmentation (UVAS) problem, leveraging the inherent rarity of pathological patterns compared to healthy ones. We enhance the existing density-based UVAS framework with two key innovations: (1) dense self-supervised learning for feature extraction, eliminating the need for supervised pretraining, and (2) learned, masking-invariant dense features as conditioning variables, replacing hand-crafted positional encodings. Trained on over 30,000 unlabeled 3D CT volumes, our fully self-supervised model, Screener, outperforms existing UVAS methods on four large-scale test datasets comprising 1,820 scans with diverse pathologies. Furthermore, in a low-shot supervised fine-tuning setting, Screener surpasses existing self-supervised pretraining methods, establishing it as a state-of-the-art foundation for pathology segmentation. The code and pretrained models will be made publicly available.", "abstract_source_venue": "OpenReviewAPI", "abstract_source_url": "https://api2.openreview.net/notes?id=i7YnUW0uWg", "openreview_id": "i7YnUW0uWg", "openreview_forum_id": "i7YnUW0uWg", "authors": [], "pdf_url": "https://openreview.net/pdf/c5674969451511c7c5004fb3f5d4ca4b0a0095a5.pdf", "summary_cn": "提出Screener模型，通过密集自监督学习和掩码不变特征，实现医学CT图像的无监督病理分割，在多个数据集上超越现有方法。", "keywords": ["无监督病理分割", "自监督学习", "CT图像", "异常检测", "密集特征", "掩码不变性"], "triple": {"method": "密集自监督学习与掩码不变特征", "result": "在四个大规模测试集上超越现有UVAS方法", "contribution": "提供无需监督预训练的高性能病理分割基础模型"}}
{"venue": "ICLR", "search_title": "From Medical Records to Diagnostic Dialogues: A Clinical-Grounded Approach and Dataset for Psychiatric Comorbidity", "full_title": "From Medical Records to Diagnostic Dialogues: A Clinical-Grounded Approach and Dataset for Psychiatric Comorbidity", "url": "https://openreview.net/forum?id=sWWAZVHtke", "year": 2026, "is_main_conference": true, "abstract_snippet": "Psychiatric comorbidity is clinically significant yet challenging due to the complexity of multiple co-occurring disorders. To address this, we develop a novel approach integrating synthetic patient electronic medical record (EMR) construction and multi-agent diagnostic dialogue generation. We create 502 synthetic EMRs for common comorbid conditions using a pipeline that ensures clinical relevance and diversity. Our multi-agent framework transfers the clinical interview protocol into a hierarchical state machine and context tree, supporting over 130 diagnostic states while maintaining clinical standards. Through this rigorous process, we construct the first large-scale dialogue dataset supporting comorbidity, containing 3,000 multi-turn diagnostic dialogues validated by psychiatrists. This dataset enhances diagnostic accuracy and treatment planning, offering a valuable resource for psychiatric comorbidity research. Compared to real-world clinical transcripts, PsyCoTalk exhibits high structural and linguistic fidelity in terms of dialogue length, token distribution, and diagnostic reasoning strategies. Licensed psychiatrists confirm the realism and diagnostic validity of the dialogues. This dataset enables the development and evaluation of models capable of multi-disorder psychiatric screening in a single conversational pass.", "abstract": "Psychiatric comorbidity is clinically significant yet challenging due to the complexity of multiple co-occurring disorders. To address this, we develop a novel approach integrating synthetic patient electronic medical record (EMR) construction and multi-agent diagnostic dialogue generation. We create 502 synthetic EMRs for common comorbid conditions using a pipeline that ensures clinical relevance and diversity. Our multi-agent framework transfers the clinical interview protocol into a hierarchical state machine and context tree, supporting over 130 diagnostic states while maintaining clinical standards. Through this rigorous process, we construct the first large-scale dialogue dataset supporting comorbidity, containing 3,000 multi-turn diagnostic dialogues validated by psychiatrists. This dataset enhances diagnostic accuracy and treatment planning, offering a valuable resource for psychiatric comorbidity research. Compared to real-world clinical transcripts, PsyCoTalk exhibits high structural and linguistic fidelity in terms of dialogue length, token distribution, and diagnostic reasoning strategies. Licensed psychiatrists confirm the realism and diagnostic validity of the dialogues. This dataset enables the development and evaluation of models capable of multi-disorder psychiatric screening in a single conversational pass.", "abstract_source_venue": "OpenReviewAPI", "abstract_source_url": "https://api2.openreview.net/notes?id=sWWAZVHtke", "openreview_id": "sWWAZVHtke", "openreview_forum_id": "sWWAZVHtke", "authors": [], "pdf_url": "https://openreview.net/pdf/3d21fa2c3d308d779a5848e66683dd42df646ead.pdf", "summary_cn": "本研究提出一种临床基础方法，通过合成电子病历和多智能体对话生成，构建首个大规模精神病共病诊断对话数据集，提升诊断准确性和治疗规划。", "keywords": ["精神病共病", "诊断对话", "电子病历合成", "多智能体框架", "临床验证", "数据集构建"], "triple": {"method": "合成电子病历与多智能体对话生成", "result": "构建包含3000个对话的数据集，经精神科医生验证具有高保真度", "contribution": "提供首个支持共病研究的大规模诊断对话资源"}}
{"venue": "ICLR", "search_title": "Moving Beyond Medical Exams: A Clinician-Annotated Fairness Dataset of Real-World Tasks and Ambiguity in Mental Healthcare", "full_title": "Moving Beyond Medical Exams: A Clinician-Annotated Fairness Dataset of Real-World Tasks and Ambiguity in Mental Healthcare", "url": "https://openreview.net/forum?id=tSy7OtONsg", "year": 2026, "is_main_conference": true, "abstract_snippet": "Current medical language model (LM) benchmarks often over-simplify the complexities of day-to-day clinical practice tasks and instead rely on evaluating LMs on multiple-choice board exam questions. \nIn psychiatry especially, these challenges are worsened by fairness and bias issues, since models can be swayed by patient demographics even when those factors should not influence clinical decisions. \nThus, we present an expert-created and annotated dataset spanning five critical domains of decision-making in mental healthcare: treatment, diagnosis, documentation, monitoring, and triage. \nThis U.S. centric dataset — created without any LM assistance — is designed to capture the nuanced clinical reasoning and daily ambiguities mental health practitioners encounter, reflecting the inherent complexities of care delivery that are missing from existing datasets. \nAlmost all base questions with five answer options each have had the decision-irrelevant demographic patient information removed and replaced with variables, e.g., for age or ethnicity, and are available for male, female, or non-binary-coded patients. \nThis design enables systematic evaluations of model performance and bias by studying how demographic factors affect decision-making. \nFor question categories dealing with ambiguity and multiple valid answer options, we create a preference dataset with uncertainties from the expert annotations.\nWe outline a series of intended use cases and demonstrate the usability of our dataset by evaluating sixteen off-the-shelf\nand six (mental) health fine-tuned LMs on category-specific task accuracy, on the fairness impact of patient demographic information on decision-making, and how consistently free-form responses deviate from human-annotated samples.", "abstract": "Current medical language model (LM) benchmarks often over-simplify the complexities of day-to-day clinical practice tasks and instead rely on evaluating LMs on multiple-choice board exam questions. \nIn psychiatry especially, these challenges are worsened by fairness and bias issues, since models can be swayed by patient demographics even when those factors should not influence clinical decisions. \nThus, we present an expert-created and annotated dataset spanning five critical domains of decision-making in mental healthcare: treatment, diagnosis, documentation, monitoring, and triage. \nThis U.S. centric dataset — created without any LM assistance — is designed to capture the nuanced clinical reasoning and daily ambiguities mental health practitioners encounter, reflecting the inherent complexities of care delivery that are missing from existing datasets. \nAlmost all base questions with five answer options each have had the decision-irrelevant demographic patient information removed and replaced with variables, e.g., for age or ethnicity, and are available for male, female, or non-binary-coded patients. \nThis design enables systematic evaluations of model performance and bias by studying how demographic factors affect decision-making. \nFor question categories dealing with ambiguity and multiple valid answer options, we create a preference dataset with uncertainties from the expert annotations.\nWe outline a series of intended use cases and demonstrate the usability of our dataset by evaluating sixteen off-the-shelf\nand six (mental) health fine-tuned LMs on category-specific task accuracy, on the fairness impact of patient demographic information on decision-making, and how consistently free-form responses deviate from human-annotated samples.", "abstract_source_venue": "OpenReviewAPI", "abstract_source_url": "https://api2.openreview.net/notes?id=tSy7OtONsg", "openreview_id": "tSy7OtONsg", "openreview_forum_id": "tSy7OtONsg", "authors": [], "pdf_url": "https://openreview.net/pdf/e748f56b1764bf478a530593b94f9db7d8cc793b.pdf", "summary_cn": "研究创建了一个专家标注的心理健康临床决策数据集，涵盖治疗、诊断等五个领域，用于评估语言模型的公平性和临床推理能力。", "keywords": ["心理健康", "临床决策", "公平性评估", "语言模型", "专家标注", "数据集"], "triple": {"method": "创建专家标注数据集，移除无关人口信息并引入变量", "result": "评估了16个现成和6个微调语言模型在任务准确性、公平性和一致性方面的表现", "contribution": "填补现有数据集空白，支持系统化评估模型公平性和临床复杂性"}}
{"venue": "ICLR", "search_title": "MedLesionVQA: A Multimodal Benchmark Emulating Clinical Visual Diagnosis for Body Surface Health", "full_title": "MedLesionVQA: A Multimodal Benchmark Emulating Clinical Visual Diagnosis for Body Surface Health", "url": "https://openreview.net/forum?id=BYtqk6AVuL", "year": 2026, "is_main_conference": true, "abstract_snippet": "Body-surface health conditions, spanning diverse clinical departments, represent some of the most frequent diagnostic scenarios and a primary target for medical multimodal large language models (MLLMs). \nYet existing medical benchmarks are either built from publicly available sources with limited expert curation or focus narrowly on disease classification, failing to reflect the stepwise recognition and reasoning processes physicians follow in real practice. \nTo address this gap, we introduce MedLesionVQA, the first benchmark explicitly designed to evaluate MLLMs on the visual diagnostic workflow for body-surface conditions in large scale. \nAll questions are derived from authentic clinical visual diagnosis scenarios and verified by medical experts with over 20 years of experience, while the data are drawn from 10k+ real patient visits, ensuring authenticity, clinical reality and diversity.\nMedLesionVQA consists of 12K in-house volunteer images (never publicly leaked) and 19K expert-verified question–answer pairs, with fine-grained annotations of 94 lesion types, 110 body regions, and 96 diseases. \nWe evaluate 20+ state-of-the-art MLLMs against human physicians: the best model reaches 56.2% accuracy, far below primary physicians (61.4%) and senior specialists (73.2%). These results expose the persistent gap between MLLMs and clinical expertise, underscoring the need for the multimodal benchmarks to drive trustworthy medical AI.", "abstract": "Body-surface health conditions, spanning diverse clinical departments, represent some of the most frequent diagnostic scenarios and a primary target for medical multimodal large language models (MLLMs). \nYet existing medical benchmarks are either built from publicly available sources with limited expert curation or focus narrowly on disease classification, failing to reflect the stepwise recognition and reasoning processes physicians follow in real practice. \nTo address this gap, we introduce MedLesionVQA, the first benchmark explicitly designed to evaluate MLLMs on the visual diagnostic workflow for body-surface conditions in large scale. \nAll questions are derived from authentic clinical visual diagnosis scenarios and verified by medical experts with over 20 years of experience, while the data are drawn from 10k+ real patient visits, ensuring authenticity, clinical reality and diversity.\nMedLesionVQA consists of 12K in-house volunteer images (never publicly leaked) and 19K expert-verified question–answer pairs, with fine-grained annotations of 94 lesion types, 110 body regions, and 96 diseases. \nWe evaluate 20+ state-of-the-art MLLMs against human physicians: the best model reaches 56.2% accuracy, far below primary physicians (61.4%) and senior specialists (73.2%). These results expose the persistent gap between MLLMs and clinical expertise, underscoring the need for the multimodal benchmarks to drive trustworthy medical AI.", "abstract_source_venue": "OpenReviewAPI", "abstract_source_url": "https://api2.openreview.net/notes?id=BYtqk6AVuL", "openreview_id": "BYtqk6AVuL", "openreview_forum_id": "BYtqk6AVuL", "authors": [], "pdf_url": "https://openreview.net/pdf/fa117d1fb4924f46fb476831b350ea1121a71f7f.pdf", "summary_cn": "MedLesionVQA是首个模拟体表健康临床视觉诊断流程的大规模多模态基准，包含真实患者图像与专家验证问答，评估显示当前MLLMs性能仍显著低于医生水平。", "keywords": ["多模态大语言模型", "临床视觉诊断", "体表健康", "医学基准", "专家验证", "性能评估"], "triple": {"method": "构建基于真实临床数据与专家验证的多模态基准", "result": "最佳模型准确率56.2%，低于初级医生(61.4%)和资深专家(73.2%)", "contribution": "填补临床诊断流程评估空白，推动可信医疗AI发展"}}
{"venue": "ICLR", "search_title": "U2-BENCH: Benchmarking Large Vision-Language Models on Ultrasound Understanding", "full_title": "U2-BENCH: Benchmarking Large Vision-Language Models on Ultrasound Understanding", "url": "https://openreview.net/forum?id=jU10qDevGg", "year": 2026, "is_main_conference": true, "abstract_snippet": "Ultrasound is a widely-used imaging modality critical to global healthcare, yet its interpretation remains challenging due to its varying image quality on operators, noises, and anatomical structures. Although large vision-language models (LVLMs) have demonstrated impressive multimodal capabilities across natural and medical domains, their performance on ultrasound remains largely unexplored. We introduce U2-BENCH, the first comprehensive benchmark to evaluate LVLMs on ultrasound understanding across classification, detection, regression, and text generation tasks. U2-BENCH aggregates 7,241 cases spanning 15 anatomical regions and defines 8 clinically inspired tasks, such as diagnosis, view recognition, lesion localization, clinical value estimation, and report generation, across 50 ultrasound application scenarios. We evaluate 23 state-of-the-art LVLMs, both open- and closed-source, general-purpose and medical-specific. Our results reveal strong performance on image-level classification, but persistent challenges in spatial reasoning and clinical language generation. U2-BENCH establishes a rigorous and unified testbed to assess and accelerate LVLM research in the uniquely multimodal domain of medical ultrasound imaging.", "abstract": "Ultrasound is a widely-used imaging modality critical to global healthcare, yet its interpretation remains challenging due to its varying image quality on operators, noises, and anatomical structures. Although large vision-language models (LVLMs) have demonstrated impressive multimodal capabilities across natural and medical domains, their performance on ultrasound remains largely unexplored. We introduce U2-BENCH, the first comprehensive benchmark to evaluate LVLMs on ultrasound understanding across classification, detection, regression, and text generation tasks. U2-BENCH aggregates 7,241 cases spanning 15 anatomical regions and defines 8 clinically inspired tasks, such as diagnosis, view recognition, lesion localization, clinical value estimation, and report generation, across 50 ultrasound application scenarios. We evaluate 23 state-of-the-art LVLMs, both open- and closed-source, general-purpose and medical-specific. Our results reveal strong performance on image-level classification, but persistent challenges in spatial reasoning and clinical language generation. U2-BENCH establishes a rigorous and unified testbed to assess and accelerate LVLM research in the uniquely multimodal domain of medical ultrasound imaging.", "abstract_source_venue": "OpenReviewAPI", "abstract_source_url": "https://api2.openreview.net/notes?id=jU10qDevGg", "openreview_id": "jU10qDevGg", "openreview_forum_id": "jU10qDevGg", "authors": [], "pdf_url": "https://openreview.net/pdf/1698db9b292ef3704023c3319a6ece1246485d4b.pdf", "summary_cn": "U2-BENCH是首个评估大型视觉语言模型在超声理解上的综合基准，涵盖多任务与解剖区域，揭示模型在空间推理和临床语言生成方面的挑战。", "keywords": ["超声理解", "大型视觉语言模型", "基准测试", "多模态", "医学影像", "临床任务"], "triple": {"method": "构建U2-BENCH基准，包含7241个案例和8个临床任务", "result": "模型在图像分类表现强，但空间推理和语言生成仍存挑战", "contribution": "为超声领域提供首个统一评估框架，推动LVLM研究"}}
{"venue": "ICLR", "search_title": "KnowGuard: Knowledge-Driven Abstention for Multi-Round Clinical Reasoning", "full_title": "KnowGuard: Knowledge-Driven Abstention for Multi-Round Clinical Reasoning", "url": "https://openreview.net/forum?id=gQRefH8upx", "year": 2026, "is_main_conference": true, "abstract_snippet": "In clinical practice, physicians refrain from making decisions when patient information is insufficient. This behavior, known as abstention, is a critical safety mechanism preventing potentially harmful misdiagnoses. Recent investigations have reported the application of large language models (LLMs) in medical scenarios. However, existing LLMs struggle with the abstentions, frequently providing overconfident responses despite incomplete information. This limitation stems from conventional abstention methods relying solely on model self-assessments, which lack systematic strategies to identify knowledge boundaries with external medical evidences. To address this, we propose \\textbf{KnowGuard}, a novel \\textit{investigate-before-abstain} paradigm that integrates systematic knowledge graph exploration for clinical decision-making. Our approach consists of two key stages operating on a shared contextualized evidence pool: 1) an evidence discovery stage that systematically explores the medical knowledge space through graph expansion and direct retrieval, and 2) an evidence evaluation stage that ranks evidence using multiple factors to adapt exploration based on patient context and conversation history. This two-stage approach enables systematic knowledge graph exploration, allowing models to trace structured reasoning paths and recognize insufficient medical evidence. We evaluate our abstention approach using open-ended multi-round clinical benchmarks that mimic realistic diagnostic scenarios, assessing abstention quality through accuracy-efficiency trade-offs beyond existing closed-form evaluations. Experimental evidence clearly demonstrates that KnowGuard outperforms state-of-the-art abstention approaches, improving diagnostic accuracy by 3.93\\% through effective diagnostic interactions averaging 5.74 conversation turns.", "abstract": "In clinical practice, physicians refrain from making decisions when patient information is insufficient. This behavior, known as abstention, is a critical safety mechanism preventing potentially harmful misdiagnoses. Recent investigations have reported the application of large language models (LLMs) in medical scenarios. However, existing LLMs struggle with the abstentions, frequently providing overconfident responses despite incomplete information. This limitation stems from conventional abstention methods relying solely on model self-assessments, which lack systematic strategies to identify knowledge boundaries with external medical evidences. To address this, we propose \\textbf{KnowGuard}, a novel \\textit{investigate-before-abstain} paradigm that integrates systematic knowledge graph exploration for clinical decision-making. Our approach consists of two key stages operating on a shared contextualized evidence pool: 1) an evidence discovery stage that systematically explores the medical knowledge space through graph expansion and direct retrieval, and 2) an evidence evaluation stage that ranks evidence using multiple factors to adapt exploration based on patient context and conversation history. This two-stage approach enables systematic knowledge graph exploration, allowing models to trace structured reasoning paths and recognize insufficient medical evidence. We evaluate our abstention approach using open-ended multi-round clinical benchmarks that mimic realistic diagnostic scenarios, assessing abstention quality through accuracy-efficiency trade-offs beyond existing closed-form evaluations. Experimental evidence clearly demonstrates that KnowGuard outperforms state-of-the-art abstention approaches, improving diagnostic accuracy by 3.93\\% through effective diagnostic interactions averaging 5.74 conversation turns.", "abstract_source_venue": "OpenReviewAPI", "abstract_source_url": "https://api2.openreview.net/notes?id=gQRefH8upx", "openreview_id": "gQRefH8upx", "openreview_forum_id": "gQRefH8upx", "authors": [], "pdf_url": "https://openreview.net/pdf/c3900b8d95d625edf60a2e0b400cb766bc55edc3.pdf", "summary_cn": "KnowGuard提出知识驱动的临床推理弃权机制，通过两阶段知识图谱探索，提升大语言模型在信息不足时的诊断准确性与安全性。", "keywords": ["临床推理", "知识图谱", "弃权机制", "大语言模型", "多轮对话", "诊断准确性"], "triple": {"method": "两阶段知识图谱探索（证据发现与评估）", "result": "诊断准确率提升3.93%，平均5.74轮对话", "contribution": "提出知识驱动的临床弃权新范式"}}
{"venue": "ICLR", "search_title": "Doctor-R1: Mastering Clinical Inquiry with Experiential Agentic Reinforcement Learning", "full_title": "Doctor-R1: Mastering Clinical Inquiry with Experiential Agentic Reinforcement Learning", "url": "https://openreview.net/forum?id=vQGHTyL0Jw", "year": 2026, "is_main_conference": true, "abstract_snippet": "The professionalism of a human doctor in outpatient service depends on two core abilities: the ability to make accurate medical decisions and the medical consultation skill to conduct strategic, empathetic patient inquiry. Existing Large Language Models (LLMs) have achieved remarkable accuracy on medical decision-making benchmarks. However, they often lack the ability to conduct the strategic and empathetic consultation, which is essential for real-world clinical scenarios. To address this gap, we propose Doctor-R1, an AI doctor agent trained to master both of the capabilities by ask high-yield questions and conduct strategic multi-turn inquiry to guide decision-making. Our framework introduces three key components: a multi-agent interactive environment, a two-tiered reward architecture that separately optimizes clinical decision-making and communicative inquiry skills, and an experience repository to ground policy learning in high-quality prior trajectories. We evaluate Doctor-R1 on OpenAI's HealthBench and MAQuE, assessed across multi-facet metrics, such as communication quality, user experience, and task accuracy. Remarkably, Doctor-R1 surpasses state-of-the-art open-source specialized LLMs by a substantial margin with higher parameter efficiency and outperforms powerful proprietary models. Furthermore, the human evaluations show a strong preference for Doctor-R1 to generate human-preferred clinical dialogue, demonstrating the effectiveness of the framework.", "abstract": "The professionalism of a human doctor in outpatient service depends on two core abilities: the ability to make accurate medical decisions and the medical consultation skill to conduct strategic, empathetic patient inquiry. Existing Large Language Models (LLMs) have achieved remarkable accuracy on medical decision-making benchmarks. However, they often lack the ability to conduct the strategic and empathetic consultation, which is essential for real-world clinical scenarios. To address this gap, we propose Doctor-R1, an AI doctor agent trained to master both of the capabilities by ask high-yield questions and conduct strategic multi-turn inquiry to guide decision-making. Our framework introduces three key components: a multi-agent interactive environment, a two-tiered reward architecture that separately optimizes clinical decision-making and communicative inquiry skills, and an experience repository to ground policy learning in high-quality prior trajectories. We evaluate Doctor-R1 on OpenAI's HealthBench and MAQuE, assessed across multi-facet metrics, such as communication quality, user experience, and task accuracy. Remarkably, Doctor-R1 surpasses state-of-the-art open-source specialized LLMs by a substantial margin with higher parameter efficiency and outperforms powerful proprietary models. Furthermore, the human evaluations show a strong preference for Doctor-R1 to generate human-preferred clinical dialogue, demonstrating the effectiveness of the framework.", "abstract_source_venue": "OpenReviewAPI", "abstract_source_url": "https://api2.openreview.net/notes?id=vQGHTyL0Jw", "openreview_id": "vQGHTyL0Jw", "openreview_forum_id": "vQGHTyL0Jw", "authors": [], "pdf_url": "https://openreview.net/pdf/b32af93cfd4fe18618df90adfb090dc5332b550a.pdf", "summary_cn": "Doctor-R1通过多智能体交互环境和双层奖励架构，结合经验库，在临床决策与沟通问诊上超越现有模型，生成更受人类偏好的对话。", "keywords": ["Doctor-R1", "临床问诊", "强化学习", "多智能体交互", "双层奖励", "经验库"], "triple": {"method": "多智能体交互环境与双层奖励强化学习", "result": "在HealthBench和MAQuE上超越开源与专有模型，生成人类偏好对话", "contribution": "提升AI医生在战略共情问诊与决策的综合性能力"}}
{"venue": "ICLR", "search_title": "AbdCTBench: Learning Clinical Biomarker Representations from Abdominal Surface Geometry", "full_title": "AbdCTBench: Learning Clinical Biomarker Representations from Abdominal Surface Geometry", "url": "https://openreview.net/forum?id=dKRAo0a9Gm", "year": 2026, "is_main_conference": true, "abstract_snippet": "Body composition analysis through CT and MRI imaging provides critical insights for cardio-metabolic health assessment but remains limited by accessibility barriers including radiation exposure, high costs, and infrastructure requirements. We present AbdCTBench, a large-scale dataset containing 23,506 CT-derived abdominal surface meshes from 18,719 patients, paired with 87 comorbidity labels, 31 specific diagnosis codes, and 16 CT-derived biomarkers. Our key insight is that external surface geometry is predictive of internal tissue composition, enabling accessible health screening through consumer devices. We establish comprehensive benchmarks across seven computer vision architectures (ResNet-18/34/50, DenseNet-121, EfficientNet-B0, ViT-Small, Swin Transformer-Base), demonstrating that models can learn robust surface-to-biomarker representations directly from 2D mesh projections. Our best-performing models achieve clinically relevant accuracy: age prediction with MAE 6.22 years (R²=0.757), mortality prediction with AUROC 0.839, and diabetes (with chronic complications) detection with AUROC 0.801. Notably, smaller architectures consistently matched or surpassed larger models, while medical-domain pre-training (RadImageNet) and self-supervised pre-training (DINOv2) showed competitive but not superior performance. AbdCTBench represents the largest publicly available dataset bridging external body geometry with internal clinical measurements, enabling future research in accessible medical AI. We plan to release the dataset, evaluation protocols, and baseline models to accelerate research in representation learning for medical applications, immediately following the review period.", "abstract": "Body composition analysis through CT and MRI imaging provides critical insights for cardio-metabolic health assessment but remains limited by accessibility barriers including radiation exposure, high costs, and infrastructure requirements. We present AbdCTBench, a large-scale dataset containing 23,506 CT-derived abdominal surface meshes from 18,719 patients, paired with 87 comorbidity labels, 31 specific diagnosis codes, and 16 CT-derived biomarkers. Our key insight is that external surface geometry is predictive of internal tissue composition, enabling accessible health screening through consumer devices. We establish comprehensive benchmarks across seven computer vision architectures (ResNet-18/34/50, DenseNet-121, EfficientNet-B0, ViT-Small, Swin Transformer-Base), demonstrating that models can learn robust surface-to-biomarker representations directly from 2D mesh projections. Our best-performing models achieve clinically relevant accuracy: age prediction with MAE 6.22 years (R²=0.757), mortality prediction with AUROC 0.839, and diabetes (with chronic complications) detection with AUROC 0.801. Notably, smaller architectures consistently matched or surpassed larger models, while medical-domain pre-training (RadImageNet) and self-supervised pre-training (DINOv2) showed competitive but not superior performance. AbdCTBench represents the largest publicly available dataset bridging external body geometry with internal clinical measurements, enabling future research in accessible medical AI. We plan to release the dataset, evaluation protocols, and baseline models to accelerate research in representation learning for medical applications, immediately following the review period.", "abstract_source_venue": "OpenReviewAPI", "abstract_source_url": "https://api2.openreview.net/notes?id=dKRAo0a9Gm", "openreview_id": "dKRAo0a9Gm", "openreview_forum_id": "dKRAo0a9Gm", "authors": [], "pdf_url": "https://openreview.net/pdf/69dbc4906a996120b5a52367af59d2c9d358f815.pdf", "summary_cn": "AbdCTBench数据集包含23,506个腹部表面网格，用于从几何预测临床生物标志物。七种视觉模型在年龄、死亡率和糖尿病预测上达到临床相关精度，小模型表现优异。", "keywords": ["腹部表面几何", "临床生物标志物", "计算机视觉", "健康筛查", "数据集", "表示学习"], "triple": {"method": "使用2D网格投影和七种视觉架构学习表面到生物标志物表示", "result": "年龄预测MAE 6.22年，死亡率AUROC 0.839，糖尿病检测AUROC 0.801", "contribution": "提供最大公开数据集，连接外部几何与内部测量，促进可访问医疗AI研究"}}
{"venue": "ICLR", "search_title": "Photon: Speedup Volume Understanding with Efficient Multimodal Large Language Models", "full_title": "Photon: Speedup Volume Understanding with Efficient Multimodal Large Language Models", "url": "https://openreview.net/forum?id=xsSJw6jJBL", "year": 2026, "is_main_conference": true, "abstract_snippet": "Multimodal large language models are promising for clinical visual question answering tasks, but scaling to 3D imaging is hindered by high computational costs. Prior methods often rely on 2D slices or fixed-length token compression, disrupting volumetric continuity and obscuring subtle findings. We present Photon, a framework that represents 3D medical volumes with token sequences of variable length.  Photon introduces instruction-conditioned token scheduling and surrogate gradient propagation to adaptively reduce tokens during both training and inference, which lowers computational cost while mitigating the attention dilution caused by redundant tokens. It incorporates a custom backpropagation rule with gradient restoration to enable differentiable optimization despite discrete token drop. To stabilize token compression and ensure reliable use of visual evidence, Photon further applies regularization objectives that mitigate language-only bias and improve reliability. Experiments on diverse medical visual question answering tasks show that Photon achieves state-of-the-art accuracy while reducing resource usage and accelerating both training and inference.", "abstract": "Multimodal large language models are promising for clinical visual question answering tasks, but scaling to 3D imaging is hindered by high computational costs. Prior methods often rely on 2D slices or fixed-length token compression, disrupting volumetric continuity and obscuring subtle findings. We present Photon, a framework that represents 3D medical volumes with token sequences of variable length.  Photon introduces instruction-conditioned token scheduling and surrogate gradient propagation to adaptively reduce tokens during both training and inference, which lowers computational cost while mitigating the attention dilution caused by redundant tokens. It incorporates a custom backpropagation rule with gradient restoration to enable differentiable optimization despite discrete token drop. To stabilize token compression and ensure reliable use of visual evidence, Photon further applies regularization objectives that mitigate language-only bias and improve reliability. Experiments on diverse medical visual question answering tasks show that Photon achieves state-of-the-art accuracy while reducing resource usage and accelerating both training and inference.", "abstract_source_venue": "OpenReviewAPI", "abstract_source_url": "https://api2.openreview.net/notes?id=xsSJw6jJBL", "openreview_id": "xsSJw6jJBL", "openreview_forum_id": "xsSJw6jJBL", "authors": [], "pdf_url": "https://openreview.net/pdf/855cc8440056500cb6fb01abe0738c9662188062.pdf", "summary_cn": "Photon框架通过可变长度令牌序列和自适应令牌调度，高效处理3D医学影像，降低计算成本并保持体积连续性，在医学视觉问答任务中实现高精度与资源节省。", "keywords": ["3D医学影像", "多模态大语言模型", "令牌压缩", "自适应调度", "视觉问答", "计算效率"], "triple": {"method": "可变长度令牌序列与自适应令牌调度", "result": "降低计算成本，加速训练推理，保持高精度", "contribution": "提出高效3D医学影像理解框架，提升多模态模型可扩展性"}}
{"venue": "ICLR", "search_title": "LiveClin: A Live Clinical Benchmark without Leakage", "full_title": "LiveClin: A Live Clinical Benchmark without Leakage", "url": "https://openreview.net/forum?id=E0WSAugJ0j", "year": 2026, "is_main_conference": true, "abstract_snippet": "The reliability of medical LLM evaluation is critically undermined by data contamination and knowledge obsolescence, leading to inflated scores on static benchmarks. To address these challenges, we introduce LiveClin, a live benchmark designed for the faithful replication of clinical practice. Built from contemporary, peer-reviewed case reports and updated biannually, LiveClin ensures clinical currency and resists data contamination. Using a verified AI-human workflow involving 239 physicians, we transform authentic patient cases into complex, multimodal evaluation scenarios that span the entire clinical pathway. The benchmark currently comprises 1,407 case reports and 6,605 questions. Our evaluation of 26 models on LiveClin reveals the profound difficulty of these real-world scenarios, with the top-performing model achieving a Case Accuracy of just 35.7%. We find that the era of \"free lunch\" improvements from simple model scaling is over, as newer models do not consistently outperform their predecessors. Furthermore, our analysis uncovers distinct reasoning weaknesses across model classes. LiveClin thus provides a continuously evolving, clinically-grounded framework to steer the development of medical LLMs towards greater reliability and real-world utility.", "abstract": "The reliability of medical LLM evaluation is critically undermined by data contamination and knowledge obsolescence, leading to inflated scores on static benchmarks. To address these challenges, we introduce LiveClin, a live benchmark designed for the faithful replication of clinical practice. Built from contemporary, peer-reviewed case reports and updated biannually, LiveClin ensures clinical currency and resists data contamination. Using a verified AI-human workflow involving 239 physicians, we transform authentic patient cases into complex, multimodal evaluation scenarios that span the entire clinical pathway. The benchmark currently comprises 1,407 case reports and 6,605 questions. Our evaluation of 26 models on LiveClin reveals the profound difficulty of these real-world scenarios, with the top-performing model achieving a Case Accuracy of just 35.7%. We find that the era of \"free lunch\" improvements from simple model scaling is over, as newer models do not consistently outperform their predecessors. Furthermore, our analysis uncovers distinct reasoning weaknesses across model classes. LiveClin thus provides a continuously evolving, clinically-grounded framework to steer the development of medical LLMs towards greater reliability and real-world utility.", "abstract_source_venue": "OpenReviewAPI", "abstract_source_url": "https://api2.openreview.net/notes?id=E0WSAugJ0j", "openreview_id": "E0WSAugJ0j", "openreview_forum_id": "E0WSAugJ0j", "authors": [], "pdf_url": "https://openreview.net/pdf/6847a1d5164668634b9180636389b5edd155ac03.pdf", "summary_cn": "LiveClin是一个动态更新的临床基准，基于真实病例构建，用于评估医学大语言模型。评估显示模型在复杂临床场景中表现不佳，准确率仅35.7%，揭示了模型扩展的局限性。", "keywords": ["医学大语言模型", "临床基准", "数据污染", "动态评估", "病例报告", "模型性能"], "triple": {"method": "基于真实病例构建动态多模态基准", "result": "顶级模型准确率仅35.7%，新模型未持续超越旧模型", "contribution": "提供持续演进的临床评估框架"}}
{"venue": "ICLR", "search_title": "VLM-SubtleBench: How Far Are VLMs from Human-Level Subtle Comparative Reasoning?", "full_title": "VLM-SubtleBench: How Far Are VLMs from Human-Level Subtle Comparative Reasoning?", "url": "https://openreview.net/forum?id=pBTXsu1i77", "year": 2026, "is_main_conference": true, "abstract_snippet": "The ability to distinguish subtle differences between visually similar images is essential for diverse domains such as industrial anomaly detection, medical imaging, and aerial surveillance. While comparative reasoning benchmarks for vision-language models (VLMs) have recently emerged, they primarily focus on images with large, salient differences and fail to capture the nuanced reasoning required for real-world applications. In this work, we introduce **VLM-SubtleBench**, a benchmark designed to evaluate VLMs on *subtle comparative reasoning*. Our benchmark covers ten difference types—Attribute, State, Emotion, Temporal, Spatial, Existence, Quantity, Quality, Viewpoint, and Action—and curate paired question–image sets reflecting these fine-grained variations. Unlike prior benchmarks restricted to natural image datasets, our benchmark spans diverse domains, including industrial, aerial, and medical imagery. Through extensive evaluation of both proprietary and open-source VLMs, we reveal systematic gaps between model and human performance across difference types and domains, and provide controlled analyses highlighting where VLMs’ reasoning sharply deteriorates. Together, our benchmark and findings establish a foundation for advancing VLMs toward human-level comparative reasoning.", "abstract": "The ability to distinguish subtle differences between visually similar images is essential for diverse domains such as industrial anomaly detection, medical imaging, and aerial surveillance. While comparative reasoning benchmarks for vision-language models (VLMs) have recently emerged, they primarily focus on images with large, salient differences and fail to capture the nuanced reasoning required for real-world applications. In this work, we introduce **VLM-SubtleBench**, a benchmark designed to evaluate VLMs on *subtle comparative reasoning*. Our benchmark covers ten difference types—Attribute, State, Emotion, Temporal, Spatial, Existence, Quantity, Quality, Viewpoint, and Action—and curate paired question–image sets reflecting these fine-grained variations. Unlike prior benchmarks restricted to natural image datasets, our benchmark spans diverse domains, including industrial, aerial, and medical imagery. Through extensive evaluation of both proprietary and open-source VLMs, we reveal systematic gaps between model and human performance across difference types and domains, and provide controlled analyses highlighting where VLMs’ reasoning sharply deteriorates. Together, our benchmark and findings establish a foundation for advancing VLMs toward human-level comparative reasoning.", "abstract_source_venue": "OpenReviewAPI", "abstract_source_url": "https://api2.openreview.net/notes?id=pBTXsu1i77", "openreview_id": "pBTXsu1i77", "openreview_forum_id": "pBTXsu1i77", "authors": [], "pdf_url": "https://openreview.net/pdf/d0d62dc6e108ceee41ca33730cb232524d40df5d.pdf", "summary_cn": "本文提出VLM-SubtleBench基准，评估视觉语言模型在细微图像差异上的比较推理能力，发现模型与人类表现存在系统差距。", "keywords": ["视觉语言模型", "比较推理", "基准测试", "细微差异", "多领域评估", "性能差距"], "triple": {"method": "构建涵盖十类差异的多领域图像-问题对基准", "result": "模型在细微推理上显著落后于人类，性能随差异类型和领域变化", "contribution": "为提升VLM至人类水平比较推理奠定基础"}}
{"venue": "ICLR", "search_title": "Dual Distillation for Few-Shot Anomaly Detection", "full_title": "Dual Distillation for Few-Shot Anomaly Detection", "url": "https://openreview.net/forum?id=tRO6G20Qba", "year": 2026, "is_main_conference": true, "abstract_snippet": "Anomaly detection is a critical task in computer vision with profound implications for medical imaging, where identifying pathologies early can directly impact patient outcomes. While recent unsupervised anomaly detection approaches show promise, they require substantial normal training data and struggle to generalize across anatomical contexts. We introduce D$^2$4FAD, a novel dual distillation framework for few-shot anomaly detection that identifies anomalies in previously unseen tasks using only a small number of normal reference images. Our approach leverages a pre-trained encoder as a teacher network to extract multi-scale features from both support and query images, while a student decoder learns to distill knowledge from the teacher on query images and self-distill on support images. We further propose a learn-to-weight mechanism that dynamically assesses the reference value of each support image conditioned on the query, optimizing anomaly detection performance. To evaluate our method, we curate a comprehensive benchmark dataset comprising 13,084 images across four organs, four imaging modalities, and five disease categories. Extensive experiments demonstrate that D$^2$4FAD significantly outperforms existing approaches, establishing a new state-of-the-art in few-shot medical anomaly detection.", "abstract": "Anomaly detection is a critical task in computer vision with profound implications for medical imaging, where identifying pathologies early can directly impact patient outcomes. While recent unsupervised anomaly detection approaches show promise, they require substantial normal training data and struggle to generalize across anatomical contexts. We introduce D$^2$4FAD, a novel dual distillation framework for few-shot anomaly detection that identifies anomalies in previously unseen tasks using only a small number of normal reference images. Our approach leverages a pre-trained encoder as a teacher network to extract multi-scale features from both support and query images, while a student decoder learns to distill knowledge from the teacher on query images and self-distill on support images. We further propose a learn-to-weight mechanism that dynamically assesses the reference value of each support image conditioned on the query, optimizing anomaly detection performance. To evaluate our method, we curate a comprehensive benchmark dataset comprising 13,084 images across four organs, four imaging modalities, and five disease categories. Extensive experiments demonstrate that D$^2$4FAD significantly outperforms existing approaches, establishing a new state-of-the-art in few-shot medical anomaly detection.", "abstract_source_venue": "OpenReviewAPI", "abstract_source_url": "https://api2.openreview.net/notes?id=tRO6G20Qba", "openreview_id": "tRO6G20Qba", "openreview_forum_id": "tRO6G20Qba", "authors": [], "pdf_url": "https://openreview.net/pdf/04ed891c8e188a04145664f0c50bc9795f7972ef.pdf", "summary_cn": "提出D$^2$4FAD双蒸馏框架，用于少样本医学异常检测，仅需少量正常参考图像，在跨器官、模态和疾病的综合基准上实现最优性能。", "keywords": ["少样本异常检测", "双蒸馏框架", "医学影像", "自蒸馏", "动态加权", "跨域泛化"], "triple": {"method": "双蒸馏框架与动态加权机制", "result": "在综合医学影像基准上显著超越现有方法", "contribution": "实现少样本跨域异常检测新SOTA"}}
{"venue": "ICLR", "search_title": "CONSIGN: Conformal Segmentation Informed by Spatial Groupings via Decomposition", "full_title": "CONSIGN: Conformal Segmentation Informed by Spatial Groupings via Decomposition", "url": "https://openreview.net/forum?id=pHF5CXB0YH", "year": 2026, "is_main_conference": true, "abstract_snippet": "Most machine learning-based image segmentation models produce pixel-wise confidence scores that represent the model’s predicted probability for each class label at every pixel. While this information can be particularly valuable in high-stakes domains such as medical imaging, these scores are heuristic in nature and do not constitute rigorous quantitative uncertainty estimates.\nConformal prediction (CP) provides a principled framework for transforming heuristic confidence scores into statistically valid uncertainty estimates. However, applying CP directly to image segmentation ignores the spatial correlations between pixels, a fundamental characteristic of image data. This can result in overly conservative and less interpretable uncertainty estimates.\nTo address this, we propose CONSIGN (*Conformal Segmentation Informed by Spatial Groupings via Decomposition*), a CP-based method that incorporates spatial correlations to improve uncertainty quantification in image segmentation.\nOur method generates meaningful prediction sets that come with user-specified, high-probability error guarantees.\nIt is compatible with any pre-trained segmentation model capable of generating multiple sample outputs.\nWe evaluate CONSIGN against two CP baselines across three medical imaging datasets and two COCO dataset subsets, using three different pre-trained segmentation models. Results demonstrate that accounting for spatial structure significantly improves performance across multiple metrics and enhances the quality of uncertainty estimates.", "abstract": "Most machine learning-based image segmentation models produce pixel-wise confidence scores that represent the model’s predicted probability for each class label at every pixel. While this information can be particularly valuable in high-stakes domains such as medical imaging, these scores are heuristic in nature and do not constitute rigorous quantitative uncertainty estimates.\nConformal prediction (CP) provides a principled framework for transforming heuristic confidence scores into statistically valid uncertainty estimates. However, applying CP directly to image segmentation ignores the spatial correlations between pixels, a fundamental characteristic of image data. This can result in overly conservative and less interpretable uncertainty estimates.\nTo address this, we propose CONSIGN (*Conformal Segmentation Informed by Spatial Groupings via Decomposition*), a CP-based method that incorporates spatial correlations to improve uncertainty quantification in image segmentation.\nOur method generates meaningful prediction sets that come with user-specified, high-probability error guarantees.\nIt is compatible with any pre-trained segmentation model capable of generating multiple sample outputs.\nWe evaluate CONSIGN against two CP baselines across three medical imaging datasets and two COCO dataset subsets, using three different pre-trained segmentation models. Results demonstrate that accounting for spatial structure significantly improves performance across multiple metrics and enhances the quality of uncertainty estimates.", "abstract_source_venue": "OpenReviewAPI", "abstract_source_url": "https://api2.openreview.net/notes?id=pHF5CXB0YH", "openreview_id": "pHF5CXB0YH", "openreview_forum_id": "pHF5CXB0YH", "authors": [], "pdf_url": "https://openreview.net/pdf/32d4e15bc1ed0eba4523776ef3ef41b08dbfaf78.pdf", "summary_cn": "CONSIGN方法通过整合空间相关性，改进图像分割中的不确定性量化，提供统计有效的预测集，在医学影像和COCO数据集上表现优于基线。", "keywords": ["图像分割", "不确定性量化", "空间相关性", "医学影像", "统计保证"], "triple": {"method": "基于共形预测整合空间分组", "result": "提升不确定性估计性能", "contribution": "提供统计有效的预测集"}}
{"venue": "ICLR", "search_title": "NurValues: Real-World Nursing Values Evaluation for Large Language Models in Clinical Context", "full_title": "NurValues: Real-World Nursing Values Evaluation for Large Language Models in Clinical Context", "url": "https://openreview.net/forum?id=ZTAvANYFL5", "year": 2026, "is_main_conference": true, "abstract_snippet": "While LLMs have demonstrated medical knowledge and conversational ability, their deployment in clinical practice raises new risks: patients may place greater trust in LLM-generated responses than in nurses' professional judgments, potentially intensifying nurse–patient conflicts. Such risks highlight the urgent need of evaluating whether LLMs align with the core nursing values upheld by human nurses. This work introduces the first benchmark for nursing value alignment, consisting of five core value dimensions distilled from international nursing codes: _Altruism_, _Human Dignity_, _Integrity_, _Justice_, and _Professionalism_. We define two-level tasks on the benchmark, considering the two characteristics of emerging nurse–patient conflicts. The **Easy-Level** dataset consists of 2,200 value-aligned and value-violating instances, which are collected through a five-month longitudinal field study across three hospitals of varying tiers; The **Hard-Level** dataset is comprised of 2,200 dialogue-based instances that embed contextual cues and subtle misleading signals, which increase adversarial complexity and better reflect the subjectivity and bias of narrators in the context of emerging nurse-patient conflicts. We evaluate a total of 23 SoTA LLMs on their ability to align with nursing values, and find that general LLMs outperform medical ones, and _Justice_ is the hardest value dimension. As the first real-world benchmark for healthcare value alignment, NurValues provides novel insights into how LLMs navigate ethical challenges in clinician–patient interactions.", "abstract": "While LLMs have demonstrated medical knowledge and conversational ability, their deployment in clinical practice raises new risks: patients may place greater trust in LLM-generated responses than in nurses' professional judgments, potentially intensifying nurse–patient conflicts. Such risks highlight the urgent need of evaluating whether LLMs align with the core nursing values upheld by human nurses. This work introduces the first benchmark for nursing value alignment, consisting of five core value dimensions distilled from international nursing codes: _Altruism_, _Human Dignity_, _Integrity_, _Justice_, and _Professionalism_. We define two-level tasks on the benchmark, considering the two characteristics of emerging nurse–patient conflicts. The **Easy-Level** dataset consists of 2,200 value-aligned and value-violating instances, which are collected through a five-month longitudinal field study across three hospitals of varying tiers; The **Hard-Level** dataset is comprised of 2,200 dialogue-based instances that embed contextual cues and subtle misleading signals, which increase adversarial complexity and better reflect the subjectivity and bias of narrators in the context of emerging nurse-patient conflicts. We evaluate a total of 23 SoTA LLMs on their ability to align with nursing values, and find that general LLMs outperform medical ones, and _Justice_ is the hardest value dimension. As the first real-world benchmark for healthcare value alignment, NurValues provides novel insights into how LLMs navigate ethical challenges in clinician–patient interactions.", "abstract_source_venue": "OpenReviewAPI", "abstract_source_url": "https://api2.openreview.net/notes?id=ZTAvANYFL5", "openreview_id": "ZTAvANYFL5", "openreview_forum_id": "ZTAvANYFL5", "authors": [], "pdf_url": "https://openreview.net/pdf/b493335d4170e574048a5f727f6cb67a2d5474a4.pdf", "summary_cn": "本研究提出首个护理价值对齐基准NurValues，基于国际护理准则定义五个核心价值维度，通过易、难两级任务评估23个LLMs，发现通用模型优于医疗模型，正义维度最难对齐。", "keywords": ["护理价值对齐", "大语言模型", "临床伦理", "基准评估", "医患冲突", "真实世界数据"], "triple": {"method": "构建两级任务基准（易级实例与难级对话）", "result": "通用LLMs优于医疗LLMs，正义维度最难对齐", "contribution": "首个真实世界护理价值对齐基准，为LLMs临床伦理评估提供新视角"}}
{"venue": "ICLR", "search_title": "CounselBench: A Large-Scale Expert Evaluation and Adversarial Benchmarking of Large Language Models in Mental Health Question Answering", "full_title": "CounselBench: A Large-Scale Expert Evaluation and Adversarial Benchmarking of Large Language Models in Mental Health Question Answering", "url": "https://openreview.net/forum?id=8MBYRZHVWT", "year": 2026, "is_main_conference": true, "abstract_snippet": "Medical question answering (QA) benchmarks often focus on multiple-choice or fact-based tasks, leaving open-ended answers to real patient questions underexplored. This gap is particularly critical in mental health, where patient questions often mix symptoms, treatment concerns, and emotional needs, requiring answers that balance clinical caution with contextual sensitivity.\nWe present CounselBench, a large-scale benchmark developed with 100 mental health professionals to evaluate and stress-test large language models (LLMs) in realistic help-seeking scenarios. The first component, CounselBench-EVAL, contains 2,000 expert evaluations of answers from GPT-4, LLaMA 3, Gemini, and online human therapists on patient questions from the public forum CounselChat. Each answer is rated across six clinically grounded dimensions, with span-level annotations and written rationales. Expert evaluations show that while LLMs achieve high scores on several dimensions, they also exhibit recurring issues, including unconstructive feedback, overgeneralization, and limited personalization or relevance. Responses were frequently flagged for safety risks, most notably unauthorized medical advice. Follow-up experiments show that LLM judges systematically overrate model responses and overlook safety concerns identified by human experts. To probe failure modes more directly, we construct CounselBench-Adv, an adversarial dataset of 120 expert-authored mental health questions designed to trigger specific model issues. Expert evaluation of 1,080 responses from nine LLMs reveals consistent, model-specific failure patterns. Together, CounselBench establishes a clinically grounded framework for benchmarking LLMs in mental health QA.", "abstract": "Medical question answering (QA) benchmarks often focus on multiple-choice or fact-based tasks, leaving open-ended answers to real patient questions underexplored. This gap is particularly critical in mental health, where patient questions often mix symptoms, treatment concerns, and emotional needs, requiring answers that balance clinical caution with contextual sensitivity.\nWe present CounselBench, a large-scale benchmark developed with 100 mental health professionals to evaluate and stress-test large language models (LLMs) in realistic help-seeking scenarios. The first component, CounselBench-EVAL, contains 2,000 expert evaluations of answers from GPT-4, LLaMA 3, Gemini, and online human therapists on patient questions from the public forum CounselChat. Each answer is rated across six clinically grounded dimensions, with span-level annotations and written rationales. Expert evaluations show that while LLMs achieve high scores on several dimensions, they also exhibit recurring issues, including unconstructive feedback, overgeneralization, and limited personalization or relevance. Responses were frequently flagged for safety risks, most notably unauthorized medical advice. Follow-up experiments show that LLM judges systematically overrate model responses and overlook safety concerns identified by human experts. To probe failure modes more directly, we construct CounselBench-Adv, an adversarial dataset of 120 expert-authored mental health questions designed to trigger specific model issues. Expert evaluation of 1,080 responses from nine LLMs reveals consistent, model-specific failure patterns. Together, CounselBench establishes a clinically grounded framework for benchmarking LLMs in mental health QA.", "abstract_source_venue": "OpenReviewAPI", "abstract_source_url": "https://api2.openreview.net/notes?id=8MBYRZHVWT", "openreview_id": "8MBYRZHVWT", "openreview_forum_id": "8MBYRZHVWT", "authors": [], "pdf_url": "https://openreview.net/pdf/4dd193bce232058aba85627a12cd7fcea5324adb.pdf", "summary_cn": "CounselBench 是一个由心理健康专家构建的大规模基准，用于评估大语言模型在心理健康问答中的表现。研究发现模型在安全性和个性化方面存在缺陷，专家评估揭示了系统性风险。", "keywords": ["心理健康问答", "专家评估", "大语言模型基准", "安全风险", "对抗性测试", "临床评估"], "triple": {"method": "构建包含专家评估和对抗性问题的基准", "result": "LLMs 在多个维度得分高但存在安全与个性化问题，自动评估高估模型表现", "contribution": "建立了临床基础的 LLMs 心理健康问答评估框架"}}
{"venue": "ICLR", "search_title": "OmniCT: Towards a Unified Slice-Volume LVLM for Comprehensive CT Analysis", "full_title": "OmniCT: Towards a Unified Slice-Volume LVLM for Comprehensive CT Analysis", "url": "https://openreview.net/forum?id=nrZI64gTvC", "year": 2026, "is_main_conference": true, "abstract_snippet": "Computed Tomography (CT) is one of the most widely used and diagnostically information-dense imaging modalities, covering critical organs such as the heart, lungs, liver, and colon. Clinical interpretation relies on both \\textbf{slice-driven} local features (e.g., sub-centimeter nodules, lesion boundaries) and \\textbf{volume-driven} spatial representations (e.g., tumor infiltration, inter-organ anatomical relations). \nHowever, existing Large Vision–Language Models (LVLMs) remain fragmented in CT slice versus volumetric understanding: slice-driven LVLMs show strong generalization but lack cross-slice spatial consistency, while volume-driven LVLMs explicitly capture volumetric semantics but suffer from coarse granularity and poor compatibility with slice inputs. The absence of a unified modeling paradigm constitutes a major bottleneck for the clinical translation of medical LVLMs. \nWe present \\textbf{OmniCT}, a powerful unified slice–volume LVLM for CT scans, which makes three contributions: \n\\textbf{(i) Spatial Consistency Enhancement (SCE):} volumetric slice composition combined with tri-axial positional encoding introduces volumetric consistency, and an MoE hybird projection enables efficient slice–volume adaptation; \n\\textbf{(ii) Organ-level Semantic Enhancement (OSE):} segmentation and ROI localization explicitly align anatomical regions, emphasizing lesion- and organ-level semantics; \n\\textbf{(iii) MedEval-CT:} the largest slice–volume CT dataset and hybrid benchmark integrates multi-level metrics for unified evaluation. \nOmniCT consistently outperforms existing methods with a substantial margin across diverse clinical tasks, satisfies both micro-level detail sensitivity and macro-level spatial reasoning, and establishes a new paradigm for cross-dimensional medical imaging modeling. \nOur project is available at \\href{https://anonymous.4open.science/r/OmniCT}{link}.", "abstract": "Computed Tomography (CT) is one of the most widely used and diagnostically information-dense imaging modalities, covering critical organs such as the heart, lungs, liver, and colon. Clinical interpretation relies on both \\textbf{slice-driven} local features (e.g., sub-centimeter nodules, lesion boundaries) and \\textbf{volume-driven} spatial representations (e.g., tumor infiltration, inter-organ anatomical relations). \nHowever, existing Large Vision–Language Models (LVLMs) remain fragmented in CT slice versus volumetric understanding: slice-driven LVLMs show strong generalization but lack cross-slice spatial consistency, while volume-driven LVLMs explicitly capture volumetric semantics but suffer from coarse granularity and poor compatibility with slice inputs. The absence of a unified modeling paradigm constitutes a major bottleneck for the clinical translation of medical LVLMs. \nWe present \\textbf{OmniCT}, a powerful unified slice–volume LVLM for CT scans, which makes three contributions: \n\\textbf{(i) Spatial Consistency Enhancement (SCE):} volumetric slice composition combined with tri-axial positional encoding introduces volumetric consistency, and an MoE hybird projection enables efficient slice–volume adaptation; \n\\textbf{(ii) Organ-level Semantic Enhancement (OSE):} segmentation and ROI localization explicitly align anatomical regions, emphasizing lesion- and organ-level semantics; \n\\textbf{(iii) MedEval-CT:} the largest slice–volume CT dataset and hybrid benchmark integrates multi-level metrics for unified evaluation. \nOmniCT consistently outperforms existing methods with a substantial margin across diverse clinical tasks, satisfies both micro-level detail sensitivity and macro-level spatial reasoning, and establishes a new paradigm for cross-dimensional medical imaging modeling. \nOur project is available at \\href{https://anonymous.4open.science/r/OmniCT}{link}.", "abstract_source_venue": "OpenReviewAPI", "abstract_source_url": "https://api2.openreview.net/notes?id=nrZI64gTvC", "openreview_id": "nrZI64gTvC", "openreview_forum_id": "nrZI64gTvC", "authors": [], "pdf_url": "https://openreview.net/pdf/2fd2a30a147c15fc0e7de70a6b4e3e705e7887a1.pdf", "summary_cn": "OmniCT提出统一切片-体积LVLM用于CT分析，通过空间一致性增强和器官级语义增强，在多种临床任务中显著优于现有方法，实现细节敏感与空间推理的平衡。", "keywords": ["CT分析", "大视觉语言模型", "切片-体积统一", "空间一致性", "器官语义增强", "医学影像建模"], "triple": {"method": "空间一致性增强与器官级语义增强", "result": "在多种临床任务中显著优于现有方法", "contribution": "建立跨维度医学影像建模新范式"}}
{"venue": "ICLR", "search_title": "Distributional Consistency Loss: Beyond Pointwise Data Terms in Inverse Problems", "full_title": "Distributional Consistency Loss: Beyond Pointwise Data Terms in Inverse Problems", "url": "https://openreview.net/forum?id=oX7fDiasfK", "year": 2026, "is_main_conference": true, "abstract_snippet": "Recovering true signals from noisy measurements is a central challenge in inverse problems spanning medical imaging, geophysics, and signal processing. Current solutions nearly always balance prior assumptions regarding the true signal (regularization) with agreement to noisy measured data (data fidelity). Conventional data fidelity loss functions, such as mean-squared error (MSE) or negative log-likelihood, seek pointwise agreement with noisy measurements, often leading to overfitting to noise. In this work, we instead evaluate data fidelity collectively by testing whether the observed measurements are statistically consistent with the noise distributions implied by the current estimate. We adopt this aggregated perspective and introduce $\\textit{distributional consistency (DC) loss}$, a data-fidelity objective that replaces pointwise matching with distribution-level calibration. DC loss acts as a direct and practical plug-in replacement for standard data consistency terms: i) it is compatible with modern unsupervised regularizers that operate without paired measurement–ground-truth data, ii) it is optimized in the same way as traditional losses, and iii) it avoids overfitting to measurement noise even without the use of priors. Its scope naturally fits many practical inverse problems where the measurement-noise distribution is known and where the measured dataset consists of many independent noisy values. We demonstrate efficacy in two key example application areas: i) in image denoising with deep image prior, using DC instead of MSE loss removes the need for early stopping and achieves higher PSNR; ii) in medical image reconstruction from Poisson-noisy data, DC loss reduces artifacts in highly-iterated reconstructions and enhances the efficacy of hand-crafted regularization. These results position DC loss as a statistically grounded, performance-enhancing alternative to conventional fidelity losses for an important class of unsupervised noise-dominated inverse problems.", "abstract": "Recovering true signals from noisy measurements is a central challenge in inverse problems spanning medical imaging, geophysics, and signal processing. Current solutions nearly always balance prior assumptions regarding the true signal (regularization) with agreement to noisy measured data (data fidelity). Conventional data fidelity loss functions, such as mean-squared error (MSE) or negative log-likelihood, seek pointwise agreement with noisy measurements, often leading to overfitting to noise. In this work, we instead evaluate data fidelity collectively by testing whether the observed measurements are statistically consistent with the noise distributions implied by the current estimate. We adopt this aggregated perspective and introduce $\\textit{distributional consistency (DC) loss}$, a data-fidelity objective that replaces pointwise matching with distribution-level calibration. DC loss acts as a direct and practical plug-in replacement for standard data consistency terms: i) it is compatible with modern unsupervised regularizers that operate without paired measurement–ground-truth data, ii) it is optimized in the same way as traditional losses, and iii) it avoids overfitting to measurement noise even without the use of priors. Its scope naturally fits many practical inverse problems where the measurement-noise distribution is known and where the measured dataset consists of many independent noisy values. We demonstrate efficacy in two key example application areas: i) in image denoising with deep image prior, using DC instead of MSE loss removes the need for early stopping and achieves higher PSNR; ii) in medical image reconstruction from Poisson-noisy data, DC loss reduces artifacts in highly-iterated reconstructions and enhances the efficacy of hand-crafted regularization. These results position DC loss as a statistically grounded, performance-enhancing alternative to conventional fidelity losses for an important class of unsupervised noise-dominated inverse problems.", "abstract_source_venue": "OpenReviewAPI", "abstract_source_url": "https://api2.openreview.net/notes?id=oX7fDiasfK", "openreview_id": "oX7fDiasfK", "openreview_forum_id": "oX7fDiasfK", "authors": [], "pdf_url": "https://openreview.net/pdf/7329f559525b886abb4f9a218922757188ec6989.pdf", "summary_cn": "提出分布一致性损失，替代传统逐点数据保真度损失，通过分布级校准避免噪声过拟合，提升无监督逆问题性能。", "keywords": ["分布一致性损失", "逆问题", "数据保真度", "噪声过拟合", "无监督学习", "医学成像"], "triple": {"method": "分布级校准替代逐点匹配", "result": "提升去噪与重建质量，避免早停", "contribution": "为无监督噪声主导逆问题提供统计基础替代损失"}}
{"venue": "ICLR", "search_title": "CardioComposer: Leveraging Differentiable Geometry for Compositional Control of Anatomical Diffusion Models", "full_title": "CardioComposer: Leveraging Differentiable Geometry for Compositional Control of Anatomical Diffusion Models", "url": "https://openreview.net/forum?id=JyboUMeEUi", "year": 2026, "is_main_conference": true, "abstract_snippet": "Generative models of 3D cardiovascular anatomy can synthesize informative structures for clinical research and medical device evaluation, but face a trade-off between geometric controllability and realism. We propose CardioComposer: a programmable, inference-time framework for generating multi-class anatomical label maps based on interpretable ellipsoidal primitives. These primitives represent geometric attributes such as the size, shape, and position of discrete substructures. We specifically develop differentiable measurement functions based on voxel-wise geometric moments, enabling loss-based gradient guidance during diffusion model sampling. We demonstrate that these losses can constrain individual geometric attributes in a disentangled manner and provide compositional control over multiple substructures. Finally, we show that our method is compatible with a wide array of anatomical systems containing non-convex substructures, spanning cardiac, vascular, and skeletal organs.", "abstract": "Generative models of 3D cardiovascular anatomy can synthesize informative structures for clinical research and medical device evaluation, but face a trade-off between geometric controllability and realism. We propose CardioComposer: a programmable, inference-time framework for generating multi-class anatomical label maps based on interpretable ellipsoidal primitives. These primitives represent geometric attributes such as the size, shape, and position of discrete substructures. We specifically develop differentiable measurement functions based on voxel-wise geometric moments, enabling loss-based gradient guidance during diffusion model sampling. We demonstrate that these losses can constrain individual geometric attributes in a disentangled manner and provide compositional control over multiple substructures. Finally, we show that our method is compatible with a wide array of anatomical systems containing non-convex substructures, spanning cardiac, vascular, and skeletal organs.", "abstract_source_venue": "OpenReviewAPI", "abstract_source_url": "https://api2.openreview.net/notes?id=JyboUMeEUi", "openreview_id": "JyboUMeEUi", "openreview_forum_id": "JyboUMeEUi", "authors": [], "pdf_url": "https://openreview.net/pdf/8eae4e829d0b1d4ce2e4939f31af00f724977c3b.pdf", "summary_cn": "CardioComposer 提出基于可解释椭球基元的可编程框架，通过可微分几何测量函数在扩散模型采样中实现解剖结构的解耦与组合控制，提升生成真实性与可控性。", "keywords": ["解剖生成模型", "可微分几何", "扩散模型", "心血管解剖", "组合控制", "几何基元"], "triple": {"method": "基于椭球基元的可微分几何测量函数", "result": "实现解剖结构的解耦与组合控制", "contribution": "提升生成模型的真实性与几何可控性"}}
{"venue": "ICLR", "search_title": "Towards Understanding The Calibration Benefits of Sharpness-Aware Minimization", "full_title": "Towards Understanding The Calibration Benefits of Sharpness-Aware Minimization", "url": "https://openreview.net/forum?id=c0ERcCz6lD", "year": 2026, "is_main_conference": true, "abstract_snippet": "Deep neural networks have been increasingly used in safety-critical applications such as medical diagnosis and autonomous driving. However, many studies suggest that they are prone to being poorly calibrated and have a propensity for overconfidence, which may have disastrous consequences. In this paper, unlike standard training such as stochastic gradient descent, we show that the recently proposed sharpness-aware minimization (SAM) counteracts this tendency towards overconfidence. The theoretical analysis suggests that SAM allows us to learn models that are already well-calibrated by implicitly maximizing the entropy of the predictive distribution. Inspired by this finding, we further propose a variant of SAM, coined as CSAM, to ameliorate model calibration. Extensive experiments on various datasets, including ImageNet-1K, demonstrate the benefits of SAM in reducing calibration error. Meanwhile, CSAM performs even better than SAM and consistently achieves lower calibration error than other approaches.", "abstract": "Deep neural networks have been increasingly used in safety-critical applications such as medical diagnosis and autonomous driving. However, many studies suggest that they are prone to being poorly calibrated and have a propensity for overconfidence, which may have disastrous consequences. In this paper, unlike standard training such as stochastic gradient descent, we show that the recently proposed sharpness-aware minimization (SAM) counteracts this tendency towards overconfidence. The theoretical analysis suggests that SAM allows us to learn models that are already well-calibrated by implicitly maximizing the entropy of the predictive distribution. Inspired by this finding, we further propose a variant of SAM, coined as CSAM, to ameliorate model calibration. Extensive experiments on various datasets, including ImageNet-1K, demonstrate the benefits of SAM in reducing calibration error. Meanwhile, CSAM performs even better than SAM and consistently achieves lower calibration error than other approaches.", "abstract_source_venue": "OpenReviewAPI", "abstract_source_url": "https://api2.openreview.net/notes?id=c0ERcCz6lD", "openreview_id": "c0ERcCz6lD", "openreview_forum_id": "c0ERcCz6lD", "authors": [], "pdf_url": "https://openreview.net/pdf/492eab9fc0738808d289ac56d737c154dff1a45f.pdf", "summary_cn": "研究表明，锐度感知最小化（SAM）能有效减少深度神经网络的过度自信，提高校准性。提出的CSAM变体进一步优化校准性能。", "keywords": ["锐度感知最小化", "模型校准", "过度自信", "深度神经网络", "CSAM", "熵最大化"], "triple": {"method": "SAM与CSAM方法", "result": "降低校准误差，优于标准训练", "contribution": "提升模型校准性，减少过度自信"}}
{"venue": "ICLR", "search_title": "Patronus: Interpretable Diffusion Models with Prototypes", "full_title": "Patronus: Interpretable Diffusion Models with Prototypes", "url": "https://openreview.net/forum?id=1bz8CA8gPo", "year": 2026, "is_main_conference": true, "abstract_snippet": "Uncovering the opacity of diffusion-based generative models is urgently needed, as their applications continue to expand while their underlying procedures largely remain a black box. \nWith a critical question -- how can the diffusion generation process be interpreted and understood? -- we proposed \\textit{Patronus}, an interpretable diffusion model that incorporates a prototypical network to encode semantics in visual patches, revealing \\textit{what} visual patterns are learned and \\textit{where} and \\textit{when} they emerge throughout denoising.\nThis interpretability of Patronus provides deeper insights into the generative mechanism, enabling the detection of shortcut learning via unwanted correlations and the tracing of semantic emergence across timesteps. We evaluate \\textit{Patronus} on four natural image datasets and one medical imaging dataset, demonstrating both faithful interpretability and strong generative performance. With this work, we open new avenues for understanding and steering diffusion models through prototype-based interpretability.", "abstract": "Uncovering the opacity of diffusion-based generative models is urgently needed, as their applications continue to expand while their underlying procedures largely remain a black box. \nWith a critical question -- how can the diffusion generation process be interpreted and understood? -- we proposed \\textit{Patronus}, an interpretable diffusion model that incorporates a prototypical network to encode semantics in visual patches, revealing \\textit{what} visual patterns are learned and \\textit{where} and \\textit{when} they emerge throughout denoising.\nThis interpretability of Patronus provides deeper insights into the generative mechanism, enabling the detection of shortcut learning via unwanted correlations and the tracing of semantic emergence across timesteps. We evaluate \\textit{Patronus} on four natural image datasets and one medical imaging dataset, demonstrating both faithful interpretability and strong generative performance. With this work, we open new avenues for understanding and steering diffusion models through prototype-based interpretability.", "abstract_source_venue": "OpenReviewAPI", "abstract_source_url": "https://api2.openreview.net/notes?id=1bz8CA8gPo", "openreview_id": "1bz8CA8gPo", "openreview_forum_id": "1bz8CA8gPo", "authors": [], "pdf_url": "https://openreview.net/pdf/8782e7d4c0c0f97700ae4aa9d3595187882fad8e.pdf", "summary_cn": "Patronus是一种可解释扩散模型，通过原型网络揭示去噪过程中视觉模式的语义、位置和时序，提升模型透明度并检测捷径学习。", "keywords": ["可解释性", "扩散模型", "原型网络", "语义分析", "去噪过程", "医学影像"], "triple": {"method": "原型网络编码视觉块语义", "result": "揭示模式在去噪中的时空出现", "contribution": "提供扩散模型可解释性新途径"}}
{"venue": "ICLR", "search_title": "Reliable Evaluation of MRI Motion Correction: Dataset and Insights", "full_title": "Reliable Evaluation of MRI Motion Correction: Dataset and Insights", "url": "https://openreview.net/forum?id=5PY8HR2Zz6", "year": 2026, "is_main_conference": true, "abstract_snippet": "Correcting motion artifacts in scientific and medical imaging is important, as they significantly impact image quality. \nHowever, evaluating deep learning-based and classical motion correction methods remains fundamentally difficult due to the lack of accessible ground-truth target data. \nTo address this challenge, we study three evaluation approaches: real-world evaluation based on reference scans, simulated motion, and reference-free evaluation, each with its merits and shortcomings. \nTo enable evaluation with real-world motion artifacts, we release PMoC3D, a dataset consisting of unprocessed $\\textbf{P}$aired $\\textbf{Mo}$tion-$\\textbf{C}$orrupted $\\textbf{3D}$ brain MRI data. \nTo advance evaluation quality, we introduce MoMRISim, a  feature-space metric trained for evaluating motion reconstructions. \nWe assess each evaluation approach and find real-world evaluation together with MoMRISim, while not perfect, to be most reliable. \nEvaluation based on simulated motion systematically exaggerates algorithm performance, and reference-free evaluation overrates oversmoothed deep learning outputs.", "abstract": "Correcting motion artifacts in scientific and medical imaging is important, as they significantly impact image quality. \nHowever, evaluating deep learning-based and classical motion correction methods remains fundamentally difficult due to the lack of accessible ground-truth target data. \nTo address this challenge, we study three evaluation approaches: real-world evaluation based on reference scans, simulated motion, and reference-free evaluation, each with its merits and shortcomings. \nTo enable evaluation with real-world motion artifacts, we release PMoC3D, a dataset consisting of unprocessed $\\textbf{P}$aired $\\textbf{Mo}$tion-$\\textbf{C}$orrupted $\\textbf{3D}$ brain MRI data. \nTo advance evaluation quality, we introduce MoMRISim, a  feature-space metric trained for evaluating motion reconstructions. \nWe assess each evaluation approach and find real-world evaluation together with MoMRISim, while not perfect, to be most reliable. \nEvaluation based on simulated motion systematically exaggerates algorithm performance, and reference-free evaluation overrates oversmoothed deep learning outputs.", "abstract_source_venue": "OpenReviewAPI", "abstract_source_url": "https://api2.openreview.net/notes?id=5PY8HR2Zz6", "openreview_id": "5PY8HR2Zz6", "openreview_forum_id": "5PY8HR2Zz6", "authors": [], "pdf_url": "https://openreview.net/pdf/9856e6c5e67d59e4e3f1f9bb5a9e153e18eb1a90.pdf", "summary_cn": "本文提出PMoC3D数据集和MoMRISim评估指标，用于可靠评估MRI运动校正方法，发现真实世界评估结合MoMRISim最可靠。", "keywords": ["MRI运动校正", "评估方法", "PMoC3D数据集", "MoMRISim指标", "深度学习", "运动伪影"], "triple": {"method": "提出PMoC3D数据集和MoMRISim评估指标", "result": "真实世界评估结合MoMRISim最可靠，模拟运动评估高估性能", "contribution": "提供可靠评估框架，促进MRI运动校正方法发展"}}
{"venue": "ICLR", "search_title": "Critic–Adviser–Reviser Cyclic Refinement: Towards High-Quality EMR Corpus Generation with LLMs", "full_title": "Critic–Adviser–Reviser Cyclic Refinement: Towards High-Quality EMR Corpus Generation with LLMs", "url": "https://openreview.net/forum?id=7y11BdJIOp", "year": 2026, "is_main_conference": true, "abstract_snippet": "Electronic medical records (EMRs) are vital for healthcare research, but their use is limited by privacy concerns. Synthetic EMR generation offers a promising alternative, yet most existing methods merely imitate real records without adhering to rigorous clinical quality principles. To address this, we introduce LLM-CARe, a stage-wise cyclic refinement framework that progressively improves EMR quality through three stages, each targeting a specific granularity: corpus, section and document. At each stage, a Critic, an Adviser, and a Reviser collaborate iteratively to evaluate, provide feedback, and refine the drafts. This structured, multi-stage process produces records that better satisfy clinical quality standards. Experiments show that LLM-CARe significantly enhances EMR quality across all levels compared to strong baselines and yields improved performance on real-world clinical tasks such as diagnosis prediction. Unlike prior work, our method requires no real EMR text for training or prompting, demonstrating the effectiveness of stage-wise, cyclic refinement for generating high-quality, privacy-preserving EMR datasets.", "abstract": "Electronic medical records (EMRs) are vital for healthcare research, but their use is limited by privacy concerns. Synthetic EMR generation offers a promising alternative, yet most existing methods merely imitate real records without adhering to rigorous clinical quality principles. To address this, we introduce LLM-CARe, a stage-wise cyclic refinement framework that progressively improves EMR quality through three stages, each targeting a specific granularity: corpus, section and document. At each stage, a Critic, an Adviser, and a Reviser collaborate iteratively to evaluate, provide feedback, and refine the drafts. This structured, multi-stage process produces records that better satisfy clinical quality standards. Experiments show that LLM-CARe significantly enhances EMR quality across all levels compared to strong baselines and yields improved performance on real-world clinical tasks such as diagnosis prediction. Unlike prior work, our method requires no real EMR text for training or prompting, demonstrating the effectiveness of stage-wise, cyclic refinement for generating high-quality, privacy-preserving EMR datasets.", "abstract_source_venue": "OpenReviewAPI", "abstract_source_url": "https://api2.openreview.net/notes?id=7y11BdJIOp", "openreview_id": "7y11BdJIOp", "openreview_forum_id": "7y11BdJIOp", "authors": [], "pdf_url": "https://openreview.net/pdf/114c74ba8ea7161eaca9a9f26721f6f60428b94b.pdf", "summary_cn": "提出LLM-CARe框架，通过批评-建议-修订三阶段循环精炼，无需真实病历训练即可生成高质量、保护隐私的合成电子病历。", "keywords": ["合成电子病历", "大语言模型", "循环精炼", "隐私保护", "临床质量", "多阶段框架"], "triple": {"method": "三阶段循环精炼框架", "result": "生成高质量合成病历，提升诊断预测性能", "contribution": "无需真实病历训练，保护隐私并满足临床标准"}}
{"venue": "ICLR", "search_title": "A Structured, Tagged, and Localized Visual Question Answering Dataset with Full Sentence Answers and Scene Graphs for Chest X-ray Images", "full_title": "A Structured, Tagged, and Localized Visual Question Answering Dataset with Full Sentence Answers and Scene Graphs for Chest X-ray Images", "url": "https://openreview.net/forum?id=LrmyW9JLYq", "year": 2026, "is_main_conference": true, "abstract_snippet": "Visual Question Answering (VQA) enables targeted and context-dependent analysis of medical images, such as chest X-rays (CXRs). However, existing VQA datasets for CXRs are typically constrained by simplistic and brief answer formats, lacking localization annotations (e.g., bounding boxes) and structured tags (e.g., region or radiological finding/disease tags). To address these limitations, we introduce MIMIC-Ext-CXR-QBA (abbr. CXR-QBA), a large-scale CXR VQA dataset derived from MIMIC-CXR, comprising 42 million QA-pairs with multi-granular, multi-part answers, detailed bounding boxes, and structured tags. \nWe automatically generated our VQA dataset from scene graphs (also made available), which we constructed using LLM-based information extraction from radiology reports. After automatic quality assessment, we identified 31M pre-training and 7.5M fine-tuning grade QA-pairs, providing the largest and most sophisticated VQA dataset for CXRs to date. Tools for using our dataset and the construction pipeline are available at https://anonymous.4open.science/r/mimic-ext-cxr-qba/ .", "abstract": "Visual Question Answering (VQA) enables targeted and context-dependent analysis of medical images, such as chest X-rays (CXRs). However, existing VQA datasets for CXRs are typically constrained by simplistic and brief answer formats, lacking localization annotations (e.g., bounding boxes) and structured tags (e.g., region or radiological finding/disease tags). To address these limitations, we introduce MIMIC-Ext-CXR-QBA (abbr. CXR-QBA), a large-scale CXR VQA dataset derived from MIMIC-CXR, comprising 42 million QA-pairs with multi-granular, multi-part answers, detailed bounding boxes, and structured tags. \nWe automatically generated our VQA dataset from scene graphs (also made available), which we constructed using LLM-based information extraction from radiology reports. After automatic quality assessment, we identified 31M pre-training and 7.5M fine-tuning grade QA-pairs, providing the largest and most sophisticated VQA dataset for CXRs to date. Tools for using our dataset and the construction pipeline are available at https://anonymous.4open.science/r/mimic-ext-cxr-qba/ .", "abstract_source_venue": "OpenReviewAPI", "abstract_source_url": "https://api2.openreview.net/notes?id=LrmyW9JLYq", "openreview_id": "LrmyW9JLYq", "openreview_forum_id": "LrmyW9JLYq", "authors": [], "pdf_url": "https://openreview.net/pdf/bb0c61323a0aaffe74ddc27974abe22176745846.pdf", "summary_cn": "本文提出CXR-QBA数据集，包含4200万QA对，提供完整句子答案、定位框和结构化标签，用于胸部X光视觉问答，是目前最大最复杂的数据集。", "keywords": ["视觉问答", "胸部X光", "数据集", "场景图", "结构化标签", "定位标注"], "triple": {"method": "基于LLM从放射报告构建场景图自动生成", "result": "创建含4200万QA对的大规模数据集", "contribution": "提供首个带完整答案和定位的CXR VQA数据集"}}
{"venue": "ICLR", "search_title": "Adaptive Domain Shift in Diffusion Models for Cross-Modality Image Translation", "full_title": "Adaptive Domain Shift in Diffusion Models for Cross-Modality Image Translation", "url": "https://openreview.net/forum?id=it0GTdiW9t", "year": 2026, "is_main_conference": true, "abstract_snippet": "Cross-modal image translation remains brittle and inefficient. Standard diffusion approaches often rely on a single, global linear transfer between domains. We find that this shortcut forces the sampler to traverse off-manifold, high-cost regions, inflating the correction burden and inviting semantic drift. We refer to this shared failure mode as fixed-schedule domain transfer. In this paper, we embed domain-shift dynamics directly into the generative process. Our model predicts a spatially varying mixing field at every reverse step and injects an explicit, target-consistent restoration term into the drift. This in-step guidance keeps large updates on-manifold and shifts the model’s role from global alignment to local residual correction. We provide a continuous-time formulation with an exact solution form and derive a practical first-order sampler that preserves marginal consistency. Empirically, across translation tasks in medical imaging, remote sensing, and electroluminescence semantic mapping, our framework improves structural fidelity and semantic consistency while converging in fewer denoising steps.", "abstract": "Cross-modal image translation remains brittle and inefficient. Standard diffusion approaches often rely on a single, global linear transfer between domains. We find that this shortcut forces the sampler to traverse off-manifold, high-cost regions, inflating the correction burden and inviting semantic drift. We refer to this shared failure mode as fixed-schedule domain transfer. In this paper, we embed domain-shift dynamics directly into the generative process. Our model predicts a spatially varying mixing field at every reverse step and injects an explicit, target-consistent restoration term into the drift. This in-step guidance keeps large updates on-manifold and shifts the model’s role from global alignment to local residual correction. We provide a continuous-time formulation with an exact solution form and derive a practical first-order sampler that preserves marginal consistency. Empirically, across translation tasks in medical imaging, remote sensing, and electroluminescence semantic mapping, our framework improves structural fidelity and semantic consistency while converging in fewer denoising steps.", "abstract_source_venue": "OpenReviewAPI", "abstract_source_url": "https://api2.openreview.net/notes?id=it0GTdiW9t", "openreview_id": "it0GTdiW9t", "openreview_forum_id": "it0GTdiW9t", "authors": [], "pdf_url": "https://openreview.net/pdf/e3abc2bee3bbd367d978082f4a9ddd9136dd0931.pdf", "summary_cn": "本文提出一种自适应域转移扩散模型，通过空间变化混合场和目标一致恢复项，提升跨模态图像翻译的结构保真度和语义一致性，并减少去噪步骤。", "keywords": ["扩散模型", "跨模态图像翻译", "自适应域转移", "语义一致性", "结构保真度", "去噪效率"], "triple": {"method": "嵌入空间变化混合场与目标一致恢复项", "result": "提升结构保真度和语义一致性，减少去噪步骤", "contribution": "提出自适应域转移扩散模型，优化跨模态翻译效率与质量"}}
{"venue": "ICLR", "search_title": "Pixel-Level Residual Diffusion Transformer: Scalable 3D CT Volume Generation", "full_title": "Pixel-Level Residual Diffusion Transformer: Scalable 3D CT Volume Generation", "url": "https://openreview.net/forum?id=bWtRZQ1rm2", "year": 2026, "is_main_conference": true, "abstract_snippet": "Generating high-resolution 3D CT volumes with fine details remains challenging due to substantial computational demands and optimization difficulties inherent to existing generative models. In this paper, we propose the Pixel-Level Residual Diffusion Transformer (PRDiT), a scalable generative framework that synthesizes high-quality 3D medical volumes directly at voxel-level. PRDiT introduces a two-stage training architecture comprising 1) a local denoiser in the form of an MLP-based blind estimator operating on overlapping 3D patches to separate low-frequency structures efficiently, and 2) a global residual diffusion transformer employing memory-efficient attention to model and refine high-frequency residuals across entire volumes. This coarse-to-fine modeling strategy simplifies optimization, enhances training stability, and effectively preserves subtle structures without the limitations of an autoencoder bottleneck. Extensive experiments conducted on the LIDC-IDRI and RAD-ChestCT datasets demonstrate that PRDiT consistently outperforms state-of-the-art models, such as HA-GAN, 3D LDM and WDM-3D, achieving significantly lower 3D FID, MMD and Wasserstein distance scores.", "abstract": "Generating high-resolution 3D CT volumes with fine details remains challenging due to substantial computational demands and optimization difficulties inherent to existing generative models. In this paper, we propose the Pixel-Level Residual Diffusion Transformer (PRDiT), a scalable generative framework that synthesizes high-quality 3D medical volumes directly at voxel-level. PRDiT introduces a two-stage training architecture comprising 1) a local denoiser in the form of an MLP-based blind estimator operating on overlapping 3D patches to separate low-frequency structures efficiently, and 2) a global residual diffusion transformer employing memory-efficient attention to model and refine high-frequency residuals across entire volumes. This coarse-to-fine modeling strategy simplifies optimization, enhances training stability, and effectively preserves subtle structures without the limitations of an autoencoder bottleneck. Extensive experiments conducted on the LIDC-IDRI and RAD-ChestCT datasets demonstrate that PRDiT consistently outperforms state-of-the-art models, such as HA-GAN, 3D LDM and WDM-3D, achieving significantly lower 3D FID, MMD and Wasserstein distance scores.", "abstract_source_venue": "OpenReviewAPI", "abstract_source_url": "https://api2.openreview.net/notes?id=bWtRZQ1rm2", "openreview_id": "bWtRZQ1rm2", "openreview_forum_id": "bWtRZQ1rm2", "authors": [], "pdf_url": "https://openreview.net/pdf/b3098f5b6d4d3efcbe7a5ee2c979504323107dd2.pdf", "summary_cn": "提出PRDiT模型，通过局部去噪与全局残差扩散Transformer两阶段训练，高效生成高质量3D CT体素图像，在多项指标上超越现有方法。", "keywords": ["3D CT生成", "扩散模型", "Transformer", "残差学习", "两阶段训练", "医学图像合成"], "triple": {"method": "两阶段训练（局部去噪MLP+全局残差扩散Transformer）", "result": "在LIDC-IDRI和RAD-ChestCT数据集上，3D FID等指标优于HA-GAN等模型", "contribution": "提出可扩展的PRDiT框架，优化训练稳定性并保留细节"}}
{"venue": "ICLR", "search_title": "SONIC: Spectral Oriented Neural Invariant Convolutions", "full_title": "SONIC: Spectral Oriented Neural Invariant Convolutions", "url": "https://openreview.net/forum?id=qDGiMrUVmc", "year": 2026, "is_main_conference": true, "abstract_snippet": "Convolutional Neural Networks (CNNs) rely on fixed-size kernels scanning local patches, which limits their ability to capture global context or long-range dependencies without very deep architectures. Vision Transformers (ViTs), in turn, provide global connectivity but lack spatial inductive bias, depend on explicit positional encodings, and remain tied to the initial patch size. Bridging these limitations requires a representation that is both structured and global.  We introduce SONIC (Spectral Oriented Neural Invariant Convolutions), a continuous spectral parameterisation that models convolutional operators using a small set of shared, orientation-selective components. These components define smooth responses across the full frequency domain, yielding global receptive fields and filters that adapt naturally across resolutions. Across synthetic benchmarks, large-scale image classification, and 3D medical datasets, SONIC shows improved robustness to geometric transformations, noise, and resolution shifts, and matches or exceeds convolutional, attention-based, and prior spectral architectures with an order of magnitude fewer parameters. These results demonstrate that continuous, orientation-aware spectral parameterisations provide a principled and scalable alternative to conventional spatial and spectral operators.", "abstract": "Convolutional Neural Networks (CNNs) rely on fixed-size kernels scanning local patches, which limits their ability to capture global context or long-range dependencies without very deep architectures. Vision Transformers (ViTs), in turn, provide global connectivity but lack spatial inductive bias, depend on explicit positional encodings, and remain tied to the initial patch size. Bridging these limitations requires a representation that is both structured and global.  We introduce SONIC (Spectral Oriented Neural Invariant Convolutions), a continuous spectral parameterisation that models convolutional operators using a small set of shared, orientation-selective components. These components define smooth responses across the full frequency domain, yielding global receptive fields and filters that adapt naturally across resolutions. Across synthetic benchmarks, large-scale image classification, and 3D medical datasets, SONIC shows improved robustness to geometric transformations, noise, and resolution shifts, and matches or exceeds convolutional, attention-based, and prior spectral architectures with an order of magnitude fewer parameters. These results demonstrate that continuous, orientation-aware spectral parameterisations provide a principled and scalable alternative to conventional spatial and spectral operators.", "abstract_source_venue": "OpenReviewAPI", "abstract_source_url": "https://api2.openreview.net/notes?id=qDGiMrUVmc", "openreview_id": "qDGiMrUVmc", "openreview_forum_id": "qDGiMrUVmc", "authors": [], "pdf_url": "https://openreview.net/pdf/da267ff334a55d86b0ce605f02d3ce16f60f9da1.pdf", "summary_cn": "SONIC提出连续谱参数化卷积，使用少量方向选择性组件建模全局感受野，在图像分类和医学数据中表现优异，参数更少。", "keywords": ["卷积神经网络", "谱参数化", "全局感受野", "方向选择性", "医学图像", "轻量化模型"], "triple": {"method": "连续谱参数化卷积", "result": "提升几何变换、噪声和分辨率变化的鲁棒性", "contribution": "提供结构化全局表示替代方案"}}
{"venue": "ICLR", "search_title": "AnesSuite: A Comprehensive Benchmark and Dataset Suite for Anesthesiology Reasoning in LLMs", "full_title": "AnesSuite: A Comprehensive Benchmark and Dataset Suite for Anesthesiology Reasoning in LLMs", "url": "https://openreview.net/forum?id=iKRQMeC7yO", "year": 2026, "is_main_conference": true, "abstract_snippet": "The application of large language models (LLMs) in the medical field has garnered significant attention, yet their reasoning capabilities in more specialized domains like anesthesiology remain underexplored. To bridge this gap, we introduce AnesSuite, the first comprehensive dataset suite specifically designed for anesthesiology reasoning in LLMs. The suite features AnesBench, an evaluation benchmark tailored to assess anesthesiology-related reasoning across three levels: factual retrieval (System 1), hybrid reasoning (System 1.x), and complex decision-making (System 2).  Alongside this benchmark, the suite includes three training datasets that provide an infrastructure for continued pre-training (CPT), supervised fine-tuning (SFT), and reinforcement learning with verifiable rewards (RLVR). Leveraging this suite, we develop Morpheus, the first baseline model collection for anesthesiology reasoning. Despite undergoing limited training with SFT and group relative policy optimization (GRPO), Morpheus demonstrates substantial performance improvements, rivaling the performance of larger-scale models. Furthermore, through comprehensive evaluations and experiments, we analyze the key factors influencing anesthesiology reasoning performance, including model characteristics, training strategies and training data. Both AnesSuite and Morpheus will be open-sourced to the public.", "abstract": "The application of large language models (LLMs) in the medical field has garnered significant attention, yet their reasoning capabilities in more specialized domains like anesthesiology remain underexplored. To bridge this gap, we introduce AnesSuite, the first comprehensive dataset suite specifically designed for anesthesiology reasoning in LLMs. The suite features AnesBench, an evaluation benchmark tailored to assess anesthesiology-related reasoning across three levels: factual retrieval (System 1), hybrid reasoning (System 1.x), and complex decision-making (System 2).  Alongside this benchmark, the suite includes three training datasets that provide an infrastructure for continued pre-training (CPT), supervised fine-tuning (SFT), and reinforcement learning with verifiable rewards (RLVR). Leveraging this suite, we develop Morpheus, the first baseline model collection for anesthesiology reasoning. Despite undergoing limited training with SFT and group relative policy optimization (GRPO), Morpheus demonstrates substantial performance improvements, rivaling the performance of larger-scale models. Furthermore, through comprehensive evaluations and experiments, we analyze the key factors influencing anesthesiology reasoning performance, including model characteristics, training strategies and training data. Both AnesSuite and Morpheus will be open-sourced to the public.", "abstract_source_venue": "OpenReviewAPI", "abstract_source_url": "https://api2.openreview.net/notes?id=iKRQMeC7yO", "openreview_id": "iKRQMeC7yO", "openreview_forum_id": "iKRQMeC7yO", "authors": [], "pdf_url": "https://openreview.net/pdf/391df963c4b7337278d7aba3b28827644e0ce3fb.pdf", "summary_cn": "本文提出首个麻醉学推理数据集套件AnesSuite及基准模型Morpheus，通过多级评估与训练数据集提升LLMs在麻醉学领域的推理能力。", "keywords": ["麻醉学推理", "大型语言模型", "评估基准", "训练数据集", "基准模型", "性能分析"], "triple": {"method": "构建AnesSuite数据集套件与Morpheus基准模型", "result": "模型性能显著提升，媲美更大规模模型", "contribution": "填补麻醉学推理评估空白并开源资源"}}
{"venue": "ICLR", "search_title": "OmniVinci: Enhancing Architecture and Data for Omni-Modal Understanding LLM", "full_title": "OmniVinci: Enhancing Architecture and Data for Omni-Modal Understanding LLM", "url": "https://openreview.net/forum?id=DZeic3NpHy", "year": 2026, "is_main_conference": true, "abstract_snippet": "Advancing machine intelligence requires developing the ability to perceive across multiple modalities, much as humans sense the world. We introduce OmniVinci, an initiative to build a strong, open-source, omni-modal LLM. We carefully study the design choices across model architecture and data curation. For model architecture, we present three key innovations: (i) OmniAlignNet for strengthening alignment between vision and audio embeddings in a shared omni-modal latent space; (ii) Temporal Embedding Grouping for capturing relative temporal alignment between vision and audio signals; and (iii) Constrained Rotary Time Embedding for encoding absolute temporal information in omni-modal embeddings.\nWe introduce a curation and synthesis pipeline that generates 24M single-modal and omni-modal conversations. We find that modalities reinforce one another in both perception and reasoning. Our model, OmniVinci, improves over Qwen2.5-Omni with +19.05 on DailyOmni (cross-modal understanding), +1.7 on MMAR (audio), and +3.9 on Video-MME (vision), while using just 0.2T training tokens — a 6× reduction compared to Qwen2.5-Omni’s 1.2T. We finally demonstrate omni-modal advantages in downstream applications spanning robotics, medical AI, and smart factory.", "abstract": "Advancing machine intelligence requires developing the ability to perceive across multiple modalities, much as humans sense the world. We introduce OmniVinci, an initiative to build a strong, open-source, omni-modal LLM. We carefully study the design choices across model architecture and data curation. For model architecture, we present three key innovations: (i) OmniAlignNet for strengthening alignment between vision and audio embeddings in a shared omni-modal latent space; (ii) Temporal Embedding Grouping for capturing relative temporal alignment between vision and audio signals; and (iii) Constrained Rotary Time Embedding for encoding absolute temporal information in omni-modal embeddings.\nWe introduce a curation and synthesis pipeline that generates 24M single-modal and omni-modal conversations. We find that modalities reinforce one another in both perception and reasoning. Our model, OmniVinci, improves over Qwen2.5-Omni with +19.05 on DailyOmni (cross-modal understanding), +1.7 on MMAR (audio), and +3.9 on Video-MME (vision), while using just 0.2T training tokens — a 6× reduction compared to Qwen2.5-Omni’s 1.2T. We finally demonstrate omni-modal advantages in downstream applications spanning robotics, medical AI, and smart factory.", "abstract_source_venue": "OpenReviewAPI", "abstract_source_url": "https://api2.openreview.net/notes?id=DZeic3NpHy", "openreview_id": "DZeic3NpHy", "openreview_forum_id": "DZeic3NpHy", "authors": [], "pdf_url": "https://openreview.net/pdf/05e72ddc6c0d6ceb8bfb486afbf9043228c8be84.pdf", "summary_cn": "OmniVinci 是一个开源全模态大语言模型，通过架构创新（如 OmniAlignNet）和数据增强，显著提升跨模态理解能力，并在多项基准测试中超越 Qwen2.5-Omni。", "keywords": ["全模态理解", "多模态对齐", "架构创新", "数据增强", "开源模型", "跨模态基准"], "triple": {"method": "提出 OmniAlignNet 等架构创新与数据增强", "result": "在 DailyOmni 等基准上显著超越 Qwen2.5-Omni", "contribution": "构建高效开源全模态模型，推动多模态 AI 应用"}}
{"venue": "ICLR", "search_title": "Rubrics as Rewards: Reinforcement Learning Beyond Verifiable Domains", "full_title": "Rubrics as Rewards: Reinforcement Learning Beyond Verifiable Domains", "url": "https://openreview.net/forum?id=c1bTcrDmt4", "year": 2026, "is_main_conference": true, "abstract_snippet": "Reinforcement Learning with Verifiable Rewards (RLVR) has proven effective for complex reasoning tasks with clear correctness signals such as math and coding. However, extending it to real-world reasoning tasks is challenging, as evaluation depends on nuanced, multi-criteria judgments rather than binary correctness. Instance-specific rubrics have recently been used in evaluation benchmarks to capture such judgments, but their potential as reward signals for on-policy post-training remains underexplored. We introduce $\\textbf{Rubrics as Rewards (\\textit{RaR})}$, an on-policy reinforcement learning method that extends RLVR beyond verifiable domains by using rubric-based feedback. Across both medical and science domains, we evaluate multiple strategies for aggregating rubric feedback into rewards. The best RaR variant achieves relative improvements of up to 31\\% on HealthBench and 7\\% on GPQA-Diamond over popular LLM-as-judge baselines that rely on direct Likert-based rewards. These results demonstrate that RaR-trained policies adapt well to diverse evaluation formats, performing strongly on both rubric-based and multiple-choice tasks. Moreover, we find that using rubrics as structured reward signals yields better alignment for smaller judges and reduces performance variance across judge scales.", "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has proven effective for complex reasoning tasks with clear correctness signals such as math and coding. However, extending it to real-world reasoning tasks is challenging, as evaluation depends on nuanced, multi-criteria judgments rather than binary correctness. Instance-specific rubrics have recently been used in evaluation benchmarks to capture such judgments, but their potential as reward signals for on-policy post-training remains underexplored. We introduce $\\textbf{Rubrics as Rewards (\\textit{RaR})}$, an on-policy reinforcement learning method that extends RLVR beyond verifiable domains by using rubric-based feedback. Across both medical and science domains, we evaluate multiple strategies for aggregating rubric feedback into rewards. The best RaR variant achieves relative improvements of up to 31\\% on HealthBench and 7\\% on GPQA-Diamond over popular LLM-as-judge baselines that rely on direct Likert-based rewards. These results demonstrate that RaR-trained policies adapt well to diverse evaluation formats, performing strongly on both rubric-based and multiple-choice tasks. Moreover, we find that using rubrics as structured reward signals yields better alignment for smaller judges and reduces performance variance across judge scales.", "abstract_source_venue": "OpenReviewAPI", "abstract_source_url": "https://api2.openreview.net/notes?id=c1bTcrDmt4", "openreview_id": "c1bTcrDmt4", "openreview_forum_id": "c1bTcrDmt4", "authors": [], "pdf_url": "https://openreview.net/pdf/23a26288538d7d0470f2cc92f491234c3e289de8.pdf", "summary_cn": "提出Rubrics as Rewards方法，利用多标准评分规则作为奖励信号，在医学和科学领域增强强化学习，超越传统可验证奖励，提升模型性能与对齐效果。", "keywords": ["强化学习", "评分规则", "奖励信号", "医学推理", "科学推理", "模型对齐"], "triple": {"method": "使用评分规则聚合反馈作为奖励", "result": "在HealthBench和GPQA-Diamond上性能提升最高达31%和7%", "contribution": "扩展RLVR至非可验证领域，提升模型适应性与对齐"}}
{"venue": "ICLR", "search_title": "Dyslexify: A Mechanistic Defense Against Typographic Attacks in CLIP", "full_title": "Dyslexify: A Mechanistic Defense Against Typographic Attacks in CLIP", "url": "https://openreview.net/forum?id=UI7mbsIZeN", "year": 2026, "is_main_conference": true, "abstract_snippet": "Typographic attacks exploit multi-modal systems by injecting text into images, leading to targeted misclassifications, malicious content generation and even Vision-Language Model jailbreaks.\nIn this work, we analyze how CLIP vision encoders behave under typographic attacks, locating specialized attention heads in the latter half of the model's layers that causally extract and transmit typographic information to the cls token.\nBuilding on these insights, we introduce Dyslexify - a method to defend CLIP models against typographic attacks by selectively ablating a typographic circuit, consisting of attention heads. Without requiring finetuning, dyslexify improves performance by up to 22.06\\% on a typographic variant of ImageNet-100, while reducing standard ImageNet-100 accuracy by less than 1\\%, and demonstrate its utility in a medical foundation model for skin lesion diagnosis. Notably, our training-free approach remains competitive with current state-of-the-art typographic defenses that rely on finetuning. To this end, we release a family of dyslexic CLIP models which are significantly more robust against typographic attacks. These models serve as suitable drop-in replacements for a broad range of safety-critical applications, where the risks of text-based manipulation outweigh the utility of text recognition.", "abstract": "Typographic attacks exploit multi-modal systems by injecting text into images, leading to targeted misclassifications, malicious content generation and even Vision-Language Model jailbreaks.\nIn this work, we analyze how CLIP vision encoders behave under typographic attacks, locating specialized attention heads in the latter half of the model's layers that causally extract and transmit typographic information to the cls token.\nBuilding on these insights, we introduce Dyslexify - a method to defend CLIP models against typographic attacks by selectively ablating a typographic circuit, consisting of attention heads. Without requiring finetuning, dyslexify improves performance by up to 22.06\\% on a typographic variant of ImageNet-100, while reducing standard ImageNet-100 accuracy by less than 1\\%, and demonstrate its utility in a medical foundation model for skin lesion diagnosis. Notably, our training-free approach remains competitive with current state-of-the-art typographic defenses that rely on finetuning. To this end, we release a family of dyslexic CLIP models which are significantly more robust against typographic attacks. These models serve as suitable drop-in replacements for a broad range of safety-critical applications, where the risks of text-based manipulation outweigh the utility of text recognition.", "abstract_source_venue": "OpenReviewAPI", "abstract_source_url": "https://api2.openreview.net/notes?id=UI7mbsIZeN", "openreview_id": "UI7mbsIZeN", "openreview_forum_id": "UI7mbsIZeN", "authors": [], "pdf_url": "https://openreview.net/pdf/f311ce65d6af434c5a3feb4ff0485d201261e9e0.pdf", "summary_cn": "本文提出Dyslexify方法，通过选择性消融CLIP模型中的注意力头电路，无需微调即可有效防御排版攻击，提升模型鲁棒性，适用于医疗等安全关键领域。", "keywords": ["排版攻击", "CLIP模型", "注意力头", "防御方法", "鲁棒性", "医疗诊断"], "triple": {"method": "选择性消融注意力头电路", "result": "提升排版攻击防御性能达22.06%，标准准确率下降小于1%", "contribution": "提出无需微调的防御方法，增强模型安全性与适用性"}}
{"venue": "ICLR", "search_title": "DM4CT: Benchmarking Diffusion Models for Computed Tomography Reconstruction", "full_title": "DM4CT: Benchmarking Diffusion Models for Computed Tomography Reconstruction", "url": "https://openreview.net/forum?id=YE5scJekg5", "year": 2026, "is_main_conference": true, "abstract_snippet": "Diffusion models have recently emerged as powerful priors for solving inverse problems. While computed tomography (CT) is theoretically a linear inverse problem, it poses many practical challenges. These include correlated noise, artifact structures, reliance on system geometry, and misaligned value ranges, which make the direct application of diffusion models more difficult than in domains like natural image generation. To systematically evaluate how diffusion models perform in this context and compare them with established reconstruction methods, we introduce DM4CT, a comprehensive benchmark for CT reconstruction. DM4CT includes datasets from both medical and industrial domains with sparse-view and noisy configurations. To explore the challenges of deploying diffusion models in practice, we additionally acquire a high-resolution CT dataset at a high-energy synchrotron facility and evaluate all methods under real experimental conditions. We benchmark nine recent diffusion-based methods alongside seven strong baselines, including model-based, unsupervised, and supervised approaches. Our analysis provides detailed insights into the behavior, strengths, and limitations of diffusion models for CT reconstruction. The real-world dataset is publicly available at zenodo.org/records/15420527, and the codebase is open-sourced at github.com/DM4CT/DM4CT.", "abstract": "Diffusion models have recently emerged as powerful priors for solving inverse problems. While computed tomography (CT) is theoretically a linear inverse problem, it poses many practical challenges. These include correlated noise, artifact structures, reliance on system geometry, and misaligned value ranges, which make the direct application of diffusion models more difficult than in domains like natural image generation. To systematically evaluate how diffusion models perform in this context and compare them with established reconstruction methods, we introduce DM4CT, a comprehensive benchmark for CT reconstruction. DM4CT includes datasets from both medical and industrial domains with sparse-view and noisy configurations. To explore the challenges of deploying diffusion models in practice, we additionally acquire a high-resolution CT dataset at a high-energy synchrotron facility and evaluate all methods under real experimental conditions. We benchmark nine recent diffusion-based methods alongside seven strong baselines, including model-based, unsupervised, and supervised approaches. Our analysis provides detailed insights into the behavior, strengths, and limitations of diffusion models for CT reconstruction. The real-world dataset is publicly available at zenodo.org/records/15420527, and the codebase is open-sourced at github.com/DM4CT/DM4CT.", "abstract_source_venue": "OpenReviewAPI", "abstract_source_url": "https://api2.openreview.net/notes?id=YE5scJekg5", "openreview_id": "YE5scJekg5", "openreview_forum_id": "YE5scJekg5", "authors": [], "pdf_url": "https://openreview.net/pdf/6c6275f2f22d44d9ad3dc21ea1c8d7155eaed09b.pdf", "summary_cn": "DM4CT是首个评估扩散模型在CT重建中性能的基准，涵盖医学与工业数据集，并在真实实验条件下对比多种方法，揭示了扩散模型的优势与局限。", "keywords": ["扩散模型", "CT重建", "基准测试", "逆问题", "稀疏视图", "噪声配置"], "triple": {"method": "构建DM4CT基准并测试九种扩散模型与七种基线方法", "result": "系统评估了扩散模型在CT重建中的表现与挑战", "contribution": "提供首个公开的CT重建扩散模型基准与代码库"}}
{"venue": "ICLR", "search_title": "Bridging Explainability and Embeddings: BEE Aware of Spuriousness", "full_title": "Bridging Explainability and Embeddings: BEE Aware of Spuriousness", "url": "https://openreview.net/forum?id=9jYpHmI8ot", "year": 2026, "is_main_conference": true, "abstract_snippet": "Current methods for detecting spurious correlations rely on data splits or error patterns, leaving many harmful shortcuts invisible when counterexamples are absent. We introduce BEE (Bridging Explainability and Embeddings), a framework that shifts the focus from model predictions to the weight space and embedding geometry underlying decisions. By analyzing how fine-tuning perturbs pretrained representations, BEE uncovers spurious correlations that remain hidden from conventional evaluation pipelines. We use linear probing as a transparent diagnostic lens, revealing spurious features that not only persist after full fine-tuning but also transfer across diverse state-of-the-art models. Our experiments cover numerous datasets and domains: vision (Waterbirds, CelebA, ImageNet-1k), language (CivilComments, MIMIC-CXR medical notes), and multiple embedding families (CLIP, CLIP-DataComp.XL, mGTE, BLIP2, SigLIP2). \nBEE consistently exposes spurious correlations: from concepts that slash the ImageNet accuracy by up to 95\\%, to clinical shortcuts in MIMIC-CXR notes that induce dangerous false negatives. Together, these results position BEE as a general and principled tool for diagnosing spurious correlations in weight space, enabling principled dataset auditing and more trustworthy foundation models. Our code is publicly available.", "abstract": "Current methods for detecting spurious correlations rely on data splits or error patterns, leaving many harmful shortcuts invisible when counterexamples are absent. We introduce BEE (Bridging Explainability and Embeddings), a framework that shifts the focus from model predictions to the weight space and embedding geometry underlying decisions. By analyzing how fine-tuning perturbs pretrained representations, BEE uncovers spurious correlations that remain hidden from conventional evaluation pipelines. We use linear probing as a transparent diagnostic lens, revealing spurious features that not only persist after full fine-tuning but also transfer across diverse state-of-the-art models. Our experiments cover numerous datasets and domains: vision (Waterbirds, CelebA, ImageNet-1k), language (CivilComments, MIMIC-CXR medical notes), and multiple embedding families (CLIP, CLIP-DataComp.XL, mGTE, BLIP2, SigLIP2). \nBEE consistently exposes spurious correlations: from concepts that slash the ImageNet accuracy by up to 95\\%, to clinical shortcuts in MIMIC-CXR notes that induce dangerous false negatives. Together, these results position BEE as a general and principled tool for diagnosing spurious correlations in weight space, enabling principled dataset auditing and more trustworthy foundation models. Our code is publicly available.", "abstract_source_venue": "OpenReviewAPI", "abstract_source_url": "https://api2.openreview.net/notes?id=9jYpHmI8ot", "openreview_id": "9jYpHmI8ot", "openreview_forum_id": "9jYpHmI8ot", "authors": [], "pdf_url": "https://openreview.net/pdf/03a5029a5aab6d2b065eee5ea8c09754d1133501.pdf", "summary_cn": "BEE框架通过分析微调对预训练表示的影响，从权重空间和嵌入几何角度揭示传统方法难以检测的虚假相关性，提升模型可信度。", "keywords": ["虚假相关性检测", "权重空间分析", "嵌入几何", "线性探测", "多领域验证", "模型诊断"], "triple": {"method": "分析微调扰动预训练表示", "result": "暴露跨模型和领域的虚假相关性", "contribution": "提供通用诊断工具增强模型可信度"}}
{"venue": "ICLR", "search_title": "Graph Mixing Additive Networks", "full_title": "Graph Mixing Additive Networks", "url": "https://openreview.net/forum?id=1MVeSLvfxU", "year": 2026, "is_main_conference": true, "abstract_snippet": "Real-world temporal data often consists of multiple signal types recorded at irregular, asynchronous intervals. For instance, in the medical domain, different types of blood tests can be measured at different times and frequencies, resulting in fragmented and unevenly scattered temporal data. Similar issues of irregular sampling occur in other domains, such as the monitoring of large systems using event log files. Effectively learning from such data requires handling sets of temporally sparse and heterogeneous signals. In this work, we propose Graph Mixing Additive Networks (GMAN), a novel and interpretable-by-design framework for learning directly from sets of graphs that represent such signals.\nGMAN provides diverse interpretability capabilities, including node-level, graph-level, and subset-level importance, and enables practitioners to trade finer-grained interpretability for greater expressivity when domain priors are available.\nGMAN achieves state-of-the-art performance in real-world high-stakes tasks, including predicting Crohn’s disease onset and hospital length of stay from routine blood test measurements and detecting fake news. Furthermore, we demonstrate how GMAN’s interpretability properties assist in revealing disease development phase transitions and provide crucial insights in the healthcare domain.", "abstract": "Real-world temporal data often consists of multiple signal types recorded at irregular, asynchronous intervals. For instance, in the medical domain, different types of blood tests can be measured at different times and frequencies, resulting in fragmented and unevenly scattered temporal data. Similar issues of irregular sampling occur in other domains, such as the monitoring of large systems using event log files. Effectively learning from such data requires handling sets of temporally sparse and heterogeneous signals. In this work, we propose Graph Mixing Additive Networks (GMAN), a novel and interpretable-by-design framework for learning directly from sets of graphs that represent such signals.\nGMAN provides diverse interpretability capabilities, including node-level, graph-level, and subset-level importance, and enables practitioners to trade finer-grained interpretability for greater expressivity when domain priors are available.\nGMAN achieves state-of-the-art performance in real-world high-stakes tasks, including predicting Crohn’s disease onset and hospital length of stay from routine blood test measurements and detecting fake news. Furthermore, we demonstrate how GMAN’s interpretability properties assist in revealing disease development phase transitions and provide crucial insights in the healthcare domain.", "abstract_source_venue": "OpenReviewAPI", "abstract_source_url": "https://api2.openreview.net/notes?id=1MVeSLvfxU", "openreview_id": "1MVeSLvfxU", "openreview_forum_id": "1MVeSLvfxU", "authors": [], "pdf_url": "https://openreview.net/pdf/7483164e7820c8dc23497c33c292b4571c032a13.pdf", "summary_cn": "提出GMAN框架，处理不规则异步时序数据，实现高精度预测与多级可解释性，应用于克罗恩病预测等任务。", "keywords": ["图神经网络", "可解释性", "时序数据", "医疗预测", "异步采样", "GMAN"], "triple": {"method": "图混合加性网络", "result": "在疾病预测等任务中达到最优性能", "contribution": "提供多级可解释性框架"}}
{"venue": "ICLR", "search_title": "Reconstruct Anything Model a lightweight foundation model for computational imaging", "full_title": "Reconstruct Anything Model a lightweight foundation model for computational imaging", "url": "https://openreview.net/forum?id=Ks9zNS6OsU", "year": 2026, "is_main_conference": true, "abstract_snippet": "Most existing learning-based methods for solving imaging inverse problems can be roughly divided into two classes: iterative algorithms, such as plug-and-play and diffusion methods leveraging pretrained denoisers, and unrolled architectures that are trained end-to-end for specific imaging problems. Iterative methods in the first class are computationally costly and often yield suboptimal reconstruction performance, whereas unrolled architectures are generally problem-specific and require expensive training. In this work, we propose a novel non-iterative, lightweight architecture that incorporates knowledge about the forward operator (acquisition physics and noise parameters) without relying on unrolling. Our model is trained to solve a wide range of inverse problems, such as deblurring, magnetic resonance imaging, computed tomography, inpainting, and super-resolution, and works on arbitrary image sizes and channels, such as grayscale, complex, and color data. The proposed model can be easily adapted to unseen inverse problems or datasets with a few fine-tuning steps (up to a few images) in a self-supervised way, without ground-truth references. Throughout a series of experiments, we demonstrate state-of-the-art performance from medical imaging to low-photon imaging and microscopy.", "abstract": "Most existing learning-based methods for solving imaging inverse problems can be roughly divided into two classes: iterative algorithms, such as plug-and-play and diffusion methods leveraging pretrained denoisers, and unrolled architectures that are trained end-to-end for specific imaging problems. Iterative methods in the first class are computationally costly and often yield suboptimal reconstruction performance, whereas unrolled architectures are generally problem-specific and require expensive training. In this work, we propose a novel non-iterative, lightweight architecture that incorporates knowledge about the forward operator (acquisition physics and noise parameters) without relying on unrolling. Our model is trained to solve a wide range of inverse problems, such as deblurring, magnetic resonance imaging, computed tomography, inpainting, and super-resolution, and works on arbitrary image sizes and channels, such as grayscale, complex, and color data. The proposed model can be easily adapted to unseen inverse problems or datasets with a few fine-tuning steps (up to a few images) in a self-supervised way, without ground-truth references. Throughout a series of experiments, we demonstrate state-of-the-art performance from medical imaging to low-photon imaging and microscopy.", "abstract_source_venue": "OpenReviewAPI", "abstract_source_url": "https://api2.openreview.net/notes?id=Ks9zNS6OsU", "openreview_id": "Ks9zNS6OsU", "openreview_forum_id": "Ks9zNS6OsU", "authors": [], "pdf_url": "https://openreview.net/pdf/b598ec18508dee145086208097886ddc8bc7c1cf.pdf", "summary_cn": "提出一种轻量级非迭代模型，用于解决多种成像逆问题，无需展开训练，可快速适应新任务，在医学成像等领域达到先进性能。", "keywords": ["计算成像", "逆问题", "轻量级模型", "非迭代方法", "自监督适应", "多任务学习"], "triple": {"method": "非迭代轻量架构结合前向算子知识", "result": "在去模糊、MRI等多任务中实现先进性能", "contribution": "提供通用、高效且易适应的成像重建方案"}}
{"venue": "ICLR", "search_title": "A Framework for Studying AI Agent Behavior: Evidence from Consumer Choice Experiments", "full_title": "A Framework for Studying AI Agent Behavior: Evidence from Consumer Choice Experiments", "url": "https://openreview.net/forum?id=xAPoscV2Bw", "year": 2026, "is_main_conference": true, "abstract_snippet": "Environments built for people are increasingly operated by a new class of economic actors: LLM-powered software agents making decisions on our behalf. These decisions range from our purchases to travel plans to medical treatment selection. Current evaluations of these agents largely focus on task competence, but we argue for a deeper assessment: how these agents choose when faced with realistic decisions. We introduce ABxLab, a framework for systematically probing agentic choice through controlled manipulations of option attributes and persuasive cues. We apply this to a realistic web-based shopping environment, where we vary prices, ratings, and psychological nudges, all of which are factors long known to shape human choice. We find that agent decisions shift predictably and substantially in response, revealing that agents are strongly biased choosers even without being subject to the cognitive constraints that shape human biases. This susceptibility reveals both risk and opportunity: risk, because agentic consumers may inherit and amplify human biases; opportunity, because consumer choice provides a powerful testbed for a behavioral science of AI agents, just as it has for the study of human behavior. We release our framework as an open benchmark for rigorous, scalable evaluation of agent decision-making.", "abstract": "Environments built for people are increasingly operated by a new class of economic actors: LLM-powered software agents making decisions on our behalf. These decisions range from our purchases to travel plans to medical treatment selection. Current evaluations of these agents largely focus on task competence, but we argue for a deeper assessment: how these agents choose when faced with realistic decisions. We introduce ABxLab, a framework for systematically probing agentic choice through controlled manipulations of option attributes and persuasive cues. We apply this to a realistic web-based shopping environment, where we vary prices, ratings, and psychological nudges, all of which are factors long known to shape human choice. We find that agent decisions shift predictably and substantially in response, revealing that agents are strongly biased choosers even without being subject to the cognitive constraints that shape human biases. This susceptibility reveals both risk and opportunity: risk, because agentic consumers may inherit and amplify human biases; opportunity, because consumer choice provides a powerful testbed for a behavioral science of AI agents, just as it has for the study of human behavior. We release our framework as an open benchmark for rigorous, scalable evaluation of agent decision-making.", "abstract_source_venue": "OpenReviewAPI", "abstract_source_url": "https://api2.openreview.net/notes?id=xAPoscV2Bw", "openreview_id": "xAPoscV2Bw", "openreview_forum_id": "xAPoscV2Bw", "authors": [], "pdf_url": "https://openreview.net/pdf/2f9583f480b2003218311f67eb597e890b3ed65a.pdf", "summary_cn": "提出ABxLab框架，通过操控选项属性和说服线索，在模拟购物环境中评估AI代理决策行为，发现其易受偏见影响，为AI行为科学提供测试基准。", "keywords": ["AI代理", "消费者选择", "行为偏见", "决策评估", "ABxLab框架", "心理暗示"], "triple": {"method": "ABxLab框架操控价格、评分和暗示", "result": "代理决策显著受偏见影响", "contribution": "为AI行为科学提供可扩展评估基准"}}
{"venue": "ICLR", "search_title": "CARL: Camera-Agnostic Representation Learning for Spectral Image Analysis", "full_title": "CARL: Camera-Agnostic Representation Learning for Spectral Image Analysis", "url": "https://openreview.net/forum?id=TpbhS1yfz0", "year": 2026, "is_main_conference": true, "abstract_snippet": "Spectral imaging offers promising applications across diverse domains, including medicine and urban scene understanding, and is already established as a critical modality in remote sensing.\nHowever, variability in channel dimensionality and captured wavelengths among spectral cameras impede the development of AI-driven methodologies, leading to camera-specific models with limited generalizability and inadequate cross-camera applicability.\nTo address this bottleneck, we introduce CARL, a model for Camera-Agnostic Representation Learning across RGB, multispectral, and hyperspectral imaging modalities.\nTo enable the conversion of a spectral image with any channel dimensionality to a camera-agnostic representation, we introduce a novel spectral encoder, featuring a self-attention-cross-attention mechanism, to distill salient spectral information into learned spectral representations.\nSpatio-spectral pre-training is achieved with a novel feature-based self-supervision strategy tailored to CARL. \nLarge-scale experiments across the domains of medical imaging, autonomous driving, and satellite imaging demonstrate our model's unique robustness to spectral heterogeneity, outperforming on datasets with simulated and real-world cross-camera spectral variations. \nThe scalability and versatility of the proposed approach position our model as a backbone for future spectral foundation models.", "abstract": "Spectral imaging offers promising applications across diverse domains, including medicine and urban scene understanding, and is already established as a critical modality in remote sensing.\nHowever, variability in channel dimensionality and captured wavelengths among spectral cameras impede the development of AI-driven methodologies, leading to camera-specific models with limited generalizability and inadequate cross-camera applicability.\nTo address this bottleneck, we introduce CARL, a model for Camera-Agnostic Representation Learning across RGB, multispectral, and hyperspectral imaging modalities.\nTo enable the conversion of a spectral image with any channel dimensionality to a camera-agnostic representation, we introduce a novel spectral encoder, featuring a self-attention-cross-attention mechanism, to distill salient spectral information into learned spectral representations.\nSpatio-spectral pre-training is achieved with a novel feature-based self-supervision strategy tailored to CARL. \nLarge-scale experiments across the domains of medical imaging, autonomous driving, and satellite imaging demonstrate our model's unique robustness to spectral heterogeneity, outperforming on datasets with simulated and real-world cross-camera spectral variations. \nThe scalability and versatility of the proposed approach position our model as a backbone for future spectral foundation models.", "abstract_source_venue": "OpenReviewAPI", "abstract_source_url": "https://api2.openreview.net/notes?id=TpbhS1yfz0", "openreview_id": "TpbhS1yfz0", "openreview_forum_id": "TpbhS1yfz0", "authors": [], "pdf_url": "https://openreview.net/pdf/0ee89a85fe26f6258b772102894b094c572577df.pdf", "summary_cn": "CARL提出相机无关表示学习方法，通过自注意力-交叉注意力机制编码光谱图像，实现跨RGB、多光谱和高光谱模态的通用表示，提升模型在医学、自动驾驶和卫星成像中的泛化能力。", "keywords": ["光谱成像", "相机无关表示", "自注意力机制", "跨模态学习", "泛化能力", "预训练策略"], "triple": {"method": "自注意力-交叉注意力编码器与特征自监督预训练", "result": "在模拟和真实跨相机光谱变化数据上表现优异", "contribution": "为光谱基础模型提供可扩展骨干网络"}}
{"venue": "ICLR", "search_title": "Knowledgeable Language Models as Black-Box Optimizers for Personalized Medicine", "full_title": "Knowledgeable Language Models as Black-Box Optimizers for Personalized Medicine", "url": "https://openreview.net/forum?id=w025bYRVkO", "year": 2026, "is_main_conference": true, "abstract_snippet": "The goal of personalized medicine is to discover a treatment regimen that optimizes a patient's clinical outcome based on their personal genetic and environmental factors. However, candidate treatments cannot be arbitrarily administered to the patient to assess their efficacy; we often instead have access to an *in silico* surrogate model that approximates the true fitness of a proposed treatment. Unfortunately, such surrogate models have been shown to fail to generalize to previously unseen patient-treatment combinations. We hypothesize that domain-specific prior knowledge—such as medical textbooks and biomedical knowledge graphs—can provide a meaningful alternative signal of the fitness of proposed treatments. To this end, we introduce **L**LM-based **E**ntropy-guided **O**ptimization with k**N**owledgeable priors (**LEON**), a mathematically principled approach to leverage large language models (LLMs) as black-box optimizers without any task-specific fine-tuning, taking advantage of their ability to contextualize unstructured domain knowledge to propose personalized treatment plans in natural language. In practice, we implement LEON via 'optimization by prompting,' which uses LLMs as stochastic engines for proposing treatment designs. Experiments on real-world optimization tasks show LEON outperforms both traditional and LLM-based methods in proposing individualized treatments for patients.", "abstract": "The goal of personalized medicine is to discover a treatment regimen that optimizes a patient's clinical outcome based on their personal genetic and environmental factors. However, candidate treatments cannot be arbitrarily administered to the patient to assess their efficacy; we often instead have access to an *in silico* surrogate model that approximates the true fitness of a proposed treatment. Unfortunately, such surrogate models have been shown to fail to generalize to previously unseen patient-treatment combinations. We hypothesize that domain-specific prior knowledge—such as medical textbooks and biomedical knowledge graphs—can provide a meaningful alternative signal of the fitness of proposed treatments. To this end, we introduce **L**LM-based **E**ntropy-guided **O**ptimization with k**N**owledgeable priors (**LEON**), a mathematically principled approach to leverage large language models (LLMs) as black-box optimizers without any task-specific fine-tuning, taking advantage of their ability to contextualize unstructured domain knowledge to propose personalized treatment plans in natural language. In practice, we implement LEON via 'optimization by prompting,' which uses LLMs as stochastic engines for proposing treatment designs. Experiments on real-world optimization tasks show LEON outperforms both traditional and LLM-based methods in proposing individualized treatments for patients.", "abstract_source_venue": "OpenReviewAPI", "abstract_source_url": "https://api2.openreview.net/notes?id=w025bYRVkO", "openreview_id": "w025bYRVkO", "openreview_forum_id": "w025bYRVkO", "authors": [], "pdf_url": "https://openreview.net/pdf/277ab901ee761247b3ff6acd2281d287a593724f.pdf", "summary_cn": "本文提出LEON方法，利用大型语言模型作为黑盒优化器，结合医学知识，无需微调即可生成个性化治疗方案，实验显示其优于传统方法。", "keywords": ["个性化医疗", "大型语言模型", "黑盒优化", "知识先验", "治疗计划", "自然语言处理"], "triple": {"method": "基于提示的优化，利用LLM作为随机引擎", "result": "在真实优化任务中优于传统和LLM方法", "contribution": "提出LEON框架，结合知识先验提升个性化治疗设计"}}
{"venue": "ICLR", "search_title": "Anchored Supervised Fine-Tuning", "full_title": "Anchored Supervised Fine-Tuning", "url": "https://openreview.net/forum?id=PORko7QT64", "year": 2026, "is_main_conference": true, "abstract_snippet": "Post-training of large language models involves a fundamental trade-off between\nsupervised fine-tuning (SFT), which efficiently mimics demonstrations but tends\nto memorize, and reinforcement learning (RL), which achieves better generaliza-\ntion at higher computational cost. Dynamic Fine-Tuning (DFT) recently emerged\nas a promising middle ground, reweighting SFT objectives with token probabili-\nties and achieving improvements in certain reasoning domains, though it exhibits\ninstability in other tasks. We provide a analysis of DFT through the reward-\nweighted regression (RWR) framework, revealing that it corresponds to a spe-\ncific auxiliary distribution choice that yields provably tighter RL bounds than\nstandard SFT. However, our analysis also uncovers a critical limitation: this con-\nstruction lacks distributional anchoring, leading to progressive drift that under-\nmines training stability. To address this, we propose Anchored Supervised Fine-\nTuning (ASFT), which augments DFT’s reweighting with lightweight KL regu-\nlarization to preserve tightness while ensuring stability. Empirically, ASFT con-\nsistently outperforms both SFT and DFT across mathematical reasoning, medical\nknowledge grounding, and code generation, achieving substantial improvements\nwith minimal computational overhead. Our RWR framework provides a system-\natic lens for understanding post-training methods and demonstrates that principled\ntheoretical analysis leads to both stronger guarantees and practical gains.", "abstract": "Post-training of large language models involves a fundamental trade-off between\nsupervised fine-tuning (SFT), which efficiently mimics demonstrations but tends\nto memorize, and reinforcement learning (RL), which achieves better generaliza-\ntion at higher computational cost. Dynamic Fine-Tuning (DFT) recently emerged\nas a promising middle ground, reweighting SFT objectives with token probabili-\nties and achieving improvements in certain reasoning domains, though it exhibits\ninstability in other tasks. We provide a analysis of DFT through the reward-\nweighted regression (RWR) framework, revealing that it corresponds to a spe-\ncific auxiliary distribution choice that yields provably tighter RL bounds than\nstandard SFT. However, our analysis also uncovers a critical limitation: this con-\nstruction lacks distributional anchoring, leading to progressive drift that under-\nmines training stability. To address this, we propose Anchored Supervised Fine-\nTuning (ASFT), which augments DFT’s reweighting with lightweight KL regu-\nlarization to preserve tightness while ensuring stability. Empirically, ASFT con-\nsistently outperforms both SFT and DFT across mathematical reasoning, medical\nknowledge grounding, and code generation, achieving substantial improvements\nwith minimal computational overhead. Our RWR framework provides a system-\natic lens for understanding post-training methods and demonstrates that principled\ntheoretical analysis leads to both stronger guarantees and practical gains.", "abstract_source_venue": "OpenReviewAPI", "abstract_source_url": "https://api2.openreview.net/notes?id=PORko7QT64", "openreview_id": "PORko7QT64", "openreview_forum_id": "PORko7QT64", "authors": [], "pdf_url": "https://openreview.net/pdf/204901b4e4fccd37b35834fefde1883f634b2682.pdf", "summary_cn": "提出锚定监督微调（ASFT），通过KL正则化增强动态微调，解决训练不稳定问题，在数学推理、医学知识和代码生成任务中优于SFT和DFT。", "keywords": ["锚定监督微调", "动态微调", "KL正则化", "奖励加权回归", "训练稳定性", "后训练方法"], "triple": {"method": "ASFT（带KL正则化的动态微调）", "result": "在多项任务中优于SFT和DFT", "contribution": "提升训练稳定性与性能"}}
{"venue": "ICLR", "search_title": "Detecting Invariant Manifolds in ReLU-Based RNNs", "full_title": "Detecting Invariant Manifolds in ReLU-Based RNNs", "url": "https://openreview.net/forum?id=EAwLAwHvhk", "year": 2026, "is_main_conference": true, "abstract_snippet": "Recurrent Neural Networks (RNNs) have found widespread applications in machine learning for time series prediction and dynamical systems reconstruction, and experienced a recent renaissance with improved training algorithms and architectural designs. Understanding why and how trained RNNs produce their behavior is important for scientific and medical applications, and explainable AI more generally. An RNN's dynamical repertoire depends on the topological and geometrical properties of its state space. Stable and unstable manifolds of periodic points play a particularly important role: They dissect a dynamical system's state space into different basins of attraction, and their intersections lead to chaotic dynamics with fractal geometry. Here we introduce a novel algorithm for detecting these manifolds, with a focus on piecewise-linear RNNs (PLRNNs) employing rectified linear units (ReLUs) as their activation function. We demonstrate how the algorithm can be used to trace the boundaries between different basins of attraction, and hence to characterize multistability, a computationally important property. We further show its utility in finding so-called homoclinic points, the intersections between stable and unstable manifolds, and thus establish the existence of chaos in PLRNNs. Finally we show for an empirical example, electrophysiological recordings from a cortical neuron, how insights into the underlying dynamics could be gained through our method.", "abstract": "Recurrent Neural Networks (RNNs) have found widespread applications in machine learning for time series prediction and dynamical systems reconstruction, and experienced a recent renaissance with improved training algorithms and architectural designs. Understanding why and how trained RNNs produce their behavior is important for scientific and medical applications, and explainable AI more generally. An RNN's dynamical repertoire depends on the topological and geometrical properties of its state space. Stable and unstable manifolds of periodic points play a particularly important role: They dissect a dynamical system's state space into different basins of attraction, and their intersections lead to chaotic dynamics with fractal geometry. Here we introduce a novel algorithm for detecting these manifolds, with a focus on piecewise-linear RNNs (PLRNNs) employing rectified linear units (ReLUs) as their activation function. We demonstrate how the algorithm can be used to trace the boundaries between different basins of attraction, and hence to characterize multistability, a computationally important property. We further show its utility in finding so-called homoclinic points, the intersections between stable and unstable manifolds, and thus establish the existence of chaos in PLRNNs. Finally we show for an empirical example, electrophysiological recordings from a cortical neuron, how insights into the underlying dynamics could be gained through our method.", "abstract_source_venue": "OpenReviewAPI", "abstract_source_url": "https://api2.openreview.net/notes?id=EAwLAwHvhk", "openreview_id": "EAwLAwHvhk", "openreview_forum_id": "EAwLAwHvhk", "authors": [], "pdf_url": "https://openreview.net/pdf/dd9f88fecc0cf41fba41d05d28d22219b516df24.pdf", "summary_cn": "提出新算法检测ReLU循环神经网络中的不变流形，用于刻画多稳态和混沌，并应用于神经元电生理数据分析。", "keywords": ["循环神经网络", "不变流形", "多稳态", "混沌", "ReLU", "动力学分析"], "triple": {"method": "新算法检测不变流形", "result": "刻画多稳态与混沌", "contribution": "提供RNN动力学解释工具"}}
{"venue": "ICLR", "search_title": "GradPruner: Gradient-guided Layer Pruning Enabling Efficient Fine-Tuning and Inference for LLMs", "full_title": "GradPruner: Gradient-guided Layer Pruning Enabling Efficient Fine-Tuning and Inference for LLMs", "url": "https://openreview.net/forum?id=bxzJorqyYM", "year": 2026, "is_main_conference": true, "abstract_snippet": "Fine-tuning Large Language Models (LLMs) with downstream data is often considered time-consuming and expensive. Structured pruning methods are primarily employed to improve the inference efficiency of pre-trained models. Meanwhile, they often require additional time and memory for training, knowledge distillation, structure search, and other strategies, making efficient model fine-tuning challenging to achieve. To simultaneously enhance the training and inference efficiency of downstream task fine-tuning, we introduce GradPruner, which can prune layers of LLMs guided by gradients in the early stages of fine-tuning. GradPruner uses the cumulative gradients of each parameter during the initial phase of fine-tuning to compute the Initial Gradient Information Accumulation Matrix (IGIA-Matrix) to assess the importance of layers and perform pruning. We sparsify the pruned layers based on the IGIA-Matrix and merge them with the remaining layers. Only elements with the same sign are merged to reduce interference from sign variations. We conducted extensive experiments on two LLMs across eight well-known datasets in downstream tasks. Including medical, financial, and general benchmark tasks. The results demonstrate that GradPruner has achieved a parameter reduction of 40% with only a 0.99% decrease in accuracy. Our code is available at https://anonymous.4open.science/r/LLM-GradPrune-436D.", "abstract": "Fine-tuning Large Language Models (LLMs) with downstream data is often considered time-consuming and expensive. Structured pruning methods are primarily employed to improve the inference efficiency of pre-trained models. Meanwhile, they often require additional time and memory for training, knowledge distillation, structure search, and other strategies, making efficient model fine-tuning challenging to achieve. To simultaneously enhance the training and inference efficiency of downstream task fine-tuning, we introduce GradPruner, which can prune layers of LLMs guided by gradients in the early stages of fine-tuning. GradPruner uses the cumulative gradients of each parameter during the initial phase of fine-tuning to compute the Initial Gradient Information Accumulation Matrix (IGIA-Matrix) to assess the importance of layers and perform pruning. We sparsify the pruned layers based on the IGIA-Matrix and merge them with the remaining layers. Only elements with the same sign are merged to reduce interference from sign variations. We conducted extensive experiments on two LLMs across eight well-known datasets in downstream tasks. Including medical, financial, and general benchmark tasks. The results demonstrate that GradPruner has achieved a parameter reduction of 40% with only a 0.99% decrease in accuracy. Our code is available at https://anonymous.4open.science/r/LLM-GradPrune-436D.", "abstract_source_venue": "OpenReviewAPI", "abstract_source_url": "https://api2.openreview.net/notes?id=bxzJorqyYM", "openreview_id": "bxzJorqyYM", "openreview_forum_id": "bxzJorqyYM", "authors": [], "pdf_url": "https://openreview.net/pdf/4e3497f23d35e926fccfad5cc046a52900589fd2.pdf", "summary_cn": "GradPruner通过梯度引导层剪枝，在微调早期评估层重要性，合并同符号元素，提升LLMs训练与推理效率，参数减少40%仅精度下降0.99%。", "keywords": ["梯度引导剪枝", "层重要性评估", "高效微调", "推理加速", "参数减少", "LLMs优化"], "triple": {"method": "基于初始梯度信息累积矩阵评估层重要性并剪枝合并", "result": "参数减少40%，精度仅下降0.99%", "contribution": "同时提升LLMs下游任务训练与推理效率"}}
{"venue": "ICLR", "search_title": "CUPID: A Plug-in Framework for Joint Aleatoric and Epistemic Uncertainty Estimation with a Single Model", "full_title": "CUPID: A Plug-in Framework for Joint Aleatoric and Epistemic Uncertainty Estimation with a Single Model", "url": "https://openreview.net/forum?id=nF81AkEzXg", "year": 2026, "is_main_conference": true, "abstract_snippet": "Accurate estimation of uncertainty in deep learning is critical for deploying models in high-stakes domains such as medical diagnosis and autonomous decision-making, where overconfident predictions can lead to harmful outcomes. In practice, understanding the reason behind a model’s uncertainty and the type of uncertainty it represents can support risk-aware decisions, enhance user trust, and guide additional data collection. However, many existing methods only address a single type of uncertainty or require modifications and retraining of the base model, making them difficult to adopt in real-world systems. We introduce CUPID (Comprehensive Uncertainty Plug-in estImation moDel), a general-purpose module that jointly estimates aleatoric and epistemic uncertainty without modifying or retraining the base model. CUPID can be flexibly inserted into any layer of a pretrained network. It models aleatoric uncertainty through a learned Bayesian identity mapping and captures epistemic uncertainty by analyzing the model’s internal responses to structured perturbations. We evaluate CUPID across a range of tasks, including classification, regression, and out-of-distribution detection. The results show that it consistently delivers competitive performance while offering layer-wise insights into the origins of uncertainty. By making uncertainty estimation modular, interpretable, and model-agnostic, CUPID supports more transparent and trustworthy AI.", "abstract": "Accurate estimation of uncertainty in deep learning is critical for deploying models in high-stakes domains such as medical diagnosis and autonomous decision-making, where overconfident predictions can lead to harmful outcomes. In practice, understanding the reason behind a model’s uncertainty and the type of uncertainty it represents can support risk-aware decisions, enhance user trust, and guide additional data collection. However, many existing methods only address a single type of uncertainty or require modifications and retraining of the base model, making them difficult to adopt in real-world systems. We introduce CUPID (Comprehensive Uncertainty Plug-in estImation moDel), a general-purpose module that jointly estimates aleatoric and epistemic uncertainty without modifying or retraining the base model. CUPID can be flexibly inserted into any layer of a pretrained network. It models aleatoric uncertainty through a learned Bayesian identity mapping and captures epistemic uncertainty by analyzing the model’s internal responses to structured perturbations. We evaluate CUPID across a range of tasks, including classification, regression, and out-of-distribution detection. The results show that it consistently delivers competitive performance while offering layer-wise insights into the origins of uncertainty. By making uncertainty estimation modular, interpretable, and model-agnostic, CUPID supports more transparent and trustworthy AI.", "abstract_source_venue": "OpenReviewAPI", "abstract_source_url": "https://api2.openreview.net/notes?id=nF81AkEzXg", "openreview_id": "nF81AkEzXg", "openreview_forum_id": "nF81AkEzXg", "authors": [], "pdf_url": "https://openreview.net/pdf/88f150e26308fbee588017b02f97ebbe013cdbb3.pdf", "summary_cn": "CUPID是一种无需修改或重训练基础模型的插件框架，可联合估计任意和认知不确定性，提升AI透明度和可信度。", "keywords": ["不确定性估计", "插件框架", "贝叶斯学习", "模型不可知", "可解释AI", "深度学习"], "triple": {"method": "贝叶斯恒等映射与结构化扰动分析", "result": "在分类、回归等任务中表现优异，提供层级不确定性洞察", "contribution": "实现模块化、可解释的联合不确定性估计"}}
{"venue": "ICLR", "search_title": "ExpGuard: LLM Content Moderation in Specialized Domains", "full_title": "ExpGuard: LLM Content Moderation in Specialized Domains", "url": "https://openreview.net/forum?id=t5cYJlV6aJ", "year": 2026, "is_main_conference": true, "abstract_snippet": "With the growing deployment of large language models (LLMs) in real-world applications, establishing robust safety guardrails to moderate their inputs and outputs has become essential to ensure adherence to safety policies. Current guardrail models predominantly address general human-LLM interactions, rendering LLMs vulnerable to harmful and adversarial content within domain-specific contexts, particularly those rich in technical jargon and specialized concepts. To address this limitation, we introduce ExpGuard, a robust and specialized guardrail model designed to protect against harmful prompts and responses across financial, medical, and legal domains. In addition, we present ExpGuardMix, a meticulously curated dataset comprising 58,928 labeled prompts paired with corresponding refusal and compliant responses, from these specific sectors. This dataset is divided into two subsets: ExpGuardTrain, for model training, and ExpGuardTest, a high-quality test set annotated by domain experts to evaluate model robustness against technical and domain-specific content. Comprehensive evaluations conducted on ExpGuardTest and eight established public benchmarks reveal that ExpGuard delivers competitive performance across the board while demonstrating exceptional resilience to domain-specific adversarial attacks, surpassing state-of-the-art models such as WildGuard by up to 8.9% in prompt classification and 15.3% in response classification. To encourage further research and development, we open-source our code, data, and model, enabling adaptation to additional domains and supporting the creation of increasingly robust guardrail models.", "abstract": "With the growing deployment of large language models (LLMs) in real-world applications, establishing robust safety guardrails to moderate their inputs and outputs has become essential to ensure adherence to safety policies. Current guardrail models predominantly address general human-LLM interactions, rendering LLMs vulnerable to harmful and adversarial content within domain-specific contexts, particularly those rich in technical jargon and specialized concepts. To address this limitation, we introduce ExpGuard, a robust and specialized guardrail model designed to protect against harmful prompts and responses across financial, medical, and legal domains. In addition, we present ExpGuardMix, a meticulously curated dataset comprising 58,928 labeled prompts paired with corresponding refusal and compliant responses, from these specific sectors. This dataset is divided into two subsets: ExpGuardTrain, for model training, and ExpGuardTest, a high-quality test set annotated by domain experts to evaluate model robustness against technical and domain-specific content. Comprehensive evaluations conducted on ExpGuardTest and eight established public benchmarks reveal that ExpGuard delivers competitive performance across the board while demonstrating exceptional resilience to domain-specific adversarial attacks, surpassing state-of-the-art models such as WildGuard by up to 8.9% in prompt classification and 15.3% in response classification. To encourage further research and development, we open-source our code, data, and model, enabling adaptation to additional domains and supporting the creation of increasingly robust guardrail models.", "abstract_source_venue": "OpenReviewAPI", "abstract_source_url": "https://api2.openreview.net/notes?id=t5cYJlV6aJ", "openreview_id": "t5cYJlV6aJ", "openreview_forum_id": "t5cYJlV6aJ", "authors": [], "pdf_url": "https://openreview.net/pdf/b53f1e78e0bf8325ca73d98dc307ca27eaec61ec.pdf", "summary_cn": "ExpGuard是针对金融、医疗和法律等专业领域设计的LLM内容审核模型，通过ExpGuardMix数据集提升对有害提示和响应的检测能力，在领域特定对抗攻击中表现优异。", "keywords": ["内容审核", "专业领域", "对抗攻击", "数据集", "大语言模型", "安全防护"], "triple": {"method": "构建ExpGuardMix数据集并训练ExpGuard模型", "result": "在领域特定对抗攻击中超越现有模型，提升分类性能", "contribution": "提供开源代码、数据和模型，增强专业领域LLM安全防护"}}
{"venue": "ICLR", "search_title": "Learning Human Habits with Rule-Guided Active Inference", "full_title": "Learning Human Habits with Rule-Guided Active Inference", "url": "https://openreview.net/forum?id=FZXwkBH6s7", "year": 2026, "is_main_conference": true, "abstract_snippet": "Humans navigate daily life by combining two modes of behavior: deliberate planning in novel situations and fast, automatic responses in familiar ones. Modeling human decision-making therefore requires capturing how people switch between these modes. We present a framework for learning human habits with rule-guided active inference, extending the view of the brain as a prediction machine that minimizes mismatches between expectations and observations, and computationally modeling of human(-like) behavior and habits. In our approach, habits emerge as symbolic rules that serve as compact, interpretable shortcuts for action. To learn these rules alongside the human models, we design a biologically inspired wake--sleep algorithm. In the wake phase, the agent engages in active inference on real trajectories: reconstructing states, updating beliefs, and harvesting candidate rules that reliably reduce free energy. In the sleep phase, the agent performs generative replay with its world model, refining parameters and consolidating or pruning rules by minimizing joint free energy. This alternating rule–model consolidation lets the agent build a reusable habit library while preserving the flexibility to plan. Experiments on basketball player movements, car-following behavior, medical diagnosis, and visual game strategy demonstrate that our framework improves predictive accuracy and efficiency compared to logic-based, deep learning, LLM-based, model-based RL, and prior active inference baselines, while producing interpretable rules that mirror human-like habits.", "abstract": "Humans navigate daily life by combining two modes of behavior: deliberate planning in novel situations and fast, automatic responses in familiar ones. Modeling human decision-making therefore requires capturing how people switch between these modes. We present a framework for learning human habits with rule-guided active inference, extending the view of the brain as a prediction machine that minimizes mismatches between expectations and observations, and computationally modeling of human(-like) behavior and habits. In our approach, habits emerge as symbolic rules that serve as compact, interpretable shortcuts for action. To learn these rules alongside the human models, we design a biologically inspired wake--sleep algorithm. In the wake phase, the agent engages in active inference on real trajectories: reconstructing states, updating beliefs, and harvesting candidate rules that reliably reduce free energy. In the sleep phase, the agent performs generative replay with its world model, refining parameters and consolidating or pruning rules by minimizing joint free energy. This alternating rule–model consolidation lets the agent build a reusable habit library while preserving the flexibility to plan. Experiments on basketball player movements, car-following behavior, medical diagnosis, and visual game strategy demonstrate that our framework improves predictive accuracy and efficiency compared to logic-based, deep learning, LLM-based, model-based RL, and prior active inference baselines, while producing interpretable rules that mirror human-like habits.", "abstract_source_venue": "OpenReviewAPI", "abstract_source_url": "https://api2.openreview.net/notes?id=FZXwkBH6s7", "openreview_id": "FZXwkBH6s7", "openreview_forum_id": "FZXwkBH6s7", "authors": [], "pdf_url": "https://openreview.net/pdf/e69a41007b896411d39fd7fdb11d986754428027.pdf", "summary_cn": "提出基于规则引导主动推理的框架，通过醒-睡算法学习人类习惯，生成可解释的符号规则，提升行为预测的准确性和效率。", "keywords": ["主动推理", "习惯学习", "符号规则", "醒-睡算法", "行为建模", "可解释性"], "triple": {"method": "规则引导的主动推理与醒-睡算法", "result": "预测准确性提升，生成可解释习惯规则", "contribution": "统一习惯与规划，增强模型解释性"}}
{"venue": "ICLR", "search_title": "MRAD: Zero-Shot Anomaly Detection with Memory-Driven Retrieval", "full_title": "MRAD: Zero-Shot Anomaly Detection with Memory-Driven Retrieval", "url": "https://openreview.net/forum?id=TQkFiW3AEX", "year": 2026, "is_main_conference": true, "abstract_snippet": "Zero-shot anomaly detection (ZSAD) often leverages pretrained vision or vision-language models, but many existing methods use prompt learning or complex modeling to fit the data distribution, resulting in high training or inference cost and limited cross-domain stability. To address these limitations, we propose Memory-Retrieval Anomaly Detection method (MRAD), a unified framework that replaces parametric fitting with a direct memory retrieval. The train-free base model, MRAD-TF,  freezes the CLIP image encoder and constructs a two-level memory bank (image-level and pixel-level) from auxiliary data, where feature-label pairs are explicitly stored as keys and values. During inference, anomaly scores are obtained directly by similarity retrieval over the memory bank. Based on the MRAD-TF, we further propose two lightweight variants as enhancements: (i) MRAD-FT fine-tunes the retrieval metric with two linear layers to enhance the discriminability between normal and anomaly; (ii) MRAD-CLIP injects the normal and anomalous region priors from the MRAD-FT as dynamic biases into CLIP's learnable text prompts, strengthening generalization to unseen categories. Across 16 industrial and medical datasets, the MRAD framework consistently demonstrates superior performance in anomaly classification and segmentation, under both train-free and training-based settings. Our work shows that fully leveraging the empirical distribution of raw data, rather than relying only on model fitting, can achieve stronger anomaly detection performance. Code will be released.", "abstract": "Zero-shot anomaly detection (ZSAD) often leverages pretrained vision or vision-language models, but many existing methods use prompt learning or complex modeling to fit the data distribution, resulting in high training or inference cost and limited cross-domain stability. To address these limitations, we propose Memory-Retrieval Anomaly Detection method (MRAD), a unified framework that replaces parametric fitting with a direct memory retrieval. The train-free base model, MRAD-TF,  freezes the CLIP image encoder and constructs a two-level memory bank (image-level and pixel-level) from auxiliary data, where feature-label pairs are explicitly stored as keys and values. During inference, anomaly scores are obtained directly by similarity retrieval over the memory bank. Based on the MRAD-TF, we further propose two lightweight variants as enhancements: (i) MRAD-FT fine-tunes the retrieval metric with two linear layers to enhance the discriminability between normal and anomaly; (ii) MRAD-CLIP injects the normal and anomalous region priors from the MRAD-FT as dynamic biases into CLIP's learnable text prompts, strengthening generalization to unseen categories. Across 16 industrial and medical datasets, the MRAD framework consistently demonstrates superior performance in anomaly classification and segmentation, under both train-free and training-based settings. Our work shows that fully leveraging the empirical distribution of raw data, rather than relying only on model fitting, can achieve stronger anomaly detection performance. Code will be released.", "abstract_source_venue": "OpenReviewAPI", "abstract_source_url": "https://api2.openreview.net/notes?id=TQkFiW3AEX", "openreview_id": "TQkFiW3AEX", "openreview_forum_id": "TQkFiW3AEX", "authors": [], "pdf_url": "https://openreview.net/pdf/8840201e27712169511414aa1dae07ebb96a38cf.pdf", "summary_cn": "提出MRAD框架，通过记忆检索实现零样本异常检测，无需训练或仅需轻量微调，在16个数据集上表现优异。", "keywords": ["零样本异常检测", "记忆检索", "CLIP模型", "无训练方法", "跨域稳定性", "工业与医疗应用"], "triple": {"method": "构建两级记忆库进行相似性检索", "result": "在16个数据集上实现优越的异常分类与分割性能", "contribution": "提出非参数化框架，提升跨域稳定性和检测效果"}}
{"venue": "ICLR", "search_title": "Rethinking Expressivity and Degradation-Awareness in Attention for All-in-One Blind Image Restoration", "full_title": "Rethinking Expressivity and Degradation-Awareness in Attention for All-in-One Blind Image Restoration", "url": "https://openreview.net/forum?id=IBzmQVia88", "year": 2026, "is_main_conference": true, "abstract_snippet": "All-in-one image restoration (IR) aims to recover high-quality images from diverse degradations, which in real-world settings are often mixed and unknown. Unlike single-task IR, this problem requires a model to approximate a family of heterogeneous inverse functions, making it fundamentally more challenging and practically important. Although recent focus has shifted toward large multimodal models, their robustness still depends on faithful low-level inputs, and the principles that govern effective restoration remain underexplored. We revisit attention mechanisms through the lens of all-in-one IR and identify two overlooked bottlenecks in widely adopted Restormer-style backbones: \\textit{(i) the value path remains purely linear}, restricting outputs to the span of inputs and weakening expressivity, and \\textit{(ii) the absence of an explicit global slot} prevents attention from encoding degradation context. To address these issues, we propose two minimal, backbone-agnostic primitives: a nonlinear value transform that upgrades attention from a selector to a selector–transformer, and a global spatial token that provides an explicit degradation-aware slot. Together, these additions improve restoration across synthetic, mixed, underwater, and medical benchmarks, with negligible overhead and consistent performance gains. Analyses with foundation model embeddings, spectral statistics, and separability measures further clarify their roles, positioning our study as a step toward rethinking attention primitives for robust all-in-one IR.", "abstract": "All-in-one image restoration (IR) aims to recover high-quality images from diverse degradations, which in real-world settings are often mixed and unknown. Unlike single-task IR, this problem requires a model to approximate a family of heterogeneous inverse functions, making it fundamentally more challenging and practically important. Although recent focus has shifted toward large multimodal models, their robustness still depends on faithful low-level inputs, and the principles that govern effective restoration remain underexplored. We revisit attention mechanisms through the lens of all-in-one IR and identify two overlooked bottlenecks in widely adopted Restormer-style backbones: \\textit{(i) the value path remains purely linear}, restricting outputs to the span of inputs and weakening expressivity, and \\textit{(ii) the absence of an explicit global slot} prevents attention from encoding degradation context. To address these issues, we propose two minimal, backbone-agnostic primitives: a nonlinear value transform that upgrades attention from a selector to a selector–transformer, and a global spatial token that provides an explicit degradation-aware slot. Together, these additions improve restoration across synthetic, mixed, underwater, and medical benchmarks, with negligible overhead and consistent performance gains. Analyses with foundation model embeddings, spectral statistics, and separability measures further clarify their roles, positioning our study as a step toward rethinking attention primitives for robust all-in-one IR.", "abstract_source_venue": "OpenReviewAPI", "abstract_source_url": "https://api2.openreview.net/notes?id=IBzmQVia88", "openreview_id": "IBzmQVia88", "openreview_forum_id": "IBzmQVia88", "authors": [], "pdf_url": "https://openreview.net/pdf/19fc7a687b32ff4c8f6a30a30621e07bbeb9b010.pdf", "summary_cn": "本文针对全盲图像修复中注意力机制的局限性，提出非线性值变换和全局空间标记，提升模型表达力和退化感知能力，在多种基准测试中取得显著改进。", "keywords": ["全盲图像修复", "注意力机制", "非线性变换", "退化感知", "表达力", "全局标记"], "triple": {"method": "引入非线性值变换和全局空间标记", "result": "在合成、混合、水下和医学基准测试中提升修复效果", "contribution": "增强注意力机制的表达力和退化感知能力"}}
{"venue": "ICLR", "search_title": "Inference-Time Dynamic Modality Selection for Incomplete Multimodal Classification", "full_title": "Inference-Time Dynamic Modality Selection for Incomplete Multimodal Classification", "url": "https://openreview.net/forum?id=PWhDUWRVhM", "year": 2026, "is_main_conference": true, "abstract_snippet": "Multimodal deep learning (MDL) has achieved remarkable success across various domains, yet its practical deployment is often hindered by incomplete multimodal data. Existing incomplete MDL methods either discard missing modalities, risking the loss of valuable task-relevant information, or recover them, potentially introducing irrelevant noise, leading to the discarding-imputation dilemma. To address this dilemma, in this paper, we propose DyMo, a new inference-time dynamic modality selection framework that adaptively identifies and integrates reliable recovered modalities, fully exploring task-relevant information beyond the conventional discard-or-impute paradigm. Central to DyMo is a novel selection algorithm that maximizes multimodal task-relevant information for each test sample. Since direct estimation of such information at test time is intractable due to the unknown data distribution, we theoretically establish a connection between information and the task loss, which we compute at inference time as a tractable proxy. Building on this, a novel principled reward function is proposed to guide modality selection. In addition, we design a flexible multimodal network architecture compatible with arbitrary modality combinations, alongside a tailored training strategy for robust representation learning. Extensive experiments on diverse natural and medical datasets show that DyMo significantly outperforms state-of-the-art incomplete/dynamic MDL methods across various missing-data scenarios. Our code will be available at https://github.com/anonymous.", "abstract": "Multimodal deep learning (MDL) has achieved remarkable success across various domains, yet its practical deployment is often hindered by incomplete multimodal data. Existing incomplete MDL methods either discard missing modalities, risking the loss of valuable task-relevant information, or recover them, potentially introducing irrelevant noise, leading to the discarding-imputation dilemma. To address this dilemma, in this paper, we propose DyMo, a new inference-time dynamic modality selection framework that adaptively identifies and integrates reliable recovered modalities, fully exploring task-relevant information beyond the conventional discard-or-impute paradigm. Central to DyMo is a novel selection algorithm that maximizes multimodal task-relevant information for each test sample. Since direct estimation of such information at test time is intractable due to the unknown data distribution, we theoretically establish a connection between information and the task loss, which we compute at inference time as a tractable proxy. Building on this, a novel principled reward function is proposed to guide modality selection. In addition, we design a flexible multimodal network architecture compatible with arbitrary modality combinations, alongside a tailored training strategy for robust representation learning. Extensive experiments on diverse natural and medical datasets show that DyMo significantly outperforms state-of-the-art incomplete/dynamic MDL methods across various missing-data scenarios. Our code will be available at https://github.com/anonymous.", "abstract_source_venue": "OpenReviewAPI", "abstract_source_url": "https://api2.openreview.net/notes?id=PWhDUWRVhM", "openreview_id": "PWhDUWRVhM", "openreview_forum_id": "PWhDUWRVhM", "authors": [], "pdf_url": "https://openreview.net/pdf/5c9fa010f820f2437e520f48a480adac61df09a7.pdf", "summary_cn": "提出DyMo框架，动态选择可靠模态以解决多模态分类中数据缺失问题，超越传统丢弃或填补方法，提升性能。", "keywords": ["多模态学习", "动态模态选择", "数据缺失", "推理时间优化", "信息最大化", "医疗分类"], "triple": {"method": "基于任务损失的信息代理与奖励函数", "result": "在自然和医疗数据集上显著优于现有方法", "contribution": "提出动态模态选择框架，突破丢弃-填补困境"}}
{"venue": "ICLR", "search_title": "FETAL-GAUGE: A BENCHMARK FOR ASSESSING VISION-LANGUAGE MODELS IN FETAL ULTRASOUND", "full_title": "FETAL-GAUGE: A BENCHMARK FOR ASSESSING VISION-LANGUAGE MODELS IN FETAL ULTRASOUND", "url": "https://openreview.net/forum?id=AHZuGrWZ0d", "year": 2026, "is_main_conference": true, "abstract_snippet": "The growing demand for prenatal ultrasound imaging has intensified a global shortage of trained sonographers, creating barriers to essential fetal health monitoring. Deep learning has the potential to enhance sonographers' efficiency and support the training of new practitioners. Vision-Language Models (VLMs) are particularly promising for ultrasound interpretation, as they can jointly process images and text to perform multiple clinical tasks within a single framework. However, despite the expansion of VLMs, no standardized benchmark exists to evaluate their performance in fetal ultrasound imaging. This gap is primarily due to the modality’s challenging nature, operator dependency, and the limited public availability of datasets. To address this gap, we present Fetal-Gauge, the first and largest visual question answering benchmark specifically designed to evaluate VLMs across various fetal ultrasound tasks. Our benchmark comprises over 42,000 images and 93,000 question-answer pairs, spanning anatomical plane identification, visual grounding of anatomical structures, fetal orientation assessment, clinical view conformity, and clinical diagnosis. We systematically evaluate several state-of-the-art VLMs, including general-purpose and medical-specific models, and reveal a substantial performance gap: the best-performing model achieves only 55\\% accuracy, far below clinical requirements. Our analysis identifies critical limitations of current VLMs in fetal ultrasound interpretation, highlighting the urgent need for domain-adapted architectures and specialized training approaches. Fetal-Gauge establishes a rigorous foundation for advancing multimodal deep learning in prenatal care and provides a pathway toward addressing global healthcare accessibility challenges. Our benchmark is publicly available at www.github.com", "abstract": "The growing demand for prenatal ultrasound imaging has intensified a global shortage of trained sonographers, creating barriers to essential fetal health monitoring. Deep learning has the potential to enhance sonographers' efficiency and support the training of new practitioners. Vision-Language Models (VLMs) are particularly promising for ultrasound interpretation, as they can jointly process images and text to perform multiple clinical tasks within a single framework. However, despite the expansion of VLMs, no standardized benchmark exists to evaluate their performance in fetal ultrasound imaging. This gap is primarily due to the modality’s challenging nature, operator dependency, and the limited public availability of datasets. To address this gap, we present Fetal-Gauge, the first and largest visual question answering benchmark specifically designed to evaluate VLMs across various fetal ultrasound tasks. Our benchmark comprises over 42,000 images and 93,000 question-answer pairs, spanning anatomical plane identification, visual grounding of anatomical structures, fetal orientation assessment, clinical view conformity, and clinical diagnosis. We systematically evaluate several state-of-the-art VLMs, including general-purpose and medical-specific models, and reveal a substantial performance gap: the best-performing model achieves only 55\\% accuracy, far below clinical requirements. Our analysis identifies critical limitations of current VLMs in fetal ultrasound interpretation, highlighting the urgent need for domain-adapted architectures and specialized training approaches. Fetal-Gauge establishes a rigorous foundation for advancing multimodal deep learning in prenatal care and provides a pathway toward addressing global healthcare accessibility challenges. Our benchmark is publicly available at www.github.com", "abstract_source_venue": "OpenReviewAPI", "abstract_source_url": "https://api2.openreview.net/notes?id=AHZuGrWZ0d", "openreview_id": "AHZuGrWZ0d", "openreview_forum_id": "AHZuGrWZ0d", "authors": [], "pdf_url": "https://openreview.net/pdf/44352c8c8183d482690a6b74231105e2e275690e.pdf", "summary_cn": "本文提出首个胎儿超声视觉问答基准Fetal-Gauge，包含超4.2万图像和9.3万问答对，评估多种视觉语言模型，发现最佳模型准确率仅55%，远低于临床需求。", "keywords": ["胎儿超声", "视觉语言模型", "基准评估", "视觉问答", "深度学习", "产前护理"], "triple": {"method": "构建大规模视觉问答基准", "result": "最佳模型准确率仅55%", "contribution": "填补胎儿超声VLM评估空白"}}
{"venue": "ICLR", "search_title": "Rethinking Radiology Report Generation: From Narrative Flow to Topic-Guided Findings", "full_title": "Rethinking Radiology Report Generation: From Narrative Flow to Topic-Guided Findings", "url": "https://openreview.net/forum?id=nV3SAjFlyv", "year": 2026, "is_main_conference": true, "abstract_snippet": "Vision-Language Models (VLMs) for radiology report generation are typically trained to mimic the narrative flow of human experts. However, we identify a potential limitation in this conventional paradigm. We hypothesize that optimizing for narrative coherence encourages models to rely on linguistic priors and inter-sentence correlations, which can weaken their grounding in direct visual evidence and lead to factual inaccuracies. To investigate this, we design a controlled experiment demonstrating that as textual context increases, a model's reliance on the input image systematically decays. We propose LLaVA-TA (Topic-guided and Anatomy-aware), a new fine-tuning framework that directly addresses this challenge by re-engineering the generation process. Instead of producing a linear narrative, LLaVA-TA decomposes the report into a set of independent, clinically-relevant topics. By training the model to generate a discrete finding for each topic conditioned on both the full image and its corresponding anatomical region, we reduce the model's reliance on narrative flow and enforce stricter visual grounding. Our experiments show that LLaVA-TA sets a new state of the art on the MIMIC-CXR dataset, significantly improving clinical accuracy on metrics like RadGraph F1 (from 29.4 to 44.0) and CheXpert F1-14 (from 39.5 to 71.5) over strong baselines. Our work demonstrates that dismantling a report's narrative structure to enforce independent, visually-grounded observations is a crucial and effective step toward building more accurate and reliable medical VLMs.", "abstract": "Vision-Language Models (VLMs) for radiology report generation are typically trained to mimic the narrative flow of human experts. However, we identify a potential limitation in this conventional paradigm. We hypothesize that optimizing for narrative coherence encourages models to rely on linguistic priors and inter-sentence correlations, which can weaken their grounding in direct visual evidence and lead to factual inaccuracies. To investigate this, we design a controlled experiment demonstrating that as textual context increases, a model's reliance on the input image systematically decays. We propose LLaVA-TA (Topic-guided and Anatomy-aware), a new fine-tuning framework that directly addresses this challenge by re-engineering the generation process. Instead of producing a linear narrative, LLaVA-TA decomposes the report into a set of independent, clinically-relevant topics. By training the model to generate a discrete finding for each topic conditioned on both the full image and its corresponding anatomical region, we reduce the model's reliance on narrative flow and enforce stricter visual grounding. Our experiments show that LLaVA-TA sets a new state of the art on the MIMIC-CXR dataset, significantly improving clinical accuracy on metrics like RadGraph F1 (from 29.4 to 44.0) and CheXpert F1-14 (from 39.5 to 71.5) over strong baselines. Our work demonstrates that dismantling a report's narrative structure to enforce independent, visually-grounded observations is a crucial and effective step toward building more accurate and reliable medical VLMs.", "abstract_source_venue": "OpenReviewAPI", "abstract_source_url": "https://api2.openreview.net/notes?id=nV3SAjFlyv", "openreview_id": "nV3SAjFlyv", "openreview_forum_id": "nV3SAjFlyv", "authors": [], "pdf_url": "https://openreview.net/pdf/068c77b0c2d8fa65c2615f2151b075e04b57103d.pdf", "summary_cn": "研究发现传统放射报告生成模型过度依赖文本叙事，导致视觉证据不足。提出LLaVA-TA框架，通过主题引导和解剖感知生成独立发现，显著提升临床准确性。", "keywords": ["放射报告生成", "视觉语言模型", "主题引导", "解剖感知", "临床准确性", "视觉基础"], "triple": {"method": "LLaVA-TA框架，基于主题引导和解剖感知生成独立发现", "result": "在MIMIC-CXR数据集上刷新SOTA，RadGraph F1从29.4提升至44.0", "contribution": "打破叙事结构，增强视觉基础，提高医学VLM准确性和可靠性"}}
{"venue": "ICLR", "search_title": "Learning Domain-Aware Task Prompt Representations for Multi-Domain All-in-One Image Restoration", "full_title": "Learning Domain-Aware Task Prompt Representations for Multi-Domain All-in-One Image Restoration", "url": "https://openreview.net/forum?id=CzVlgDOF7L", "year": 2026, "is_main_conference": true, "abstract_snippet": "Recently, significant breakthroughs have been made in all-in-one image restoration (AiOIR), which can handle multiple restoration tasks with a single model. However, existing methods typically focus on a specific image domain, such as natural scene, medical imaging, or remote sensing. In this work, we aim to extend AiOIR to multiple domains and propose the first multi-domain all-in-one image restoration method, DATPRL-IR, based on our proposed Domain-Aware Task Prompt Representation L}earning. Specifically, we first construct a task prompt pool containing multiple task prompts, in which task-related knowledge is implicitly encoded. For each input image, the model adaptively selects the most relevant task prompts and composes them into an instance-level task representation via a prompt composition mechanism (PCM). Furthermore, to endow the model with domain awareness, we introduce another domain prompt pool and distill domain priors from multimodal large language models into the domain prompts. PCM is utilized to combine the adaptively selected domain prompts into a domain representation for each input image. Finally, the two representations are fused to form a domain-aware task prompt representation which can make full use of both specific and shared knowledge across tasks and domains to guide the subsequent restoration process. Extensive experiments demonstrate that our DATRL-IR significantly outperforms existing SOTA image restoration methods, while exhibiting strong generalization capabilities. We believe that this work provides a new research paradigm and represents a step towards more unified image restoration.", "abstract": "Recently, significant breakthroughs have been made in all-in-one image restoration (AiOIR), which can handle multiple restoration tasks with a single model. However, existing methods typically focus on a specific image domain, such as natural scene, medical imaging, or remote sensing. In this work, we aim to extend AiOIR to multiple domains and propose the first multi-domain all-in-one image restoration method, DATPRL-IR, based on our proposed Domain-Aware Task Prompt Representation L}earning. Specifically, we first construct a task prompt pool containing multiple task prompts, in which task-related knowledge is implicitly encoded. For each input image, the model adaptively selects the most relevant task prompts and composes them into an instance-level task representation via a prompt composition mechanism (PCM). Furthermore, to endow the model with domain awareness, we introduce another domain prompt pool and distill domain priors from multimodal large language models into the domain prompts. PCM is utilized to combine the adaptively selected domain prompts into a domain representation for each input image. Finally, the two representations are fused to form a domain-aware task prompt representation which can make full use of both specific and shared knowledge across tasks and domains to guide the subsequent restoration process. Extensive experiments demonstrate that our DATRL-IR significantly outperforms existing SOTA image restoration methods, while exhibiting strong generalization capabilities. We believe that this work provides a new research paradigm and represents a step towards more unified image restoration.", "abstract_source_venue": "OpenReviewAPI", "abstract_source_url": "https://api2.openreview.net/notes?id=CzVlgDOF7L", "openreview_id": "CzVlgDOF7L", "openreview_forum_id": "CzVlgDOF7L", "authors": [], "pdf_url": "https://openreview.net/pdf/0806528f3fbff20678e5f2045e5c1f3195e5a4fd.pdf", "summary_cn": "提出DATPRL-IR方法，通过领域感知任务提示表示学习，实现多领域一体化图像恢复，显著超越现有方法并展现强泛化能力。", "keywords": ["多领域图像恢复", "任务提示表示", "领域感知", "提示组合机制", "一体化模型", "泛化能力"], "triple": {"method": "领域感知任务提示表示学习", "result": "性能显著超越现有SOTA方法", "contribution": "提出首个多领域一体化图像恢复新范式"}}
{"venue": "ICLR", "search_title": "A-TPT: Angular Diversity Calibration Properties for Test-Time Prompt Tuning of Vision-Language Models", "full_title": "A-TPT: Angular Diversity Calibration Properties for Test-Time Prompt Tuning of Vision-Language Models", "url": "https://openreview.net/forum?id=VhlSBZebEw", "year": 2026, "is_main_conference": true, "abstract_snippet": "Test-time prompt tuning (TPT) has emerged as a promising technique for adapting large vision-language models (VLMs) to unseen tasks without relying on labeled data. However, the lack of dispersion between textual features can hurt calibration performance, which raises concerns about VLMs' reliability, trustworthiness, and safety. Current TPT approaches primarily focus on improving prompt calibration by either maximizing average textual feature dispersion or enforcing orthogonality constraints to encourage angular separation. However, these methods may not always have optimal angular separation between class-wise textual features, which implies overlooking the critical role of angular diversity. To address this, we propose A-TPT, a novel TPT framework that introduces angular diversity to encourage uniformity in the distribution of normalized textual features induced by corresponding learnable prompts. This uniformity is achieved by maximizing the minimum pairwise angular distance between features on the unit hypersphere. We show that our approach consistently surpasses state-of-the-art TPT methods in reducing the aggregate average calibration error while maintaining comparable accuracy through extensive experiments with various backbones on different datasets. Notably, our approach exhibits superior zero-shot calibration performance on natural distribution shifts and generalizes well to medical datasets. We provide extensive analyses, including theoretical aspects, to establish the grounding of A-TPT. These results highlight the potency of promoting angular diversity to achieve well-dispersed textual features, significantly improving VLM calibration during test-time adaptation. Our code will be made publicly available.", "abstract": "Test-time prompt tuning (TPT) has emerged as a promising technique for adapting large vision-language models (VLMs) to unseen tasks without relying on labeled data. However, the lack of dispersion between textual features can hurt calibration performance, which raises concerns about VLMs' reliability, trustworthiness, and safety. Current TPT approaches primarily focus on improving prompt calibration by either maximizing average textual feature dispersion or enforcing orthogonality constraints to encourage angular separation. However, these methods may not always have optimal angular separation between class-wise textual features, which implies overlooking the critical role of angular diversity. To address this, we propose A-TPT, a novel TPT framework that introduces angular diversity to encourage uniformity in the distribution of normalized textual features induced by corresponding learnable prompts. This uniformity is achieved by maximizing the minimum pairwise angular distance between features on the unit hypersphere. We show that our approach consistently surpasses state-of-the-art TPT methods in reducing the aggregate average calibration error while maintaining comparable accuracy through extensive experiments with various backbones on different datasets. Notably, our approach exhibits superior zero-shot calibration performance on natural distribution shifts and generalizes well to medical datasets. We provide extensive analyses, including theoretical aspects, to establish the grounding of A-TPT. These results highlight the potency of promoting angular diversity to achieve well-dispersed textual features, significantly improving VLM calibration during test-time adaptation. Our code will be made publicly available.", "abstract_source_venue": "OpenReviewAPI", "abstract_source_url": "https://api2.openreview.net/notes?id=VhlSBZebEw", "openreview_id": "VhlSBZebEw", "openreview_forum_id": "VhlSBZebEw", "authors": [], "pdf_url": "https://openreview.net/pdf/55afd4785f8112ea78a4b2263d53c4780d325307.pdf", "summary_cn": "提出A-TPT框架，通过最大化文本特征间最小角距离增强角多样性，提升视觉语言模型在测试时提示调优中的校准性能，减少校准误差。", "keywords": ["测试时提示调优", "角多样性", "视觉语言模型", "校准性能", "特征分布", "零样本校准"], "triple": {"method": "最大化最小成对角距离", "result": "降低平均校准误差，保持准确率", "contribution": "提升VLM校准性能与可靠性"}}
{"venue": "ICLR", "search_title": "Bridging Radiology and Pathology Foundation Models via Concept-Based Multimodal Co-Adaptation", "full_title": "Bridging Radiology and Pathology Foundation Models via Concept-Based Multimodal Co-Adaptation", "url": "https://openreview.net/forum?id=oxgcPoDkNv", "year": 2026, "is_main_conference": true, "abstract_snippet": "Pretrained medical foundation models (FMs) have shown strong generalization across diverse imaging tasks, such as disease classification in radiology and tumor grading in histopathology. While recent advances in parameter-efficient finetuning have enabled effective adaptation of FMs to downstream tasks, these approaches are typically designed for a single modality. In contrast, many clinical workflows rely on joint diagnosis from heterogeneous domains, such as radiology and pathology, where fully leveraging the representation capacity of multiple FMs remains an open challenge. To address this gap, we propose Concept Tuning and Fusing (CTF), a parameter-efficient framework that uses clinically grounded concepts as a shared semantic interface to enable cross-modal co-adaptation before fusion. By incorporating task-specific concepts that are relevant across modalities, CTF aligns radiology and pathology representations, thereby enhancing their complementarity and enabling interpretation. We further design a Global–Context–Shared Prompt (GCSP) mechanism, which employs a small set of learnable tokens to capture domain-specific priors, shared patient-level information, and cross-domain context. The resulting concept alignment scores from each modality are then fused to produce a final prediction. Extensive experiments demonstrate that CTF outperforms strong unimodal, latent-fusion, and adapter-based baselines (e.g., AUC 0.903 on TCGA-GBMLGG). Notably, CTF achieves these gains without finetuning the full FMs, requiring only 0.15\\% additional parameters, thus highlighting the effectiveness of concept-based multimodal co-adaptation. Our code is anonymously available at: https://anonymous.4open.science/r/CTF-27C2.", "abstract": "Pretrained medical foundation models (FMs) have shown strong generalization across diverse imaging tasks, such as disease classification in radiology and tumor grading in histopathology. While recent advances in parameter-efficient finetuning have enabled effective adaptation of FMs to downstream tasks, these approaches are typically designed for a single modality. In contrast, many clinical workflows rely on joint diagnosis from heterogeneous domains, such as radiology and pathology, where fully leveraging the representation capacity of multiple FMs remains an open challenge. To address this gap, we propose Concept Tuning and Fusing (CTF), a parameter-efficient framework that uses clinically grounded concepts as a shared semantic interface to enable cross-modal co-adaptation before fusion. By incorporating task-specific concepts that are relevant across modalities, CTF aligns radiology and pathology representations, thereby enhancing their complementarity and enabling interpretation. We further design a Global–Context–Shared Prompt (GCSP) mechanism, which employs a small set of learnable tokens to capture domain-specific priors, shared patient-level information, and cross-domain context. The resulting concept alignment scores from each modality are then fused to produce a final prediction. Extensive experiments demonstrate that CTF outperforms strong unimodal, latent-fusion, and adapter-based baselines (e.g., AUC 0.903 on TCGA-GBMLGG). Notably, CTF achieves these gains without finetuning the full FMs, requiring only 0.15\\% additional parameters, thus highlighting the effectiveness of concept-based multimodal co-adaptation. Our code is anonymously available at: https://anonymous.4open.science/r/CTF-27C2.", "abstract_source_venue": "OpenReviewAPI", "abstract_source_url": "https://api2.openreview.net/notes?id=oxgcPoDkNv", "openreview_id": "oxgcPoDkNv", "openreview_forum_id": "oxgcPoDkNv", "authors": [], "pdf_url": "https://openreview.net/pdf/e7e3ab7ec9279cd5c0c17f1afa02346cb35f1452.pdf", "summary_cn": "提出概念调谐与融合框架，利用临床概念作为共享语义接口，实现放射学与病理学基础模型的多模态协同适应，提升跨模态预测性能。", "keywords": ["多模态学习", "基础模型", "概念对齐", "参数高效调优", "放射病理融合", "协同适应"], "triple": {"method": "概念调谐与融合框架", "result": "AUC达0.903，优于单模态及潜在融合基线", "contribution": "实现跨模态协同适应，参数效率高"}}
{"venue": "ICLR", "search_title": "ADEPT: Continual Pretraining via Adaptive Expansion and Dynamic Decoupled Tuning", "full_title": "ADEPT: Continual Pretraining via Adaptive Expansion and Dynamic Decoupled Tuning", "url": "https://openreview.net/forum?id=vcWDDfA4Ev", "year": 2026, "is_main_conference": true, "abstract_snippet": "Conventional continual pretraining (CPT) for large language model (LLM) domain adaptation often suffers from catastrophic forgetting and limited domain capacity. Existing strategies adopt layer expansion, introducing additional trainable parameters to accommodate new knowledge. However, the uniform expansion and updates still entangle general and domain learning, undermining its effectiveness. Our pilot studies reveal that LLMs exhibit functional specialization, where layers and units differentially encode general-critical capabilities, suggesting that parameter expansion and optimization should be function-aware. We then propose ADEPT, Adaptive Expansion and Dynamic Decoupled Tuning for continual pretraining, a two-stage framework for domain-adaptive CPT. ADEPT first performs General-Competence Guided Selective Layer Expansion, duplicating layers least critical for the general domain to increase representational capacity while minimizing interference with general knowledge. It then applies Adaptive Unit-Wise Decoupled Tuning, disentangling parameter units within expanded layers according to their general-domain importance and assigning asymmetric learning rates to balance knowledge injection and retention. Experiments on mathematical and medical domains show that ADEPT outperforms full-parameter CPT by up to 5.76% on the general benchmarks and 5.58% on the target domain benchmarks with only 15% of parameters tuned and less than 50% training time. Ablation studies, theoretical analysis, and extended investigations further demonstrate the necessity of targeted expansion and decoupled optimization, providing new principles for efficient and robust domain-adaptive CPT. Our code is open-sourced at https://anonymous.4open.science/r/ADEPT-F2E3", "abstract": "Conventional continual pretraining (CPT) for large language model (LLM) domain adaptation often suffers from catastrophic forgetting and limited domain capacity. Existing strategies adopt layer expansion, introducing additional trainable parameters to accommodate new knowledge. However, the uniform expansion and updates still entangle general and domain learning, undermining its effectiveness. Our pilot studies reveal that LLMs exhibit functional specialization, where layers and units differentially encode general-critical capabilities, suggesting that parameter expansion and optimization should be function-aware. We then propose ADEPT, Adaptive Expansion and Dynamic Decoupled Tuning for continual pretraining, a two-stage framework for domain-adaptive CPT. ADEPT first performs General-Competence Guided Selective Layer Expansion, duplicating layers least critical for the general domain to increase representational capacity while minimizing interference with general knowledge. It then applies Adaptive Unit-Wise Decoupled Tuning, disentangling parameter units within expanded layers according to their general-domain importance and assigning asymmetric learning rates to balance knowledge injection and retention. Experiments on mathematical and medical domains show that ADEPT outperforms full-parameter CPT by up to 5.76% on the general benchmarks and 5.58% on the target domain benchmarks with only 15% of parameters tuned and less than 50% training time. Ablation studies, theoretical analysis, and extended investigations further demonstrate the necessity of targeted expansion and decoupled optimization, providing new principles for efficient and robust domain-adaptive CPT. Our code is open-sourced at https://anonymous.4open.science/r/ADEPT-F2E3", "abstract_source_venue": "OpenReviewAPI", "abstract_source_url": "https://api2.openreview.net/notes?id=vcWDDfA4Ev", "openreview_id": "vcWDDfA4Ev", "openreview_forum_id": "vcWDDfA4Ev", "authors": [], "pdf_url": "https://openreview.net/pdf/99af89d5a20d66ec436a712f5809f5767f41b6d4.pdf", "summary_cn": "ADEPT提出自适应扩展与动态解耦调优的两阶段框架，用于大语言模型持续预训练，有效缓解灾难性遗忘并提升领域适应能力。", "keywords": ["持续预训练", "灾难性遗忘", "自适应扩展", "解耦调优", "领域适应", "大语言模型"], "triple": {"method": "自适应层扩展与单元解耦调优", "result": "在数学和医学领域性能提升，训练参数和时间减少", "contribution": "提出高效鲁棒的领域自适应持续预训练新原则"}}
{"venue": "ICLR", "search_title": "Contextual Similarity Distillation: Ensemble Uncertainties with a Single Model", "full_title": "Contextual Similarity Distillation: Ensemble Uncertainties with a Single Model", "url": "https://openreview.net/forum?id=arms7s9dDK", "year": 2026, "is_main_conference": true, "abstract_snippet": "Uncertainty quantification is a critical aspect of reinforcement learning and deep learning, with numerous applications ranging from efficient exploration and stable offline reinforcement learning to outlier detection in medical diagnostics. The scale of modern neural networks, however, complicates the use of many theoretically well-motivated approaches such as full Bayesian inference. Approximate methods like deep ensembles can provide reliable uncertainty estimates but still remain computationally expensive. In this work, we propose contextual similarity distillation, a novel approach that explicitly estimates the variance of an ensemble of deep neural networks with a single model, without ever learning or evaluating such an ensemble in the first place. Our method builds on the predictable learning dynamics of wide neural networks, governed by the neural tangent kernel, to derive an efficient approximation of the predictive variance of an infinite ensemble. Specifically, we reinterpret the computation of ensemble variance as a supervised regression problem with kernel similarities as regression targets. The resulting model can estimate predictive variance at inference time with a single forward pass, and can make use of unlabeled target-domain data or data augmentations to refine its uncertainty estimates. We empirically validate our method across a variety of out-of-distribution detection benchmarks and sparse-reward reinforcement learning environments. We find that our single-model method performs competitively and sometimes superior to ensemble-based baselines and serves as a reliable signal for efficient exploration. These results, we believe, position contextual similarity distillation as a principled and scalable alternative for uncertainty quantification in reinforcement learning and general deep learning.", "abstract": "Uncertainty quantification is a critical aspect of reinforcement learning and deep learning, with numerous applications ranging from efficient exploration and stable offline reinforcement learning to outlier detection in medical diagnostics. The scale of modern neural networks, however, complicates the use of many theoretically well-motivated approaches such as full Bayesian inference. Approximate methods like deep ensembles can provide reliable uncertainty estimates but still remain computationally expensive. In this work, we propose contextual similarity distillation, a novel approach that explicitly estimates the variance of an ensemble of deep neural networks with a single model, without ever learning or evaluating such an ensemble in the first place. Our method builds on the predictable learning dynamics of wide neural networks, governed by the neural tangent kernel, to derive an efficient approximation of the predictive variance of an infinite ensemble. Specifically, we reinterpret the computation of ensemble variance as a supervised regression problem with kernel similarities as regression targets. The resulting model can estimate predictive variance at inference time with a single forward pass, and can make use of unlabeled target-domain data or data augmentations to refine its uncertainty estimates. We empirically validate our method across a variety of out-of-distribution detection benchmarks and sparse-reward reinforcement learning environments. We find that our single-model method performs competitively and sometimes superior to ensemble-based baselines and serves as a reliable signal for efficient exploration. These results, we believe, position contextual similarity distillation as a principled and scalable alternative for uncertainty quantification in reinforcement learning and general deep learning.", "abstract_source_venue": "OpenReviewAPI", "abstract_source_url": "https://api2.openreview.net/notes?id=arms7s9dDK", "openreview_id": "arms7s9dDK", "openreview_forum_id": "arms7s9dDK", "authors": [], "pdf_url": "https://openreview.net/pdf/e6c4a3600c2182e3b8909b07eeeb32e04294d41d.pdf", "summary_cn": "提出上下文相似性蒸馏方法，通过单个模型高效估计深度集成的不确定性，无需实际训练集成，在离群检测和强化学习中表现优异。", "keywords": ["不确定性量化", "上下文相似性蒸馏", "深度集成", "神经正切核", "离群检测", "强化学习"], "triple": {"method": "基于神经正切核的相似性回归", "result": "单模型不确定性估计媲美集成方法", "contribution": "提供可扩展的高效不确定性量化方案"}}
{"venue": "ICLR", "search_title": "Healthcare Insurance Fraud Detection via Continual Fiedler Vector Graph Model", "full_title": "Healthcare Insurance Fraud Detection via Continual Fiedler Vector Graph Model", "url": "https://openreview.net/forum?id=ZWDvIKMkMG", "year": 2026, "is_main_conference": true, "abstract_snippet": "Healthcare insurance fraud detection presents unique machine learning challenges: labeled data are scarce due to delayed verification processes, and fraudulent behaviors evolve rapidly, often manifesting in complex, graph-structured interactions. Existing methods struggle in such settings. Pretraining routines typically overlook structural anomalies under limited supervision, while online models often fail to adapt to changing fraud patterns without labeled updates.\nTo address these issues, we propose the Continual Fiedler Vector Graph model (ConFVG), a fraud detection framework designed for label-scarce and non-stationary environments. The framework comprises two key components. To mitigate label scarcity, we develop a Fiedler Vector-guided graph autoencoder that leverages spectral graph properties to learn structure-aware node representations. The Fiedler Vector, derived from the second smallest eigenvalue of the graph Laplacian, captures global topological signals such as community boundaries and connectivity bottlenecks, which are patterns frequently associated with collusive fraud. This enables the model to identify structurally anomalous nodes without relying on labels. To handle evolving graph streams, we propose a Subgraph Attention Fusion (SAF) module that constructs neighborhood subgraphs and applies attention-based reweighting to emphasize emerging high-risk structures. This design allows the model to adapt to new fraud patterns in real time. A Mean Teacher mechanism further stabilizes online updates and prevents forgetting of previously acquired knowledge.\nExperiments on real-world medical fraud datasets demonstrate that the Continual Fiedler Vector Graph model outperforms state-of-the-art baselines in both low-label and distribution-shift scenarios, offering a scalable and structure-sensitive solution for real-time fraud detection.", "abstract": "Healthcare insurance fraud detection presents unique machine learning challenges: labeled data are scarce due to delayed verification processes, and fraudulent behaviors evolve rapidly, often manifesting in complex, graph-structured interactions. Existing methods struggle in such settings. Pretraining routines typically overlook structural anomalies under limited supervision, while online models often fail to adapt to changing fraud patterns without labeled updates.\nTo address these issues, we propose the Continual Fiedler Vector Graph model (ConFVG), a fraud detection framework designed for label-scarce and non-stationary environments. The framework comprises two key components. To mitigate label scarcity, we develop a Fiedler Vector-guided graph autoencoder that leverages spectral graph properties to learn structure-aware node representations. The Fiedler Vector, derived from the second smallest eigenvalue of the graph Laplacian, captures global topological signals such as community boundaries and connectivity bottlenecks, which are patterns frequently associated with collusive fraud. This enables the model to identify structurally anomalous nodes without relying on labels. To handle evolving graph streams, we propose a Subgraph Attention Fusion (SAF) module that constructs neighborhood subgraphs and applies attention-based reweighting to emphasize emerging high-risk structures. This design allows the model to adapt to new fraud patterns in real time. A Mean Teacher mechanism further stabilizes online updates and prevents forgetting of previously acquired knowledge.\nExperiments on real-world medical fraud datasets demonstrate that the Continual Fiedler Vector Graph model outperforms state-of-the-art baselines in both low-label and distribution-shift scenarios, offering a scalable and structure-sensitive solution for real-time fraud detection.", "abstract_source_venue": "OpenReviewAPI", "abstract_source_url": "https://api2.openreview.net/notes?id=ZWDvIKMkMG", "openreview_id": "ZWDvIKMkMG", "openreview_forum_id": "ZWDvIKMkMG", "authors": [], "pdf_url": "https://openreview.net/pdf/81a0f8133f264a8cd209ce795cb83d4619fa598b.pdf", "summary_cn": "提出ConFVG模型，利用Fiedler向量捕捉图结构异常，结合子图注意力融合处理动态欺诈模式，在标签稀缺和分布变化场景下优于现有方法。", "keywords": ["医疗保险欺诈检测", "图神经网络", "Fiedler向量", "持续学习", "结构异常检测", "子图注意力"], "triple": {"method": "Fiedler向量图自编码器与子图注意力融合", "result": "在低标签和分布偏移场景中超越现有基线", "contribution": "提供可扩展、结构敏感的实时欺诈检测方案"}}
{"venue": "ICLR", "search_title": "ReLaSH: Reconstructing Joint Latent Spaces for Efficient Generation of Synthetic Hypergraphs with Hyperlink Attributes", "full_title": "ReLaSH: Reconstructing Joint Latent Spaces for Efficient Generation of Synthetic Hypergraphs with Hyperlink Attributes", "url": "https://openreview.net/forum?id=SG3kS2h44t", "year": 2026, "is_main_conference": true, "abstract_snippet": "Hypergraph network data, which capture multi-way interactions among entities, have become increasingly prevalent in the big data era, spanning fields such as social science, medical research, and biology. Generating synthetic hyperlinks with attributes from an observed hypergraph has broad applications in data augmentation, simulation, and advancing the understanding of real-world complex systems. This task, however, poses unique challenges due to special properties of hypergraphs, including discreteness, hyperlink sparsity, and the mixed data types of hyperlinks and their attributes, rendering many existing generative models unsuitable. In this paper, we introduce ReLaSH (REconstructing joint LAtent Spaces for Hypergraphs with attributes), a general generative framework for producing realistic synthetic hypergraph data with hyperlink attributes via training a likelihood-based joint embedding model and reconstructing the joint latent space. Given a hypergraph dataset, ReLaSH first embeds the hyperlinks and their attributes into a joint latent space by training a likelihood-based model, and then reconstructs this joint latent space using a distribution-free generator. The generation task is completed by first sampling embeddings from the distribution-free generator and then decoding them into hyperlinks and attributes through the trained likelihood-based model. Compared with existing generative models, ReLaSH explicitly accounts for the unique structure of hypergraphs and jointly models hyperlinks and their attributes. Moreover, the likelihood-based embedding model provides efficiency and interpretability relative to deep black-box architectures, while the distribution-free generator in the joint latent space ensures flexibility. We theoretically demonstrate consistency and generalizability of ReLaSH. Empirical results on a range of real-world datasets from diverse domains demonstrate the strong performance of ReLaSH, underscoring its broad utility and effectiveness in practical applications.", "abstract": "Hypergraph network data, which capture multi-way interactions among entities, have become increasingly prevalent in the big data era, spanning fields such as social science, medical research, and biology. Generating synthetic hyperlinks with attributes from an observed hypergraph has broad applications in data augmentation, simulation, and advancing the understanding of real-world complex systems. This task, however, poses unique challenges due to special properties of hypergraphs, including discreteness, hyperlink sparsity, and the mixed data types of hyperlinks and their attributes, rendering many existing generative models unsuitable. In this paper, we introduce ReLaSH (REconstructing joint LAtent Spaces for Hypergraphs with attributes), a general generative framework for producing realistic synthetic hypergraph data with hyperlink attributes via training a likelihood-based joint embedding model and reconstructing the joint latent space. Given a hypergraph dataset, ReLaSH first embeds the hyperlinks and their attributes into a joint latent space by training a likelihood-based model, and then reconstructs this joint latent space using a distribution-free generator. The generation task is completed by first sampling embeddings from the distribution-free generator and then decoding them into hyperlinks and attributes through the trained likelihood-based model. Compared with existing generative models, ReLaSH explicitly accounts for the unique structure of hypergraphs and jointly models hyperlinks and their attributes. Moreover, the likelihood-based embedding model provides efficiency and interpretability relative to deep black-box architectures, while the distribution-free generator in the joint latent space ensures flexibility. We theoretically demonstrate consistency and generalizability of ReLaSH. Empirical results on a range of real-world datasets from diverse domains demonstrate the strong performance of ReLaSH, underscoring its broad utility and effectiveness in practical applications.", "abstract_source_venue": "OpenReviewAPI", "abstract_source_url": "https://api2.openreview.net/notes?id=SG3kS2h44t", "openreview_id": "SG3kS2h44t", "openreview_forum_id": "SG3kS2h44t", "authors": [], "pdf_url": "https://openreview.net/pdf/e703e6645951c7a2d3b51da984838dc424ca2c24.pdf", "summary_cn": "ReLaSH框架通过联合嵌入与重构潜在空间，高效生成带属性的超图数据，解决了现有模型不适用的问题。", "keywords": ["超图生成", "联合潜在空间", "属性建模", "数据增强", "生成模型", "可解释性"], "triple": {"method": "训练基于似然的联合嵌入模型并重构潜在空间", "result": "在多个真实数据集上表现优异，生成逼真超图数据", "contribution": "提出灵活高效的超图生成框架，兼顾结构与属性"}}
{"venue": "ICLR", "search_title": "Unveiling the Mechanism of Continuous Representation Full-Waveform Inversion: A Wave Based Neural Tangent Kernel Framework", "full_title": "Unveiling the Mechanism of Continuous Representation Full-Waveform Inversion: A Wave Based Neural Tangent Kernel Framework", "url": "https://openreview.net/forum?id=blqYa21WOv", "year": 2026, "is_main_conference": true, "abstract_snippet": "Full-waveform inversion (FWI) estimates physical parameters in the wave equation from limited measurements and has been widely applied in geophysical exploration, medical imaging, and non-destructive testing. Conventional FWI methods are limited by their notorious sensitivity to the accuracy of the initial models. Recent progress in continuous representation FWI (CR-FWI) demonstrates that representing parameter models with a coordinate-based neural network, such as implicit neural representation (INR), can mitigate the dependence on initial models. However, its underlying mechanism remains unclear, and INR-based FWI shows slower high-frequency convergence. In this work, we investigate the general CR-FWI framework and develop a unified theoretical understanding by extending the neural tangent kernel (NTK) for FWI to establish a wave-based NTK framework. Unlike standard NTK, our analysis reveals that wave-based NTK is not constant, both at initialization and during training, due to the inherent nonlinearity of FWI. We further show that the eigenvalue decay behavior of the wave-based NTK can explain why CR-FWI alleviates the dependency on initial models and shows slower high-frequency convergence. Building on these insights, we propose several CR-FWI methods with tailored eigenvalue decay properties for FWI, including a novel hybrid representation combining INR and multi-resolution grid (termed IG-FWI) that achieves a more balanced trade-off between robustness and high-frequency convergence rate. Applications in geophysical exploration on Marmousi, 2D SEG/EAGE Salt and Overthrust, 2004 BP model, and the more realistic 2014 Chevron models show the superior performance of our proposed methods compared to conventional FWI and existing INR-based FWI methods.", "abstract": "Full-waveform inversion (FWI) estimates physical parameters in the wave equation from limited measurements and has been widely applied in geophysical exploration, medical imaging, and non-destructive testing. Conventional FWI methods are limited by their notorious sensitivity to the accuracy of the initial models. Recent progress in continuous representation FWI (CR-FWI) demonstrates that representing parameter models with a coordinate-based neural network, such as implicit neural representation (INR), can mitigate the dependence on initial models. However, its underlying mechanism remains unclear, and INR-based FWI shows slower high-frequency convergence. In this work, we investigate the general CR-FWI framework and develop a unified theoretical understanding by extending the neural tangent kernel (NTK) for FWI to establish a wave-based NTK framework. Unlike standard NTK, our analysis reveals that wave-based NTK is not constant, both at initialization and during training, due to the inherent nonlinearity of FWI. We further show that the eigenvalue decay behavior of the wave-based NTK can explain why CR-FWI alleviates the dependency on initial models and shows slower high-frequency convergence. Building on these insights, we propose several CR-FWI methods with tailored eigenvalue decay properties for FWI, including a novel hybrid representation combining INR and multi-resolution grid (termed IG-FWI) that achieves a more balanced trade-off between robustness and high-frequency convergence rate. Applications in geophysical exploration on Marmousi, 2D SEG/EAGE Salt and Overthrust, 2004 BP model, and the more realistic 2014 Chevron models show the superior performance of our proposed methods compared to conventional FWI and existing INR-based FWI methods.", "abstract_source_venue": "OpenReviewAPI", "abstract_source_url": "https://api2.openreview.net/notes?id=blqYa21WOv", "openreview_id": "blqYa21WOv", "openreview_forum_id": "blqYa21WOv", "authors": [], "pdf_url": "https://openreview.net/pdf/743022d9ab841f52ab407b172fccdbab8768d582.pdf", "summary_cn": "研究提出基于神经正切核的波动理论框架，揭示连续表示全波形反演机制，解释其降低初始模型依赖但高频收敛慢的原因，并提出改进方法提升性能。", "keywords": ["全波形反演", "神经正切核", "连续表示", "初始模型依赖", "高频收敛", "混合表示"], "triple": {"method": "扩展神经正切核至波动方程，建立波动NTK框架", "result": "解释CR-FWI降低初始模型依赖但高频收敛慢，提出改进方法性能更优", "contribution": "提供统一理论理解，提出平衡稳健性与收敛速度的混合表示方法"}}
{"venue": "ICLR", "search_title": "OpenPros: A Large-Scale Dataset for Limited View Prostate Ultrasound Computed Tomography", "full_title": "OpenPros: A Large-Scale Dataset for Limited View Prostate Ultrasound Computed Tomography", "url": "https://openreview.net/forum?id=kcFEpBagea", "year": 2026, "is_main_conference": true, "abstract_snippet": "Prostate cancer is one of the most common and lethal cancers among men, making its early detection critically important. Although ultrasound imaging offers greater accessibility and cost-effectiveness compared to MRI, traditional transrectal ultrasound (TRUS) methods suffer from low sensitivity, especially in detecting anteriorly located tumors. Ultrasound computed tomography (USCT) provides quantitative tissue characterization, but its clinical implementation faces significant challenges, particularly under anatomically constrained limited-angle acquisition conditions specific to prostate imaging. To address these unmet needs, we introduce OpenPros, the first large-scale benchmark dataset for limited-angle prostate USCT designed to systematically evaluate ML methods for inverse problems. Our dataset includes over 280,000 paired samples of realistic 2D speed-of-sound (SOS) phantoms and corresponding ultrasound full-waveform data, generated from anatomically accurate 3D digital prostate models derived from real clinical MRI/CT scans and ex vivo ultrasound measurements, annotated by medical experts. Simulations are conducted under clinically realistic configurations using advanced finite-difference time-domain (FDTD) and Runge-Kutta acoustic wave solvers, both provided as open-source components. Through comprehensive benchmarking, we find that deep learning methods significantly outperform traditional physics-based algorithms in inference efficiency and reconstruction accuracy. However, our results also reveal that current machine learning methods fail to deliver clinically acceptable, high-resolution reconstructions, underscoring critical gaps in generalization, robustness, and uncertainty quantification. By publicly releasing OpenPros, we provide the community with a rigorous benchmark that not only enables fair method comparison but also motivates new advances in physics-informed learning, foundation models for scientific imaging, and uncertainty-aware reconstruction—bridging the gap between academic ML research and real-world clinical deployment.", "abstract": "Prostate cancer is one of the most common and lethal cancers among men, making its early detection critically important. Although ultrasound imaging offers greater accessibility and cost-effectiveness compared to MRI, traditional transrectal ultrasound (TRUS) methods suffer from low sensitivity, especially in detecting anteriorly located tumors. Ultrasound computed tomography (USCT) provides quantitative tissue characterization, but its clinical implementation faces significant challenges, particularly under anatomically constrained limited-angle acquisition conditions specific to prostate imaging. To address these unmet needs, we introduce OpenPros, the first large-scale benchmark dataset for limited-angle prostate USCT designed to systematically evaluate ML methods for inverse problems. Our dataset includes over 280,000 paired samples of realistic 2D speed-of-sound (SOS) phantoms and corresponding ultrasound full-waveform data, generated from anatomically accurate 3D digital prostate models derived from real clinical MRI/CT scans and ex vivo ultrasound measurements, annotated by medical experts. Simulations are conducted under clinically realistic configurations using advanced finite-difference time-domain (FDTD) and Runge-Kutta acoustic wave solvers, both provided as open-source components. Through comprehensive benchmarking, we find that deep learning methods significantly outperform traditional physics-based algorithms in inference efficiency and reconstruction accuracy. However, our results also reveal that current machine learning methods fail to deliver clinically acceptable, high-resolution reconstructions, underscoring critical gaps in generalization, robustness, and uncertainty quantification. By publicly releasing OpenPros, we provide the community with a rigorous benchmark that not only enables fair method comparison but also motivates new advances in physics-informed learning, foundation models for scientific imaging, and uncertainty-aware reconstruction—bridging the gap between academic ML research and real-world clinical deployment.", "abstract_source_venue": "OpenReviewAPI", "abstract_source_url": "https://api2.openreview.net/notes?id=kcFEpBagea", "openreview_id": "kcFEpBagea", "openreview_forum_id": "kcFEpBagea", "authors": [], "pdf_url": "https://openreview.net/pdf/bda2427a109b6042f4892ecbe5bb4a3cb0b0b62d.pdf", "summary_cn": "OpenPros是首个大规模有限角度前列腺超声CT数据集，包含28万配对样本，用于评估ML逆问题方法。深度学习在效率和精度上优于传统算法，但临床高分辨率重建仍存差距。", "keywords": ["前列腺超声CT", "有限角度成像", "深度学习", "逆问题", "数据集", "重建精度"], "triple": {"method": "使用FDTD和Runge-Kutta声波求解器生成仿真数据", "result": "深度学习效率与精度更优，但临床高分辨率重建不足", "contribution": "提供首个大规模基准数据集，推动物理信息学习与临床部署"}}
{"venue": "ICLR", "search_title": "A Generalized Geometric Theoretical Framework of Centroid Discriminant Analysis for Linear Classification of Multi-dimensional Data", "full_title": "A Generalized Geometric Theoretical Framework of Centroid Discriminant Analysis for Linear Classification of Multi-dimensional Data", "url": "https://openreview.net/forum?id=bp9DOHb1mk", "year": 2026, "is_main_conference": true, "abstract_snippet": "With the advent of the neural network era, traditional machine learning methods have increasingly been overshadowed. Nevertheless, continuing to research about the role of geometry for learning in data science is crucial to envision and understand new principles behind the design of efficient machine learning. Linear classifiers are favored in certain tasks due to their reduced susceptibility to overfitting and their ability to provide interpretable decision boundaries. However, achieving both scalability and high predictive performance in linear classification remains a persistent challenge. Here, we propose a theoretical framework named geometric discriminant analysis (GDA). GDA includes the family of linear classifiers that can be expressed as function of a centroid discriminant basis (CDB0) - the connection line between two centroids - adjusted by geometric corrections under different constraints. We demonstrate that linear discriminant analysis (LDA) is a subcase of the GDA theoretical framework, and we show its convergence to CDB0 under certain conditions. Then, based on the GDA framework, we propose an efficient linear classifier named centroid discriminant analysis (CDA) which is defined as a special case of GDA under a 2D plane geometric constraint. CDA training is initialized starting from CDB0 and involves the iterative calculation of new adjusted centroid discriminant lines whose optimal rotations on the associated 2D planes are searched via Bayesian optimization. CDA has good scalability (quadratic time complexity) which is lower than LDA and support vectors machine (SVM) (cubic complexity). Results on 27 real datasets across classification tasks of standard images, medical images and chemical properties, offer empirical evidence that CDA outperforms other linear methods such as LDA, SVM and logistic regression (LR) in terms of scalability, performance and stability. Furthermore, we show that linear CDA can be generalized to nonlinear CDA via kernel method, demonstrating improvements on both linear CDA and kernel SVM with tests on three challenging datasets of images and chemical data. GDA general validity as a new theoretical framework may inspire the design of new classifiers under the definition of different geometric constraints, paving the way towards more deeper understanding of the role of geometry in learning from data.", "abstract": "With the advent of the neural network era, traditional machine learning methods have increasingly been overshadowed. Nevertheless, continuing to research about the role of geometry for learning in data science is crucial to envision and understand new principles behind the design of efficient machine learning. Linear classifiers are favored in certain tasks due to their reduced susceptibility to overfitting and their ability to provide interpretable decision boundaries. However, achieving both scalability and high predictive performance in linear classification remains a persistent challenge. Here, we propose a theoretical framework named geometric discriminant analysis (GDA). GDA includes the family of linear classifiers that can be expressed as function of a centroid discriminant basis (CDB0) - the connection line between two centroids - adjusted by geometric corrections under different constraints. We demonstrate that linear discriminant analysis (LDA) is a subcase of the GDA theoretical framework, and we show its convergence to CDB0 under certain conditions. Then, based on the GDA framework, we propose an efficient linear classifier named centroid discriminant analysis (CDA) which is defined as a special case of GDA under a 2D plane geometric constraint. CDA training is initialized starting from CDB0 and involves the iterative calculation of new adjusted centroid discriminant lines whose optimal rotations on the associated 2D planes are searched via Bayesian optimization. CDA has good scalability (quadratic time complexity) which is lower than LDA and support vectors machine (SVM) (cubic complexity). Results on 27 real datasets across classification tasks of standard images, medical images and chemical properties, offer empirical evidence that CDA outperforms other linear methods such as LDA, SVM and logistic regression (LR) in terms of scalability, performance and stability. Furthermore, we show that linear CDA can be generalized to nonlinear CDA via kernel method, demonstrating improvements on both linear CDA and kernel SVM with tests on three challenging datasets of images and chemical data. GDA general validity as a new theoretical framework may inspire the design of new classifiers under the definition of different geometric constraints, paving the way towards more deeper understanding of the role of geometry in learning from data.", "abstract_source_venue": "OpenReviewAPI", "abstract_source_url": "https://api2.openreview.net/notes?id=bp9DOHb1mk", "openreview_id": "bp9DOHb1mk", "openreview_forum_id": "bp9DOHb1mk", "authors": [], "pdf_url": "https://openreview.net/pdf/74ba7766a3fb0f519ee2ef9cfc48b1c861ac1e2b.pdf", "summary_cn": "提出几何判别分析（GDA）理论框架，包含基于质心判别基的线性分类器。基于GDA的质心判别分析（CDA）在27个数据集上优于LDA、SVM和逻辑回归，具有更好的可扩展性和性能。", "keywords": ["几何判别分析", "线性分类", "质心判别基", "贝叶斯优化", "可扩展性", "核方法"], "triple": {"method": "基于质心判别基的几何框架与贝叶斯优化", "result": "CDA在27个数据集上优于LDA、SVM和LR", "contribution": "提出GDA理论框架，推动几何在机器学习中的理解与应用"}}
